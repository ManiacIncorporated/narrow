{
  "best_global_step": 7000,
  "best_metric": 1.8247697353363037,
  "best_model_checkpoint": "./psych-llm/n0.50_r0.50/checkpoint-2500",
  "epoch": 6.0805,
  "eval_steps": 500,
  "global_step": 20000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00025,
      "grad_norm": 332.6608581542969,
      "learning_rate": 2.0000000000000002e-07,
      "loss": 9.2416,
      "step": 5
    },
    {
      "epoch": 0.0005,
      "grad_norm": 196.0044403076172,
      "learning_rate": 4.5e-07,
      "loss": 9.1473,
      "step": 10
    },
    {
      "epoch": 0.00075,
      "grad_norm": 169.60427856445312,
      "learning_rate": 7.000000000000001e-07,
      "loss": 8.9833,
      "step": 15
    },
    {
      "epoch": 0.001,
      "grad_norm": 90.69220733642578,
      "learning_rate": 9.5e-07,
      "loss": 8.627,
      "step": 20
    },
    {
      "epoch": 0.00125,
      "grad_norm": 70.55764770507812,
      "learning_rate": 1.2000000000000002e-06,
      "loss": 8.0377,
      "step": 25
    },
    {
      "epoch": 0.0015,
      "grad_norm": 46.99163055419922,
      "learning_rate": 1.45e-06,
      "loss": 7.4506,
      "step": 30
    },
    {
      "epoch": 0.00175,
      "grad_norm": 42.48370361328125,
      "learning_rate": 1.7000000000000002e-06,
      "loss": 6.806,
      "step": 35
    },
    {
      "epoch": 0.002,
      "grad_norm": 30.28043556213379,
      "learning_rate": 1.95e-06,
      "loss": 6.6235,
      "step": 40
    },
    {
      "epoch": 0.00225,
      "grad_norm": 26.433860778808594,
      "learning_rate": 2.2e-06,
      "loss": 6.4312,
      "step": 45
    },
    {
      "epoch": 0.0025,
      "grad_norm": 29.02863121032715,
      "learning_rate": 2.4500000000000003e-06,
      "loss": 6.1869,
      "step": 50
    },
    {
      "epoch": 0.00275,
      "grad_norm": 40.29967498779297,
      "learning_rate": 2.7e-06,
      "loss": 6.3308,
      "step": 55
    },
    {
      "epoch": 0.003,
      "grad_norm": 36.439212799072266,
      "learning_rate": 2.95e-06,
      "loss": 5.9571,
      "step": 60
    },
    {
      "epoch": 0.00325,
      "grad_norm": 24.90250587463379,
      "learning_rate": 3.2000000000000003e-06,
      "loss": 5.9141,
      "step": 65
    },
    {
      "epoch": 0.0035,
      "grad_norm": 26.284814834594727,
      "learning_rate": 3.4500000000000004e-06,
      "loss": 5.5958,
      "step": 70
    },
    {
      "epoch": 0.00375,
      "grad_norm": 20.0127010345459,
      "learning_rate": 3.7e-06,
      "loss": 5.3736,
      "step": 75
    },
    {
      "epoch": 0.004,
      "grad_norm": 41.44436264038086,
      "learning_rate": 3.95e-06,
      "loss": 5.2841,
      "step": 80
    },
    {
      "epoch": 0.00425,
      "grad_norm": 28.323646545410156,
      "learning_rate": 4.2000000000000004e-06,
      "loss": 5.1696,
      "step": 85
    },
    {
      "epoch": 0.0045,
      "grad_norm": 22.97652244567871,
      "learning_rate": 4.45e-06,
      "loss": 5.1091,
      "step": 90
    },
    {
      "epoch": 0.00475,
      "grad_norm": 28.029102325439453,
      "learning_rate": 4.7e-06,
      "loss": 4.9158,
      "step": 95
    },
    {
      "epoch": 0.005,
      "grad_norm": 27.653772354125977,
      "learning_rate": 4.950000000000001e-06,
      "loss": 4.8688,
      "step": 100
    },
    {
      "epoch": 0.00525,
      "grad_norm": 21.295265197753906,
      "learning_rate": 5.2e-06,
      "loss": 4.6749,
      "step": 105
    },
    {
      "epoch": 0.0055,
      "grad_norm": 24.745864868164062,
      "learning_rate": 5.45e-06,
      "loss": 4.7819,
      "step": 110
    },
    {
      "epoch": 0.00575,
      "grad_norm": 19.96503448486328,
      "learning_rate": 5.7000000000000005e-06,
      "loss": 4.5053,
      "step": 115
    },
    {
      "epoch": 0.006,
      "grad_norm": 24.672609329223633,
      "learning_rate": 5.95e-06,
      "loss": 4.6216,
      "step": 120
    },
    {
      "epoch": 0.00625,
      "grad_norm": 24.570589065551758,
      "learning_rate": 6.2e-06,
      "loss": 4.3658,
      "step": 125
    },
    {
      "epoch": 0.0065,
      "grad_norm": 23.358076095581055,
      "learning_rate": 6.45e-06,
      "loss": 4.6221,
      "step": 130
    },
    {
      "epoch": 0.00675,
      "grad_norm": 19.272167205810547,
      "learning_rate": 6.700000000000001e-06,
      "loss": 4.3116,
      "step": 135
    },
    {
      "epoch": 0.007,
      "grad_norm": 18.563493728637695,
      "learning_rate": 6.950000000000001e-06,
      "loss": 4.3695,
      "step": 140
    },
    {
      "epoch": 0.00725,
      "grad_norm": 19.476438522338867,
      "learning_rate": 7.2e-06,
      "loss": 4.4775,
      "step": 145
    },
    {
      "epoch": 0.0075,
      "grad_norm": 23.058509826660156,
      "learning_rate": 7.45e-06,
      "loss": 4.471,
      "step": 150
    },
    {
      "epoch": 0.00775,
      "grad_norm": 23.83031463623047,
      "learning_rate": 7.7e-06,
      "loss": 4.3709,
      "step": 155
    },
    {
      "epoch": 0.008,
      "grad_norm": 18.804931640625,
      "learning_rate": 7.95e-06,
      "loss": 4.278,
      "step": 160
    },
    {
      "epoch": 0.00825,
      "grad_norm": 21.893917083740234,
      "learning_rate": 8.200000000000001e-06,
      "loss": 4.4834,
      "step": 165
    },
    {
      "epoch": 0.0085,
      "grad_norm": 28.492090225219727,
      "learning_rate": 8.45e-06,
      "loss": 4.2749,
      "step": 170
    },
    {
      "epoch": 0.00875,
      "grad_norm": 18.469928741455078,
      "learning_rate": 8.7e-06,
      "loss": 4.1695,
      "step": 175
    },
    {
      "epoch": 0.009,
      "grad_norm": 16.212039947509766,
      "learning_rate": 8.95e-06,
      "loss": 4.186,
      "step": 180
    },
    {
      "epoch": 0.00925,
      "grad_norm": 16.28546905517578,
      "learning_rate": 9.2e-06,
      "loss": 4.1213,
      "step": 185
    },
    {
      "epoch": 0.0095,
      "grad_norm": 18.025165557861328,
      "learning_rate": 9.450000000000001e-06,
      "loss": 3.9924,
      "step": 190
    },
    {
      "epoch": 0.00975,
      "grad_norm": 32.81068801879883,
      "learning_rate": 9.7e-06,
      "loss": 3.9791,
      "step": 195
    },
    {
      "epoch": 0.01,
      "grad_norm": 16.756324768066406,
      "learning_rate": 9.950000000000001e-06,
      "loss": 4.0811,
      "step": 200
    },
    {
      "epoch": 0.01025,
      "grad_norm": 20.726743698120117,
      "learning_rate": 1.02e-05,
      "loss": 3.966,
      "step": 205
    },
    {
      "epoch": 0.0105,
      "grad_norm": 17.606735229492188,
      "learning_rate": 1.045e-05,
      "loss": 4.1316,
      "step": 210
    },
    {
      "epoch": 0.01075,
      "grad_norm": 17.917858123779297,
      "learning_rate": 1.0700000000000001e-05,
      "loss": 3.8079,
      "step": 215
    },
    {
      "epoch": 0.011,
      "grad_norm": 21.32921028137207,
      "learning_rate": 1.095e-05,
      "loss": 3.8267,
      "step": 220
    },
    {
      "epoch": 0.01125,
      "grad_norm": 16.93169403076172,
      "learning_rate": 1.1200000000000001e-05,
      "loss": 3.6834,
      "step": 225
    },
    {
      "epoch": 0.0115,
      "grad_norm": 21.44501495361328,
      "learning_rate": 1.145e-05,
      "loss": 3.6129,
      "step": 230
    },
    {
      "epoch": 0.01175,
      "grad_norm": 21.357126235961914,
      "learning_rate": 1.1700000000000001e-05,
      "loss": 3.6432,
      "step": 235
    },
    {
      "epoch": 0.012,
      "grad_norm": 18.023841857910156,
      "learning_rate": 1.195e-05,
      "loss": 3.3415,
      "step": 240
    },
    {
      "epoch": 0.01225,
      "grad_norm": 18.900602340698242,
      "learning_rate": 1.22e-05,
      "loss": 3.5668,
      "step": 245
    },
    {
      "epoch": 0.0125,
      "grad_norm": 14.337401390075684,
      "learning_rate": 1.2450000000000001e-05,
      "loss": 3.1836,
      "step": 250
    },
    {
      "epoch": 0.01275,
      "grad_norm": 14.979552268981934,
      "learning_rate": 1.27e-05,
      "loss": 2.9261,
      "step": 255
    },
    {
      "epoch": 0.013,
      "grad_norm": 14.574263572692871,
      "learning_rate": 1.2950000000000001e-05,
      "loss": 2.8646,
      "step": 260
    },
    {
      "epoch": 0.01325,
      "grad_norm": 14.383328437805176,
      "learning_rate": 1.32e-05,
      "loss": 2.9596,
      "step": 265
    },
    {
      "epoch": 0.0135,
      "grad_norm": 18.52274513244629,
      "learning_rate": 1.3450000000000002e-05,
      "loss": 2.8324,
      "step": 270
    },
    {
      "epoch": 0.01375,
      "grad_norm": 22.495433807373047,
      "learning_rate": 1.3700000000000001e-05,
      "loss": 3.397,
      "step": 275
    },
    {
      "epoch": 0.014,
      "grad_norm": 16.594276428222656,
      "learning_rate": 1.3950000000000002e-05,
      "loss": 4.0227,
      "step": 280
    },
    {
      "epoch": 0.01425,
      "grad_norm": 16.890438079833984,
      "learning_rate": 1.42e-05,
      "loss": 3.8045,
      "step": 285
    },
    {
      "epoch": 0.0145,
      "grad_norm": 15.985323905944824,
      "learning_rate": 1.4449999999999999e-05,
      "loss": 3.4495,
      "step": 290
    },
    {
      "epoch": 0.01475,
      "grad_norm": 14.515738487243652,
      "learning_rate": 1.47e-05,
      "loss": 3.5611,
      "step": 295
    },
    {
      "epoch": 0.015,
      "grad_norm": 15.308746337890625,
      "learning_rate": 1.4950000000000001e-05,
      "loss": 3.3214,
      "step": 300
    },
    {
      "epoch": 0.01525,
      "grad_norm": 19.790143966674805,
      "learning_rate": 1.52e-05,
      "loss": 2.5985,
      "step": 305
    },
    {
      "epoch": 0.0155,
      "grad_norm": 19.048532485961914,
      "learning_rate": 1.545e-05,
      "loss": 3.0591,
      "step": 310
    },
    {
      "epoch": 0.01575,
      "grad_norm": 14.764385223388672,
      "learning_rate": 1.5700000000000002e-05,
      "loss": 2.9659,
      "step": 315
    },
    {
      "epoch": 0.016,
      "grad_norm": 15.381394386291504,
      "learning_rate": 1.595e-05,
      "loss": 2.4999,
      "step": 320
    },
    {
      "epoch": 0.01625,
      "grad_norm": 20.283103942871094,
      "learning_rate": 1.62e-05,
      "loss": 2.716,
      "step": 325
    },
    {
      "epoch": 0.0165,
      "grad_norm": 21.653141021728516,
      "learning_rate": 1.645e-05,
      "loss": 2.8719,
      "step": 330
    },
    {
      "epoch": 0.01675,
      "grad_norm": 16.956829071044922,
      "learning_rate": 1.6700000000000003e-05,
      "loss": 3.4236,
      "step": 335
    },
    {
      "epoch": 0.017,
      "grad_norm": 18.541574478149414,
      "learning_rate": 1.6950000000000002e-05,
      "loss": 3.5538,
      "step": 340
    },
    {
      "epoch": 0.01725,
      "grad_norm": 15.988021850585938,
      "learning_rate": 1.7199999999999998e-05,
      "loss": 3.2484,
      "step": 345
    },
    {
      "epoch": 0.0175,
      "grad_norm": 19.389240264892578,
      "learning_rate": 1.745e-05,
      "loss": 3.3075,
      "step": 350
    },
    {
      "epoch": 0.01775,
      "grad_norm": 15.050938606262207,
      "learning_rate": 1.77e-05,
      "loss": 2.8109,
      "step": 355
    },
    {
      "epoch": 0.018,
      "grad_norm": 15.515076637268066,
      "learning_rate": 1.795e-05,
      "loss": 2.8817,
      "step": 360
    },
    {
      "epoch": 0.01825,
      "grad_norm": 15.862982749938965,
      "learning_rate": 1.8200000000000002e-05,
      "loss": 3.2386,
      "step": 365
    },
    {
      "epoch": 0.0185,
      "grad_norm": 15.436165809631348,
      "learning_rate": 1.845e-05,
      "loss": 3.5885,
      "step": 370
    },
    {
      "epoch": 0.01875,
      "grad_norm": 18.43939208984375,
      "learning_rate": 1.87e-05,
      "loss": 3.2246,
      "step": 375
    },
    {
      "epoch": 0.019,
      "grad_norm": 12.966292381286621,
      "learning_rate": 1.895e-05,
      "loss": 3.5317,
      "step": 380
    },
    {
      "epoch": 0.01925,
      "grad_norm": 15.393996238708496,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 3.3332,
      "step": 385
    },
    {
      "epoch": 0.0195,
      "grad_norm": 15.701432228088379,
      "learning_rate": 1.9450000000000002e-05,
      "loss": 3.2917,
      "step": 390
    },
    {
      "epoch": 0.01975,
      "grad_norm": 15.126559257507324,
      "learning_rate": 1.97e-05,
      "loss": 3.0078,
      "step": 395
    },
    {
      "epoch": 0.02,
      "grad_norm": 13.826329231262207,
      "learning_rate": 1.995e-05,
      "loss": 3.1531,
      "step": 400
    },
    {
      "epoch": 0.02025,
      "grad_norm": 16.58074951171875,
      "learning_rate": 2.0200000000000003e-05,
      "loss": 3.3948,
      "step": 405
    },
    {
      "epoch": 0.0205,
      "grad_norm": 15.386434555053711,
      "learning_rate": 2.045e-05,
      "loss": 3.2724,
      "step": 410
    },
    {
      "epoch": 0.02075,
      "grad_norm": 14.030088424682617,
      "learning_rate": 2.07e-05,
      "loss": 2.9162,
      "step": 415
    },
    {
      "epoch": 0.021,
      "grad_norm": 19.78839111328125,
      "learning_rate": 2.095e-05,
      "loss": 3.2483,
      "step": 420
    },
    {
      "epoch": 0.02125,
      "grad_norm": 11.883502960205078,
      "learning_rate": 2.12e-05,
      "loss": 3.1692,
      "step": 425
    },
    {
      "epoch": 0.0215,
      "grad_norm": 14.984627723693848,
      "learning_rate": 2.145e-05,
      "loss": 2.7707,
      "step": 430
    },
    {
      "epoch": 0.02175,
      "grad_norm": 16.353145599365234,
      "learning_rate": 2.1700000000000002e-05,
      "loss": 3.0637,
      "step": 435
    },
    {
      "epoch": 0.022,
      "grad_norm": 13.92194652557373,
      "learning_rate": 2.195e-05,
      "loss": 3.4292,
      "step": 440
    },
    {
      "epoch": 0.02225,
      "grad_norm": 19.80682945251465,
      "learning_rate": 2.22e-05,
      "loss": 3.4889,
      "step": 445
    },
    {
      "epoch": 0.0225,
      "grad_norm": 15.356945037841797,
      "learning_rate": 2.245e-05,
      "loss": 3.5099,
      "step": 450
    },
    {
      "epoch": 0.02275,
      "grad_norm": 17.905685424804688,
      "learning_rate": 2.2700000000000003e-05,
      "loss": 3.161,
      "step": 455
    },
    {
      "epoch": 0.023,
      "grad_norm": 11.38597583770752,
      "learning_rate": 2.2950000000000002e-05,
      "loss": 3.3186,
      "step": 460
    },
    {
      "epoch": 0.02325,
      "grad_norm": 14.124802589416504,
      "learning_rate": 2.32e-05,
      "loss": 2.9688,
      "step": 465
    },
    {
      "epoch": 0.0235,
      "grad_norm": 15.41091251373291,
      "learning_rate": 2.345e-05,
      "loss": 3.199,
      "step": 470
    },
    {
      "epoch": 0.02375,
      "grad_norm": 9.98116397857666,
      "learning_rate": 2.37e-05,
      "loss": 2.8677,
      "step": 475
    },
    {
      "epoch": 0.024,
      "grad_norm": 17.382966995239258,
      "learning_rate": 2.395e-05,
      "loss": 2.8923,
      "step": 480
    },
    {
      "epoch": 0.02425,
      "grad_norm": 14.735393524169922,
      "learning_rate": 2.4200000000000002e-05,
      "loss": 3.0137,
      "step": 485
    },
    {
      "epoch": 0.0245,
      "grad_norm": 18.789077758789062,
      "learning_rate": 2.445e-05,
      "loss": 3.3934,
      "step": 490
    },
    {
      "epoch": 0.02475,
      "grad_norm": 11.666470527648926,
      "learning_rate": 2.47e-05,
      "loss": 2.9536,
      "step": 495
    },
    {
      "epoch": 0.025,
      "grad_norm": 20.9415283203125,
      "learning_rate": 2.495e-05,
      "loss": 2.9748,
      "step": 500
    },
    {
      "epoch": 0.025,
      "eval_loss": 2.8316359519958496,
      "eval_runtime": 5.632,
      "eval_samples_per_second": 181.818,
      "eval_steps_per_second": 22.727,
      "step": 500
    },
    {
      "epoch": 0.02525,
      "grad_norm": 9.471538543701172,
      "learning_rate": 2.5200000000000003e-05,
      "loss": 2.9201,
      "step": 505
    },
    {
      "epoch": 0.0255,
      "grad_norm": 13.763753890991211,
      "learning_rate": 2.5450000000000002e-05,
      "loss": 3.1184,
      "step": 510
    },
    {
      "epoch": 0.02575,
      "grad_norm": 12.545760154724121,
      "learning_rate": 2.57e-05,
      "loss": 3.251,
      "step": 515
    },
    {
      "epoch": 0.026,
      "grad_norm": 13.57673168182373,
      "learning_rate": 2.595e-05,
      "loss": 2.8209,
      "step": 520
    },
    {
      "epoch": 0.02625,
      "grad_norm": 17.534456253051758,
      "learning_rate": 2.6200000000000003e-05,
      "loss": 3.4374,
      "step": 525
    },
    {
      "epoch": 0.0265,
      "grad_norm": 12.541692733764648,
      "learning_rate": 2.6450000000000003e-05,
      "loss": 2.8758,
      "step": 530
    },
    {
      "epoch": 0.02675,
      "grad_norm": 17.857362747192383,
      "learning_rate": 2.6700000000000002e-05,
      "loss": 3.0012,
      "step": 535
    },
    {
      "epoch": 0.027,
      "grad_norm": 18.423131942749023,
      "learning_rate": 2.6950000000000005e-05,
      "loss": 2.9846,
      "step": 540
    },
    {
      "epoch": 0.02725,
      "grad_norm": 12.258695602416992,
      "learning_rate": 2.7200000000000004e-05,
      "loss": 2.8838,
      "step": 545
    },
    {
      "epoch": 0.0275,
      "grad_norm": 14.417679786682129,
      "learning_rate": 2.7450000000000003e-05,
      "loss": 2.8873,
      "step": 550
    },
    {
      "epoch": 0.02775,
      "grad_norm": 13.237683296203613,
      "learning_rate": 2.7700000000000002e-05,
      "loss": 3.1735,
      "step": 555
    },
    {
      "epoch": 0.028,
      "grad_norm": 18.269010543823242,
      "learning_rate": 2.7950000000000005e-05,
      "loss": 2.9129,
      "step": 560
    },
    {
      "epoch": 0.02825,
      "grad_norm": 15.359588623046875,
      "learning_rate": 2.8199999999999998e-05,
      "loss": 3.2973,
      "step": 565
    },
    {
      "epoch": 0.0285,
      "grad_norm": 13.740433692932129,
      "learning_rate": 2.845e-05,
      "loss": 2.9928,
      "step": 570
    },
    {
      "epoch": 0.02875,
      "grad_norm": 18.890483856201172,
      "learning_rate": 2.87e-05,
      "loss": 3.232,
      "step": 575
    },
    {
      "epoch": 0.029,
      "grad_norm": 12.265748977661133,
      "learning_rate": 2.895e-05,
      "loss": 3.0104,
      "step": 580
    },
    {
      "epoch": 0.02925,
      "grad_norm": 12.063905715942383,
      "learning_rate": 2.9199999999999998e-05,
      "loss": 2.9227,
      "step": 585
    },
    {
      "epoch": 0.0295,
      "grad_norm": 8.872262001037598,
      "learning_rate": 2.945e-05,
      "loss": 2.5394,
      "step": 590
    },
    {
      "epoch": 0.02975,
      "grad_norm": 14.783597946166992,
      "learning_rate": 2.97e-05,
      "loss": 2.9664,
      "step": 595
    },
    {
      "epoch": 0.03,
      "grad_norm": 10.18107795715332,
      "learning_rate": 2.995e-05,
      "loss": 2.7644,
      "step": 600
    },
    {
      "epoch": 0.03025,
      "grad_norm": 15.997217178344727,
      "learning_rate": 3.02e-05,
      "loss": 2.898,
      "step": 605
    },
    {
      "epoch": 0.0305,
      "grad_norm": 11.658476829528809,
      "learning_rate": 3.045e-05,
      "loss": 3.032,
      "step": 610
    },
    {
      "epoch": 0.03075,
      "grad_norm": 15.634554862976074,
      "learning_rate": 3.07e-05,
      "loss": 2.8484,
      "step": 615
    },
    {
      "epoch": 0.031,
      "grad_norm": 10.084019660949707,
      "learning_rate": 3.095e-05,
      "loss": 2.7347,
      "step": 620
    },
    {
      "epoch": 0.03125,
      "grad_norm": 11.566332817077637,
      "learning_rate": 3.12e-05,
      "loss": 3.0781,
      "step": 625
    },
    {
      "epoch": 0.0315,
      "grad_norm": 10.312504768371582,
      "learning_rate": 3.145e-05,
      "loss": 3.0314,
      "step": 630
    },
    {
      "epoch": 0.03175,
      "grad_norm": 10.315170288085938,
      "learning_rate": 3.1700000000000005e-05,
      "loss": 2.9196,
      "step": 635
    },
    {
      "epoch": 0.032,
      "grad_norm": 11.083795547485352,
      "learning_rate": 3.1950000000000004e-05,
      "loss": 2.814,
      "step": 640
    },
    {
      "epoch": 0.03225,
      "grad_norm": 13.845507621765137,
      "learning_rate": 3.2200000000000003e-05,
      "loss": 2.8627,
      "step": 645
    },
    {
      "epoch": 0.0325,
      "grad_norm": 11.0933256149292,
      "learning_rate": 3.245e-05,
      "loss": 2.4793,
      "step": 650
    },
    {
      "epoch": 0.03275,
      "grad_norm": 10.492321014404297,
      "learning_rate": 3.27e-05,
      "loss": 2.4251,
      "step": 655
    },
    {
      "epoch": 0.033,
      "grad_norm": 12.0371732711792,
      "learning_rate": 3.295e-05,
      "loss": 2.768,
      "step": 660
    },
    {
      "epoch": 0.03325,
      "grad_norm": 11.272455215454102,
      "learning_rate": 3.32e-05,
      "loss": 2.8151,
      "step": 665
    },
    {
      "epoch": 0.0335,
      "grad_norm": 10.229520797729492,
      "learning_rate": 3.345000000000001e-05,
      "loss": 2.8889,
      "step": 670
    },
    {
      "epoch": 0.03375,
      "grad_norm": 13.216583251953125,
      "learning_rate": 3.3700000000000006e-05,
      "loss": 3.1228,
      "step": 675
    },
    {
      "epoch": 0.034,
      "grad_norm": 8.474196434020996,
      "learning_rate": 3.3950000000000005e-05,
      "loss": 3.1139,
      "step": 680
    },
    {
      "epoch": 0.03425,
      "grad_norm": 11.184823989868164,
      "learning_rate": 3.4200000000000005e-05,
      "loss": 2.6234,
      "step": 685
    },
    {
      "epoch": 0.0345,
      "grad_norm": 13.171247482299805,
      "learning_rate": 3.445e-05,
      "loss": 2.8525,
      "step": 690
    },
    {
      "epoch": 0.03475,
      "grad_norm": 8.603484153747559,
      "learning_rate": 3.4699999999999996e-05,
      "loss": 2.9453,
      "step": 695
    },
    {
      "epoch": 0.035,
      "grad_norm": 9.557312965393066,
      "learning_rate": 3.495e-05,
      "loss": 3.0527,
      "step": 700
    },
    {
      "epoch": 0.03525,
      "grad_norm": 9.332314491271973,
      "learning_rate": 3.52e-05,
      "loss": 2.8821,
      "step": 705
    },
    {
      "epoch": 0.0355,
      "grad_norm": 9.666382789611816,
      "learning_rate": 3.545e-05,
      "loss": 2.8176,
      "step": 710
    },
    {
      "epoch": 0.03575,
      "grad_norm": 10.09421157836914,
      "learning_rate": 3.57e-05,
      "loss": 2.7163,
      "step": 715
    },
    {
      "epoch": 0.036,
      "grad_norm": 15.570270538330078,
      "learning_rate": 3.595e-05,
      "loss": 3.1386,
      "step": 720
    },
    {
      "epoch": 0.03625,
      "grad_norm": 8.873991966247559,
      "learning_rate": 3.62e-05,
      "loss": 2.9222,
      "step": 725
    },
    {
      "epoch": 0.0365,
      "grad_norm": 17.975177764892578,
      "learning_rate": 3.645e-05,
      "loss": 2.9656,
      "step": 730
    },
    {
      "epoch": 0.03675,
      "grad_norm": 11.03941822052002,
      "learning_rate": 3.6700000000000004e-05,
      "loss": 2.953,
      "step": 735
    },
    {
      "epoch": 0.037,
      "grad_norm": 7.477421760559082,
      "learning_rate": 3.6950000000000004e-05,
      "loss": 2.9258,
      "step": 740
    },
    {
      "epoch": 0.03725,
      "grad_norm": 9.912332534790039,
      "learning_rate": 3.72e-05,
      "loss": 2.8041,
      "step": 745
    },
    {
      "epoch": 0.0375,
      "grad_norm": 9.618053436279297,
      "learning_rate": 3.745e-05,
      "loss": 3.1015,
      "step": 750
    },
    {
      "epoch": 0.03775,
      "grad_norm": 8.682802200317383,
      "learning_rate": 3.77e-05,
      "loss": 2.7946,
      "step": 755
    },
    {
      "epoch": 0.038,
      "grad_norm": 9.623044967651367,
      "learning_rate": 3.795e-05,
      "loss": 2.7297,
      "step": 760
    },
    {
      "epoch": 0.03825,
      "grad_norm": 13.346869468688965,
      "learning_rate": 3.82e-05,
      "loss": 2.8338,
      "step": 765
    },
    {
      "epoch": 0.0385,
      "grad_norm": 10.015835762023926,
      "learning_rate": 3.845e-05,
      "loss": 2.7981,
      "step": 770
    },
    {
      "epoch": 0.03875,
      "grad_norm": 12.527046203613281,
      "learning_rate": 3.8700000000000006e-05,
      "loss": 3.0037,
      "step": 775
    },
    {
      "epoch": 0.039,
      "grad_norm": 15.23563003540039,
      "learning_rate": 3.8950000000000005e-05,
      "loss": 2.7192,
      "step": 780
    },
    {
      "epoch": 0.03925,
      "grad_norm": 8.117526054382324,
      "learning_rate": 3.9200000000000004e-05,
      "loss": 2.6694,
      "step": 785
    },
    {
      "epoch": 0.0395,
      "grad_norm": 11.037257194519043,
      "learning_rate": 3.9450000000000003e-05,
      "loss": 3.0285,
      "step": 790
    },
    {
      "epoch": 0.03975,
      "grad_norm": 8.168716430664062,
      "learning_rate": 3.97e-05,
      "loss": 2.7099,
      "step": 795
    },
    {
      "epoch": 0.04,
      "grad_norm": 8.607379913330078,
      "learning_rate": 3.995e-05,
      "loss": 2.6527,
      "step": 800
    },
    {
      "epoch": 0.04025,
      "grad_norm": 8.845088958740234,
      "learning_rate": 4.02e-05,
      "loss": 2.8449,
      "step": 805
    },
    {
      "epoch": 0.0405,
      "grad_norm": 12.607258796691895,
      "learning_rate": 4.045000000000001e-05,
      "loss": 2.6949,
      "step": 810
    },
    {
      "epoch": 0.04075,
      "grad_norm": 8.285894393920898,
      "learning_rate": 4.07e-05,
      "loss": 2.6331,
      "step": 815
    },
    {
      "epoch": 0.041,
      "grad_norm": 8.661749839782715,
      "learning_rate": 4.095e-05,
      "loss": 2.9217,
      "step": 820
    },
    {
      "epoch": 0.04125,
      "grad_norm": 9.598248481750488,
      "learning_rate": 4.12e-05,
      "loss": 2.9516,
      "step": 825
    },
    {
      "epoch": 0.0415,
      "grad_norm": 7.924359321594238,
      "learning_rate": 4.145e-05,
      "loss": 3.0735,
      "step": 830
    },
    {
      "epoch": 0.04175,
      "grad_norm": 8.416391372680664,
      "learning_rate": 4.17e-05,
      "loss": 2.9998,
      "step": 835
    },
    {
      "epoch": 0.042,
      "grad_norm": 9.700359344482422,
      "learning_rate": 4.195e-05,
      "loss": 2.7951,
      "step": 840
    },
    {
      "epoch": 0.04225,
      "grad_norm": 11.453875541687012,
      "learning_rate": 4.22e-05,
      "loss": 2.9331,
      "step": 845
    },
    {
      "epoch": 0.0425,
      "grad_norm": 8.63502025604248,
      "learning_rate": 4.245e-05,
      "loss": 2.8783,
      "step": 850
    },
    {
      "epoch": 0.04275,
      "grad_norm": 10.016520500183105,
      "learning_rate": 4.27e-05,
      "loss": 2.8773,
      "step": 855
    },
    {
      "epoch": 0.043,
      "grad_norm": 12.834203720092773,
      "learning_rate": 4.295e-05,
      "loss": 2.697,
      "step": 860
    },
    {
      "epoch": 0.04325,
      "grad_norm": 9.693192481994629,
      "learning_rate": 4.32e-05,
      "loss": 3.1815,
      "step": 865
    },
    {
      "epoch": 0.0435,
      "grad_norm": 11.76453685760498,
      "learning_rate": 4.345e-05,
      "loss": 3.0126,
      "step": 870
    },
    {
      "epoch": 0.04375,
      "grad_norm": 9.127300262451172,
      "learning_rate": 4.3700000000000005e-05,
      "loss": 2.6373,
      "step": 875
    },
    {
      "epoch": 0.044,
      "grad_norm": 8.462579727172852,
      "learning_rate": 4.3950000000000004e-05,
      "loss": 2.7478,
      "step": 880
    },
    {
      "epoch": 0.04425,
      "grad_norm": 8.860200881958008,
      "learning_rate": 4.4200000000000004e-05,
      "loss": 2.6174,
      "step": 885
    },
    {
      "epoch": 0.0445,
      "grad_norm": 8.757865905761719,
      "learning_rate": 4.445e-05,
      "loss": 2.736,
      "step": 890
    },
    {
      "epoch": 0.04475,
      "grad_norm": 10.600226402282715,
      "learning_rate": 4.47e-05,
      "loss": 2.9893,
      "step": 895
    },
    {
      "epoch": 0.045,
      "grad_norm": 7.350685119628906,
      "learning_rate": 4.495e-05,
      "loss": 2.8275,
      "step": 900
    },
    {
      "epoch": 0.04525,
      "grad_norm": 8.923379898071289,
      "learning_rate": 4.52e-05,
      "loss": 2.7217,
      "step": 905
    },
    {
      "epoch": 0.0455,
      "grad_norm": 11.707754135131836,
      "learning_rate": 4.545000000000001e-05,
      "loss": 2.7269,
      "step": 910
    },
    {
      "epoch": 0.04575,
      "grad_norm": 10.10006046295166,
      "learning_rate": 4.5700000000000006e-05,
      "loss": 2.7453,
      "step": 915
    },
    {
      "epoch": 0.046,
      "grad_norm": 8.115103721618652,
      "learning_rate": 4.5950000000000006e-05,
      "loss": 2.6734,
      "step": 920
    },
    {
      "epoch": 0.04625,
      "grad_norm": 9.617769241333008,
      "learning_rate": 4.6200000000000005e-05,
      "loss": 2.9278,
      "step": 925
    },
    {
      "epoch": 0.0465,
      "grad_norm": 7.266482830047607,
      "learning_rate": 4.6450000000000004e-05,
      "loss": 2.6137,
      "step": 930
    },
    {
      "epoch": 0.04675,
      "grad_norm": 8.149477005004883,
      "learning_rate": 4.6700000000000003e-05,
      "loss": 2.6554,
      "step": 935
    },
    {
      "epoch": 0.047,
      "grad_norm": 8.91352653503418,
      "learning_rate": 4.695e-05,
      "loss": 2.5889,
      "step": 940
    },
    {
      "epoch": 0.04725,
      "grad_norm": 7.630932807922363,
      "learning_rate": 4.72e-05,
      "loss": 2.772,
      "step": 945
    },
    {
      "epoch": 0.0475,
      "grad_norm": 9.222395896911621,
      "learning_rate": 4.745e-05,
      "loss": 2.4911,
      "step": 950
    },
    {
      "epoch": 0.04775,
      "grad_norm": 9.079864501953125,
      "learning_rate": 4.77e-05,
      "loss": 2.7481,
      "step": 955
    },
    {
      "epoch": 0.048,
      "grad_norm": 8.932063102722168,
      "learning_rate": 4.795e-05,
      "loss": 2.8147,
      "step": 960
    },
    {
      "epoch": 0.04825,
      "grad_norm": 11.424839973449707,
      "learning_rate": 4.82e-05,
      "loss": 2.9676,
      "step": 965
    },
    {
      "epoch": 0.0485,
      "grad_norm": 8.192866325378418,
      "learning_rate": 4.845e-05,
      "loss": 2.658,
      "step": 970
    },
    {
      "epoch": 0.04875,
      "grad_norm": 6.8061017990112305,
      "learning_rate": 4.87e-05,
      "loss": 2.653,
      "step": 975
    },
    {
      "epoch": 0.049,
      "grad_norm": 8.257204055786133,
      "learning_rate": 4.8950000000000004e-05,
      "loss": 2.7295,
      "step": 980
    },
    {
      "epoch": 0.04925,
      "grad_norm": 10.222881317138672,
      "learning_rate": 4.92e-05,
      "loss": 2.6743,
      "step": 985
    },
    {
      "epoch": 0.0495,
      "grad_norm": 14.0502290725708,
      "learning_rate": 4.945e-05,
      "loss": 2.7046,
      "step": 990
    },
    {
      "epoch": 0.04975,
      "grad_norm": 7.541602611541748,
      "learning_rate": 4.97e-05,
      "loss": 2.4084,
      "step": 995
    },
    {
      "epoch": 0.05,
      "grad_norm": 11.24448013305664,
      "learning_rate": 4.995e-05,
      "loss": 2.6911,
      "step": 1000
    },
    {
      "epoch": 0.05,
      "eval_loss": 2.4175193309783936,
      "eval_runtime": 5.7662,
      "eval_samples_per_second": 177.588,
      "eval_steps_per_second": 22.198,
      "step": 1000
    },
    {
      "epoch": 0.05025,
      "grad_norm": 8.609549522399902,
      "learning_rate": 4.9999994532075326e-05,
      "loss": 2.7864,
      "step": 1005
    },
    {
      "epoch": 0.0505,
      "grad_norm": 7.862979888916016,
      "learning_rate": 4.9999972318635423e-05,
      "loss": 2.6742,
      "step": 1010
    },
    {
      "epoch": 0.05075,
      "grad_norm": 8.007709503173828,
      "learning_rate": 4.9999933017950186e-05,
      "loss": 2.8015,
      "step": 1015
    },
    {
      "epoch": 0.051,
      "grad_norm": 7.492678165435791,
      "learning_rate": 4.999987663004646e-05,
      "loss": 2.6813,
      "step": 1020
    },
    {
      "epoch": 0.05125,
      "grad_norm": 7.498814105987549,
      "learning_rate": 4.999980315496279e-05,
      "loss": 2.7485,
      "step": 1025
    },
    {
      "epoch": 0.0515,
      "grad_norm": 7.73525857925415,
      "learning_rate": 4.9999712592749395e-05,
      "loss": 2.7352,
      "step": 1030
    },
    {
      "epoch": 0.05175,
      "grad_norm": 8.994585037231445,
      "learning_rate": 4.999960494346818e-05,
      "loss": 2.7889,
      "step": 1035
    },
    {
      "epoch": 0.052,
      "grad_norm": 7.830686092376709,
      "learning_rate": 4.999948020719272e-05,
      "loss": 2.444,
      "step": 1040
    },
    {
      "epoch": 0.05225,
      "grad_norm": 6.44850492477417,
      "learning_rate": 4.999933838400827e-05,
      "loss": 2.7074,
      "step": 1045
    },
    {
      "epoch": 0.0525,
      "grad_norm": 6.484663486480713,
      "learning_rate": 4.999917947401176e-05,
      "loss": 2.6239,
      "step": 1050
    },
    {
      "epoch": 0.05275,
      "grad_norm": 8.852861404418945,
      "learning_rate": 4.999900347731181e-05,
      "loss": 2.8347,
      "step": 1055
    },
    {
      "epoch": 0.053,
      "grad_norm": 6.503540515899658,
      "learning_rate": 4.9998810394028716e-05,
      "loss": 2.596,
      "step": 1060
    },
    {
      "epoch": 0.05325,
      "grad_norm": 7.2051591873168945,
      "learning_rate": 4.999860022429443e-05,
      "loss": 2.6731,
      "step": 1065
    },
    {
      "epoch": 0.0535,
      "grad_norm": 7.628105640411377,
      "learning_rate": 4.999837296825263e-05,
      "loss": 2.6433,
      "step": 1070
    },
    {
      "epoch": 0.05375,
      "grad_norm": 6.350590229034424,
      "learning_rate": 4.999812862605861e-05,
      "loss": 2.6076,
      "step": 1075
    },
    {
      "epoch": 0.054,
      "grad_norm": 6.814349174499512,
      "learning_rate": 4.99978671978794e-05,
      "loss": 2.3428,
      "step": 1080
    },
    {
      "epoch": 0.05425,
      "grad_norm": 8.602519035339355,
      "learning_rate": 4.9997588683893674e-05,
      "loss": 2.4464,
      "step": 1085
    },
    {
      "epoch": 0.0545,
      "grad_norm": 7.379042625427246,
      "learning_rate": 4.99972930842918e-05,
      "loss": 2.7909,
      "step": 1090
    },
    {
      "epoch": 0.05475,
      "grad_norm": 7.229223728179932,
      "learning_rate": 4.999698039927581e-05,
      "loss": 2.9334,
      "step": 1095
    },
    {
      "epoch": 0.055,
      "grad_norm": 9.514841079711914,
      "learning_rate": 4.999665062905942e-05,
      "loss": 2.6516,
      "step": 1100
    },
    {
      "epoch": 0.05525,
      "grad_norm": 5.746400833129883,
      "learning_rate": 4.999630377386803e-05,
      "loss": 2.6912,
      "step": 1105
    },
    {
      "epoch": 0.0555,
      "grad_norm": 7.890767574310303,
      "learning_rate": 4.999593983393872e-05,
      "loss": 2.5869,
      "step": 1110
    },
    {
      "epoch": 0.05575,
      "grad_norm": 6.931436061859131,
      "learning_rate": 4.999555880952023e-05,
      "loss": 2.6624,
      "step": 1115
    },
    {
      "epoch": 0.056,
      "grad_norm": 7.587721347808838,
      "learning_rate": 4.9995160700872976e-05,
      "loss": 2.4679,
      "step": 1120
    },
    {
      "epoch": 0.05625,
      "grad_norm": 8.747061729431152,
      "learning_rate": 4.999474550826909e-05,
      "loss": 2.872,
      "step": 1125
    },
    {
      "epoch": 0.0565,
      "grad_norm": 6.569738388061523,
      "learning_rate": 4.999431323199232e-05,
      "loss": 2.5764,
      "step": 1130
    },
    {
      "epoch": 0.05675,
      "grad_norm": 8.023942947387695,
      "learning_rate": 4.999386387233815e-05,
      "loss": 2.8402,
      "step": 1135
    },
    {
      "epoch": 0.057,
      "grad_norm": 6.288106918334961,
      "learning_rate": 4.9993397429613695e-05,
      "loss": 2.4333,
      "step": 1140
    },
    {
      "epoch": 0.05725,
      "grad_norm": 6.734976768493652,
      "learning_rate": 4.999291390413777e-05,
      "loss": 2.7326,
      "step": 1145
    },
    {
      "epoch": 0.0575,
      "grad_norm": 5.638284683227539,
      "learning_rate": 4.999241329624087e-05,
      "loss": 2.606,
      "step": 1150
    },
    {
      "epoch": 0.05775,
      "grad_norm": 7.081733226776123,
      "learning_rate": 4.999189560626514e-05,
      "loss": 2.4289,
      "step": 1155
    },
    {
      "epoch": 0.058,
      "grad_norm": 5.838747024536133,
      "learning_rate": 4.999136083456441e-05,
      "loss": 2.4324,
      "step": 1160
    },
    {
      "epoch": 0.05825,
      "grad_norm": 8.448747634887695,
      "learning_rate": 4.999080898150422e-05,
      "loss": 2.3449,
      "step": 1165
    },
    {
      "epoch": 0.0585,
      "grad_norm": 6.965798377990723,
      "learning_rate": 4.9990240047461733e-05,
      "loss": 2.7727,
      "step": 1170
    },
    {
      "epoch": 0.05875,
      "grad_norm": 10.61223316192627,
      "learning_rate": 4.998965403282583e-05,
      "loss": 2.5368,
      "step": 1175
    },
    {
      "epoch": 0.059,
      "grad_norm": 7.4553422927856445,
      "learning_rate": 4.998905093799702e-05,
      "loss": 2.7819,
      "step": 1180
    },
    {
      "epoch": 0.05925,
      "grad_norm": 6.595829963684082,
      "learning_rate": 4.998843076338753e-05,
      "loss": 2.4122,
      "step": 1185
    },
    {
      "epoch": 0.0595,
      "grad_norm": 8.18661880493164,
      "learning_rate": 4.9987793509421244e-05,
      "loss": 2.7857,
      "step": 1190
    },
    {
      "epoch": 0.05975,
      "grad_norm": 10.484152793884277,
      "learning_rate": 4.998713917653371e-05,
      "loss": 2.5858,
      "step": 1195
    },
    {
      "epoch": 0.06,
      "grad_norm": 8.01288890838623,
      "learning_rate": 4.998646776517216e-05,
      "loss": 2.7596,
      "step": 1200
    },
    {
      "epoch": 0.06025,
      "grad_norm": 6.758542537689209,
      "learning_rate": 4.998577927579551e-05,
      "loss": 2.5247,
      "step": 1205
    },
    {
      "epoch": 0.0605,
      "grad_norm": 8.694050788879395,
      "learning_rate": 4.998507370887433e-05,
      "loss": 2.9147,
      "step": 1210
    },
    {
      "epoch": 0.06075,
      "grad_norm": 7.824736595153809,
      "learning_rate": 4.998435106489086e-05,
      "loss": 2.4342,
      "step": 1215
    },
    {
      "epoch": 0.061,
      "grad_norm": 7.1482462882995605,
      "learning_rate": 4.9983611344339016e-05,
      "loss": 2.5698,
      "step": 1220
    },
    {
      "epoch": 0.06125,
      "grad_norm": 9.1261568069458,
      "learning_rate": 4.998285454772441e-05,
      "loss": 2.6239,
      "step": 1225
    },
    {
      "epoch": 0.0615,
      "grad_norm": 6.233458518981934,
      "learning_rate": 4.998208067556429e-05,
      "loss": 2.453,
      "step": 1230
    },
    {
      "epoch": 0.06175,
      "grad_norm": 8.831205368041992,
      "learning_rate": 4.99812897283876e-05,
      "loss": 2.4826,
      "step": 1235
    },
    {
      "epoch": 0.062,
      "grad_norm": 7.7438273429870605,
      "learning_rate": 4.998048170673494e-05,
      "loss": 2.5617,
      "step": 1240
    },
    {
      "epoch": 0.06225,
      "grad_norm": 8.511394500732422,
      "learning_rate": 4.9979656611158576e-05,
      "loss": 2.6353,
      "step": 1245
    },
    {
      "epoch": 0.0625,
      "grad_norm": 6.550946235656738,
      "learning_rate": 4.997881444222247e-05,
      "loss": 2.2987,
      "step": 1250
    },
    {
      "epoch": 0.06275,
      "grad_norm": 8.651806831359863,
      "learning_rate": 4.997795520050223e-05,
      "loss": 2.5039,
      "step": 1255
    },
    {
      "epoch": 0.063,
      "grad_norm": 7.555785179138184,
      "learning_rate": 4.9977078886585137e-05,
      "loss": 2.6486,
      "step": 1260
    },
    {
      "epoch": 0.06325,
      "grad_norm": 7.102833271026611,
      "learning_rate": 4.9976185501070136e-05,
      "loss": 2.6749,
      "step": 1265
    },
    {
      "epoch": 0.0635,
      "grad_norm": 5.952548980712891,
      "learning_rate": 4.997527504456787e-05,
      "loss": 2.333,
      "step": 1270
    },
    {
      "epoch": 0.06375,
      "grad_norm": 6.400311470031738,
      "learning_rate": 4.997434751770061e-05,
      "loss": 2.3535,
      "step": 1275
    },
    {
      "epoch": 0.064,
      "grad_norm": 8.069629669189453,
      "learning_rate": 4.9973402921102316e-05,
      "loss": 2.365,
      "step": 1280
    },
    {
      "epoch": 0.06425,
      "grad_norm": 9.459650039672852,
      "learning_rate": 4.9972441255418606e-05,
      "loss": 2.7751,
      "step": 1285
    },
    {
      "epoch": 0.0645,
      "grad_norm": 6.218774795532227,
      "learning_rate": 4.997146252130678e-05,
      "loss": 2.4945,
      "step": 1290
    },
    {
      "epoch": 0.06475,
      "grad_norm": 8.80373764038086,
      "learning_rate": 4.997046671943578e-05,
      "loss": 2.7559,
      "step": 1295
    },
    {
      "epoch": 0.065,
      "grad_norm": 8.890571594238281,
      "learning_rate": 4.9969453850486234e-05,
      "loss": 2.7944,
      "step": 1300
    },
    {
      "epoch": 0.06525,
      "grad_norm": 8.164043426513672,
      "learning_rate": 4.996842391515044e-05,
      "loss": 2.8061,
      "step": 1305
    },
    {
      "epoch": 0.0655,
      "grad_norm": 8.019671440124512,
      "learning_rate": 4.9967376914132336e-05,
      "loss": 2.5668,
      "step": 1310
    },
    {
      "epoch": 0.06575,
      "grad_norm": 6.171674728393555,
      "learning_rate": 4.996631284814754e-05,
      "loss": 2.5917,
      "step": 1315
    },
    {
      "epoch": 0.066,
      "grad_norm": 8.697516441345215,
      "learning_rate": 4.996523171792332e-05,
      "loss": 2.4386,
      "step": 1320
    },
    {
      "epoch": 0.06625,
      "grad_norm": 6.031949996948242,
      "learning_rate": 4.9964133524198633e-05,
      "loss": 2.747,
      "step": 1325
    },
    {
      "epoch": 0.0665,
      "grad_norm": 6.0758843421936035,
      "learning_rate": 4.996301826772408e-05,
      "loss": 2.6678,
      "step": 1330
    },
    {
      "epoch": 0.06675,
      "grad_norm": 8.492558479309082,
      "learning_rate": 4.9961885949261936e-05,
      "loss": 2.5281,
      "step": 1335
    },
    {
      "epoch": 0.067,
      "grad_norm": 6.530932426452637,
      "learning_rate": 4.996073656958611e-05,
      "loss": 2.6862,
      "step": 1340
    },
    {
      "epoch": 0.06725,
      "grad_norm": 6.325329780578613,
      "learning_rate": 4.995957012948221e-05,
      "loss": 2.3593,
      "step": 1345
    },
    {
      "epoch": 0.0675,
      "grad_norm": 7.668563365936279,
      "learning_rate": 4.995838662974748e-05,
      "loss": 2.4688,
      "step": 1350
    },
    {
      "epoch": 0.06775,
      "grad_norm": 6.179581165313721,
      "learning_rate": 4.9957186071190834e-05,
      "loss": 2.4937,
      "step": 1355
    },
    {
      "epoch": 0.068,
      "grad_norm": 4.42112398147583,
      "learning_rate": 4.995596845463284e-05,
      "loss": 2.6195,
      "step": 1360
    },
    {
      "epoch": 0.06825,
      "grad_norm": 6.450810432434082,
      "learning_rate": 4.995473378090573e-05,
      "loss": 2.5774,
      "step": 1365
    },
    {
      "epoch": 0.0685,
      "grad_norm": 7.021387577056885,
      "learning_rate": 4.995348205085339e-05,
      "loss": 2.6049,
      "step": 1370
    },
    {
      "epoch": 0.06875,
      "grad_norm": 6.046529769897461,
      "learning_rate": 4.995221326533136e-05,
      "loss": 2.4743,
      "step": 1375
    },
    {
      "epoch": 0.069,
      "grad_norm": 4.26240348815918,
      "learning_rate": 4.995092742520686e-05,
      "loss": 2.3573,
      "step": 1380
    },
    {
      "epoch": 0.06925,
      "grad_norm": 6.638156890869141,
      "learning_rate": 4.994962453135873e-05,
      "loss": 2.5346,
      "step": 1385
    },
    {
      "epoch": 0.0695,
      "grad_norm": 4.53425931930542,
      "learning_rate": 4.994830458467749e-05,
      "loss": 2.3024,
      "step": 1390
    },
    {
      "epoch": 0.06975,
      "grad_norm": 4.408112049102783,
      "learning_rate": 4.994696758606532e-05,
      "loss": 2.421,
      "step": 1395
    },
    {
      "epoch": 0.07,
      "grad_norm": 7.036376476287842,
      "learning_rate": 4.994561353643604e-05,
      "loss": 2.5812,
      "step": 1400
    },
    {
      "epoch": 0.07025,
      "grad_norm": 5.8998870849609375,
      "learning_rate": 4.994424243671513e-05,
      "loss": 2.1077,
      "step": 1405
    },
    {
      "epoch": 0.0705,
      "grad_norm": 4.705588340759277,
      "learning_rate": 4.9942854287839725e-05,
      "loss": 2.336,
      "step": 1410
    },
    {
      "epoch": 0.07075,
      "grad_norm": 5.795966625213623,
      "learning_rate": 4.994144909075861e-05,
      "loss": 2.2226,
      "step": 1415
    },
    {
      "epoch": 0.071,
      "grad_norm": 6.124486923217773,
      "learning_rate": 4.994002684643223e-05,
      "loss": 2.139,
      "step": 1420
    },
    {
      "epoch": 0.07125,
      "grad_norm": 5.057814121246338,
      "learning_rate": 4.9938587555832664e-05,
      "loss": 2.2673,
      "step": 1425
    },
    {
      "epoch": 0.0715,
      "grad_norm": 6.542652606964111,
      "learning_rate": 4.993713121994367e-05,
      "loss": 2.4258,
      "step": 1430
    },
    {
      "epoch": 0.07175,
      "grad_norm": 6.44785737991333,
      "learning_rate": 4.993565783976062e-05,
      "loss": 2.67,
      "step": 1435
    },
    {
      "epoch": 0.072,
      "grad_norm": 6.577790260314941,
      "learning_rate": 4.993416741629057e-05,
      "loss": 2.4513,
      "step": 1440
    },
    {
      "epoch": 0.07225,
      "grad_norm": 5.430110931396484,
      "learning_rate": 4.993265995055221e-05,
      "loss": 2.3653,
      "step": 1445
    },
    {
      "epoch": 0.0725,
      "grad_norm": 4.890500068664551,
      "learning_rate": 4.993113544357586e-05,
      "loss": 2.1153,
      "step": 1450
    },
    {
      "epoch": 0.07275,
      "grad_norm": 5.753777980804443,
      "learning_rate": 4.9929593896403534e-05,
      "loss": 2.3844,
      "step": 1455
    },
    {
      "epoch": 0.073,
      "grad_norm": 5.502735614776611,
      "learning_rate": 4.992803531008885e-05,
      "loss": 2.3895,
      "step": 1460
    },
    {
      "epoch": 0.07325,
      "grad_norm": 7.280923366546631,
      "learning_rate": 4.992645968569709e-05,
      "loss": 2.4439,
      "step": 1465
    },
    {
      "epoch": 0.0735,
      "grad_norm": 7.758944034576416,
      "learning_rate": 4.992486702430518e-05,
      "loss": 2.1209,
      "step": 1470
    },
    {
      "epoch": 0.07375,
      "grad_norm": 4.938582897186279,
      "learning_rate": 4.9923257327001676e-05,
      "loss": 2.5018,
      "step": 1475
    },
    {
      "epoch": 0.074,
      "grad_norm": 5.637670040130615,
      "learning_rate": 4.9921630594886806e-05,
      "loss": 2.2106,
      "step": 1480
    },
    {
      "epoch": 0.07425,
      "grad_norm": 5.659066200256348,
      "learning_rate": 4.991998682907243e-05,
      "loss": 2.3586,
      "step": 1485
    },
    {
      "epoch": 0.0745,
      "grad_norm": 9.093928337097168,
      "learning_rate": 4.991832603068203e-05,
      "loss": 2.4766,
      "step": 1490
    },
    {
      "epoch": 0.07475,
      "grad_norm": 3.886587142944336,
      "learning_rate": 4.991664820085074e-05,
      "loss": 2.1712,
      "step": 1495
    },
    {
      "epoch": 0.075,
      "grad_norm": 4.213752746582031,
      "learning_rate": 4.9914953340725377e-05,
      "loss": 2.052,
      "step": 1500
    },
    {
      "epoch": 0.075,
      "eval_loss": 2.246741771697998,
      "eval_runtime": 5.4585,
      "eval_samples_per_second": 187.596,
      "eval_steps_per_second": 23.45,
      "step": 1500
    },
    {
      "epoch": 0.07525,
      "grad_norm": 6.139965057373047,
      "learning_rate": 4.991324145146433e-05,
      "loss": 2.4886,
      "step": 1505
    },
    {
      "epoch": 0.0755,
      "grad_norm": 4.688581943511963,
      "learning_rate": 4.991151253423767e-05,
      "loss": 2.3819,
      "step": 1510
    },
    {
      "epoch": 0.07575,
      "grad_norm": 5.951380252838135,
      "learning_rate": 4.9909766590227094e-05,
      "loss": 2.3575,
      "step": 1515
    },
    {
      "epoch": 0.076,
      "grad_norm": 5.881429195404053,
      "learning_rate": 4.9908003620625945e-05,
      "loss": 2.476,
      "step": 1520
    },
    {
      "epoch": 0.07625,
      "grad_norm": 4.62600040435791,
      "learning_rate": 4.990622362663918e-05,
      "loss": 2.1736,
      "step": 1525
    },
    {
      "epoch": 0.0765,
      "grad_norm": 5.484920501708984,
      "learning_rate": 4.9904426609483425e-05,
      "loss": 2.2,
      "step": 1530
    },
    {
      "epoch": 0.07675,
      "grad_norm": 5.999587059020996,
      "learning_rate": 4.990261257038691e-05,
      "loss": 2.1927,
      "step": 1535
    },
    {
      "epoch": 0.077,
      "grad_norm": 5.995348930358887,
      "learning_rate": 4.9900781510589526e-05,
      "loss": 2.039,
      "step": 1540
    },
    {
      "epoch": 0.07725,
      "grad_norm": 4.862228870391846,
      "learning_rate": 4.9898933431342775e-05,
      "loss": 2.2454,
      "step": 1545
    },
    {
      "epoch": 0.0775,
      "grad_norm": 5.674276828765869,
      "learning_rate": 4.989706833390981e-05,
      "loss": 2.2467,
      "step": 1550
    },
    {
      "epoch": 0.07775,
      "grad_norm": 5.125243663787842,
      "learning_rate": 4.9895186219565405e-05,
      "loss": 2.075,
      "step": 1555
    },
    {
      "epoch": 0.078,
      "grad_norm": 5.135522365570068,
      "learning_rate": 4.9893287089595964e-05,
      "loss": 2.1873,
      "step": 1560
    },
    {
      "epoch": 0.07825,
      "grad_norm": 4.225062370300293,
      "learning_rate": 4.989137094529953e-05,
      "loss": 2.0503,
      "step": 1565
    },
    {
      "epoch": 0.0785,
      "grad_norm": 6.647088050842285,
      "learning_rate": 4.9889437787985764e-05,
      "loss": 2.1406,
      "step": 1570
    },
    {
      "epoch": 0.07875,
      "grad_norm": 4.897021293640137,
      "learning_rate": 4.988748761897597e-05,
      "loss": 2.0944,
      "step": 1575
    },
    {
      "epoch": 0.079,
      "grad_norm": 8.786798477172852,
      "learning_rate": 4.988552043960305e-05,
      "loss": 2.1823,
      "step": 1580
    },
    {
      "epoch": 0.07925,
      "grad_norm": 5.530460357666016,
      "learning_rate": 4.988353625121158e-05,
      "loss": 2.2284,
      "step": 1585
    },
    {
      "epoch": 0.0795,
      "grad_norm": 5.568400859832764,
      "learning_rate": 4.988153505515771e-05,
      "loss": 2.1478,
      "step": 1590
    },
    {
      "epoch": 0.07975,
      "grad_norm": 6.6827006340026855,
      "learning_rate": 4.987951685280925e-05,
      "loss": 2.2234,
      "step": 1595
    },
    {
      "epoch": 0.08,
      "grad_norm": 3.969071865081787,
      "learning_rate": 4.987748164554563e-05,
      "loss": 2.1231,
      "step": 1600
    },
    {
      "epoch": 0.08025,
      "grad_norm": 4.995045185089111,
      "learning_rate": 4.9875429434757874e-05,
      "loss": 2.2826,
      "step": 1605
    },
    {
      "epoch": 0.0805,
      "grad_norm": 5.364326477050781,
      "learning_rate": 4.987336022184866e-05,
      "loss": 2.0565,
      "step": 1610
    },
    {
      "epoch": 0.08075,
      "grad_norm": 8.229832649230957,
      "learning_rate": 4.9871274008232286e-05,
      "loss": 2.2194,
      "step": 1615
    },
    {
      "epoch": 0.081,
      "grad_norm": 4.753478050231934,
      "learning_rate": 4.986917079533465e-05,
      "loss": 2.205,
      "step": 1620
    },
    {
      "epoch": 0.08125,
      "grad_norm": 6.085668563842773,
      "learning_rate": 4.9867050584593265e-05,
      "loss": 2.33,
      "step": 1625
    },
    {
      "epoch": 0.0815,
      "grad_norm": 4.818737983703613,
      "learning_rate": 4.98649133774573e-05,
      "loss": 2.2019,
      "step": 1630
    },
    {
      "epoch": 0.08175,
      "grad_norm": 4.622413158416748,
      "learning_rate": 4.98627591753875e-05,
      "loss": 2.1414,
      "step": 1635
    },
    {
      "epoch": 0.082,
      "grad_norm": 4.422964096069336,
      "learning_rate": 4.9860587979856244e-05,
      "loss": 2.2181,
      "step": 1640
    },
    {
      "epoch": 0.08225,
      "grad_norm": 4.814355850219727,
      "learning_rate": 4.9858399792347525e-05,
      "loss": 2.1227,
      "step": 1645
    },
    {
      "epoch": 0.0825,
      "grad_norm": 4.661029815673828,
      "learning_rate": 4.9856194614356956e-05,
      "loss": 2.2181,
      "step": 1650
    },
    {
      "epoch": 0.08275,
      "grad_norm": 5.645421504974365,
      "learning_rate": 4.985397244739174e-05,
      "loss": 2.1151,
      "step": 1655
    },
    {
      "epoch": 0.083,
      "grad_norm": 5.859399318695068,
      "learning_rate": 4.985173329297071e-05,
      "loss": 2.1479,
      "step": 1660
    },
    {
      "epoch": 0.08325,
      "grad_norm": 5.9802703857421875,
      "learning_rate": 4.984947715262433e-05,
      "loss": 2.1228,
      "step": 1665
    },
    {
      "epoch": 0.0835,
      "grad_norm": 7.0083909034729,
      "learning_rate": 4.9847204027894615e-05,
      "loss": 2.3826,
      "step": 1670
    },
    {
      "epoch": 0.08375,
      "grad_norm": 4.706477642059326,
      "learning_rate": 4.984491392033524e-05,
      "loss": 2.0562,
      "step": 1675
    },
    {
      "epoch": 0.084,
      "grad_norm": 6.368319034576416,
      "learning_rate": 4.984260683151149e-05,
      "loss": 2.0286,
      "step": 1680
    },
    {
      "epoch": 0.08425,
      "grad_norm": 3.658266544342041,
      "learning_rate": 4.984028276300021e-05,
      "loss": 1.9175,
      "step": 1685
    },
    {
      "epoch": 0.0845,
      "grad_norm": 6.812838554382324,
      "learning_rate": 4.9837941716389904e-05,
      "loss": 2.2736,
      "step": 1690
    },
    {
      "epoch": 0.08475,
      "grad_norm": 4.863814830780029,
      "learning_rate": 4.983558369328063e-05,
      "loss": 2.0173,
      "step": 1695
    },
    {
      "epoch": 0.085,
      "grad_norm": 5.210622310638428,
      "learning_rate": 4.983320869528409e-05,
      "loss": 1.9652,
      "step": 1700
    },
    {
      "epoch": 0.08525,
      "grad_norm": 5.141545295715332,
      "learning_rate": 4.983081672402358e-05,
      "loss": 2.0861,
      "step": 1705
    },
    {
      "epoch": 0.0855,
      "grad_norm": 4.797116756439209,
      "learning_rate": 4.9828407781133965e-05,
      "loss": 1.9864,
      "step": 1710
    },
    {
      "epoch": 0.08575,
      "grad_norm": 4.749032020568848,
      "learning_rate": 4.9825981868261756e-05,
      "loss": 2.0684,
      "step": 1715
    },
    {
      "epoch": 0.086,
      "grad_norm": 5.164583206176758,
      "learning_rate": 4.982353898706503e-05,
      "loss": 2.235,
      "step": 1720
    },
    {
      "epoch": 0.08625,
      "grad_norm": 4.232330799102783,
      "learning_rate": 4.982107913921349e-05,
      "loss": 1.9094,
      "step": 1725
    },
    {
      "epoch": 0.0865,
      "grad_norm": 4.071442127227783,
      "learning_rate": 4.9818602326388395e-05,
      "loss": 1.9239,
      "step": 1730
    },
    {
      "epoch": 0.08675,
      "grad_norm": 4.3470563888549805,
      "learning_rate": 4.9816108550282646e-05,
      "loss": 1.9666,
      "step": 1735
    },
    {
      "epoch": 0.087,
      "grad_norm": 7.460428714752197,
      "learning_rate": 4.98135978126007e-05,
      "loss": 2.2428,
      "step": 1740
    },
    {
      "epoch": 0.08725,
      "grad_norm": 5.387420654296875,
      "learning_rate": 4.9811070115058624e-05,
      "loss": 2.0599,
      "step": 1745
    },
    {
      "epoch": 0.0875,
      "grad_norm": 5.970615386962891,
      "learning_rate": 4.980852545938408e-05,
      "loss": 2.2848,
      "step": 1750
    },
    {
      "epoch": 0.08775,
      "grad_norm": 5.8011016845703125,
      "learning_rate": 4.980596384731632e-05,
      "loss": 2.1212,
      "step": 1755
    },
    {
      "epoch": 0.088,
      "grad_norm": 5.344608306884766,
      "learning_rate": 4.9803385280606176e-05,
      "loss": 2.0501,
      "step": 1760
    },
    {
      "epoch": 0.08825,
      "grad_norm": 5.118975639343262,
      "learning_rate": 4.980078976101607e-05,
      "loss": 2.1225,
      "step": 1765
    },
    {
      "epoch": 0.0885,
      "grad_norm": 4.45448112487793,
      "learning_rate": 4.9798177290320025e-05,
      "loss": 1.9087,
      "step": 1770
    },
    {
      "epoch": 0.08875,
      "grad_norm": 4.421710014343262,
      "learning_rate": 4.979554787030363e-05,
      "loss": 2.1416,
      "step": 1775
    },
    {
      "epoch": 0.089,
      "grad_norm": 5.052739143371582,
      "learning_rate": 4.9792901502764075e-05,
      "loss": 2.0996,
      "step": 1780
    },
    {
      "epoch": 0.08925,
      "grad_norm": 5.099411964416504,
      "learning_rate": 4.9790238189510124e-05,
      "loss": 2.1544,
      "step": 1785
    },
    {
      "epoch": 0.0895,
      "grad_norm": 7.27427339553833,
      "learning_rate": 4.978755793236213e-05,
      "loss": 2.1682,
      "step": 1790
    },
    {
      "epoch": 0.08975,
      "grad_norm": 5.510068416595459,
      "learning_rate": 4.9784860733152026e-05,
      "loss": 1.9315,
      "step": 1795
    },
    {
      "epoch": 0.09,
      "grad_norm": 5.214048385620117,
      "learning_rate": 4.978214659372331e-05,
      "loss": 2.1045,
      "step": 1800
    },
    {
      "epoch": 0.09025,
      "grad_norm": 4.70754337310791,
      "learning_rate": 4.977941551593109e-05,
      "loss": 2.1765,
      "step": 1805
    },
    {
      "epoch": 0.0905,
      "grad_norm": 5.353738784790039,
      "learning_rate": 4.977666750164202e-05,
      "loss": 1.9423,
      "step": 1810
    },
    {
      "epoch": 0.09075,
      "grad_norm": 5.094300270080566,
      "learning_rate": 4.977390255273433e-05,
      "loss": 1.7676,
      "step": 1815
    },
    {
      "epoch": 0.091,
      "grad_norm": 4.465933322906494,
      "learning_rate": 4.977112067109785e-05,
      "loss": 2.0102,
      "step": 1820
    },
    {
      "epoch": 0.09125,
      "grad_norm": 4.060147762298584,
      "learning_rate": 4.9768321858633985e-05,
      "loss": 1.9353,
      "step": 1825
    },
    {
      "epoch": 0.0915,
      "grad_norm": 4.8424224853515625,
      "learning_rate": 4.9765506117255665e-05,
      "loss": 2.2785,
      "step": 1830
    },
    {
      "epoch": 0.09175,
      "grad_norm": 4.071983814239502,
      "learning_rate": 4.9762673448887445e-05,
      "loss": 2.2767,
      "step": 1835
    },
    {
      "epoch": 0.092,
      "grad_norm": 4.174991130828857,
      "learning_rate": 4.9759823855465425e-05,
      "loss": 1.9738,
      "step": 1840
    },
    {
      "epoch": 0.09225,
      "grad_norm": 5.685105800628662,
      "learning_rate": 4.975695733893726e-05,
      "loss": 2.2084,
      "step": 1845
    },
    {
      "epoch": 0.0925,
      "grad_norm": 5.597139835357666,
      "learning_rate": 4.975407390126221e-05,
      "loss": 1.9774,
      "step": 1850
    },
    {
      "epoch": 0.09275,
      "grad_norm": 4.555553436279297,
      "learning_rate": 4.975117354441106e-05,
      "loss": 2.1291,
      "step": 1855
    },
    {
      "epoch": 0.093,
      "grad_norm": 6.357362270355225,
      "learning_rate": 4.974825627036618e-05,
      "loss": 2.059,
      "step": 1860
    },
    {
      "epoch": 0.09325,
      "grad_norm": 4.2658610343933105,
      "learning_rate": 4.9745322081121504e-05,
      "loss": 2.2308,
      "step": 1865
    },
    {
      "epoch": 0.0935,
      "grad_norm": 6.061540603637695,
      "learning_rate": 4.974237097868252e-05,
      "loss": 2.2617,
      "step": 1870
    },
    {
      "epoch": 0.09375,
      "grad_norm": 4.45533561706543,
      "learning_rate": 4.9739402965066276e-05,
      "loss": 2.2596,
      "step": 1875
    },
    {
      "epoch": 0.094,
      "grad_norm": 4.3768439292907715,
      "learning_rate": 4.9736418042301384e-05,
      "loss": 2.0163,
      "step": 1880
    },
    {
      "epoch": 0.09425,
      "grad_norm": 6.143226146697998,
      "learning_rate": 4.973341621242802e-05,
      "loss": 2.1251,
      "step": 1885
    },
    {
      "epoch": 0.0945,
      "grad_norm": 3.8164706230163574,
      "learning_rate": 4.973039747749788e-05,
      "loss": 1.8755,
      "step": 1890
    },
    {
      "epoch": 0.09475,
      "grad_norm": 4.651381015777588,
      "learning_rate": 4.9727361839574274e-05,
      "loss": 2.2342,
      "step": 1895
    },
    {
      "epoch": 0.095,
      "grad_norm": 6.655630588531494,
      "learning_rate": 4.9724309300732006e-05,
      "loss": 2.2108,
      "step": 1900
    },
    {
      "epoch": 0.09525,
      "grad_norm": 5.68287467956543,
      "learning_rate": 4.972123986305747e-05,
      "loss": 2.0992,
      "step": 1905
    },
    {
      "epoch": 0.0955,
      "grad_norm": 4.683038711547852,
      "learning_rate": 4.9718153528648596e-05,
      "loss": 2.1765,
      "step": 1910
    },
    {
      "epoch": 0.09575,
      "grad_norm": 6.263393878936768,
      "learning_rate": 4.9715050299614863e-05,
      "loss": 2.1101,
      "step": 1915
    },
    {
      "epoch": 0.096,
      "grad_norm": 4.754069805145264,
      "learning_rate": 4.9711930178077296e-05,
      "loss": 2.079,
      "step": 1920
    },
    {
      "epoch": 0.09625,
      "grad_norm": 5.009681701660156,
      "learning_rate": 4.970879316616848e-05,
      "loss": 2.0409,
      "step": 1925
    },
    {
      "epoch": 0.0965,
      "grad_norm": 4.133429050445557,
      "learning_rate": 4.970563926603252e-05,
      "loss": 1.9177,
      "step": 1930
    },
    {
      "epoch": 0.09675,
      "grad_norm": 3.9272971153259277,
      "learning_rate": 4.970246847982508e-05,
      "loss": 1.9716,
      "step": 1935
    },
    {
      "epoch": 0.097,
      "grad_norm": 4.353458404541016,
      "learning_rate": 4.9699280809713366e-05,
      "loss": 1.9733,
      "step": 1940
    },
    {
      "epoch": 0.09725,
      "grad_norm": 4.654109001159668,
      "learning_rate": 4.969607625787612e-05,
      "loss": 1.8873,
      "step": 1945
    },
    {
      "epoch": 0.0975,
      "grad_norm": 10.7420015335083,
      "learning_rate": 4.969285482650362e-05,
      "loss": 2.1109,
      "step": 1950
    },
    {
      "epoch": 0.09775,
      "grad_norm": 7.7290215492248535,
      "learning_rate": 4.968961651779769e-05,
      "loss": 2.0217,
      "step": 1955
    },
    {
      "epoch": 0.098,
      "grad_norm": 4.220831394195557,
      "learning_rate": 4.968636133397167e-05,
      "loss": 2.0403,
      "step": 1960
    },
    {
      "epoch": 0.09825,
      "grad_norm": 3.95465087890625,
      "learning_rate": 4.968308927725046e-05,
      "loss": 1.8771,
      "step": 1965
    },
    {
      "epoch": 0.0985,
      "grad_norm": 4.108774662017822,
      "learning_rate": 4.967980034987048e-05,
      "loss": 1.9528,
      "step": 1970
    },
    {
      "epoch": 0.09875,
      "grad_norm": 4.1874799728393555,
      "learning_rate": 4.967649455407968e-05,
      "loss": 1.9751,
      "step": 1975
    },
    {
      "epoch": 0.099,
      "grad_norm": 3.9970755577087402,
      "learning_rate": 4.967317189213753e-05,
      "loss": 1.8458,
      "step": 1980
    },
    {
      "epoch": 0.09925,
      "grad_norm": 8.650774002075195,
      "learning_rate": 4.966983236631505e-05,
      "loss": 2.213,
      "step": 1985
    },
    {
      "epoch": 0.0995,
      "grad_norm": 3.2457568645477295,
      "learning_rate": 4.966647597889477e-05,
      "loss": 1.979,
      "step": 1990
    },
    {
      "epoch": 0.09975,
      "grad_norm": 3.888188123703003,
      "learning_rate": 4.9663102732170754e-05,
      "loss": 1.91,
      "step": 1995
    },
    {
      "epoch": 0.1,
      "grad_norm": 4.122952938079834,
      "learning_rate": 4.9659712628448575e-05,
      "loss": 2.1729,
      "step": 2000
    },
    {
      "epoch": 0.1,
      "eval_loss": 2.2426300048828125,
      "eval_runtime": 5.2481,
      "eval_samples_per_second": 195.118,
      "eval_steps_per_second": 24.39,
      "step": 2000
    },
    {
      "epoch": 0.10025,
      "grad_norm": 5.377037048339844,
      "learning_rate": 4.9656305670045346e-05,
      "loss": 2.1925,
      "step": 2005
    },
    {
      "epoch": 0.1005,
      "grad_norm": 4.610778331756592,
      "learning_rate": 4.965288185928968e-05,
      "loss": 2.1069,
      "step": 2010
    },
    {
      "epoch": 0.10075,
      "grad_norm": 4.876117706298828,
      "learning_rate": 4.9649441198521726e-05,
      "loss": 2.098,
      "step": 2015
    },
    {
      "epoch": 0.101,
      "grad_norm": 5.681414604187012,
      "learning_rate": 4.9645983690093143e-05,
      "loss": 2.1966,
      "step": 2020
    },
    {
      "epoch": 0.10125,
      "grad_norm": 4.77568244934082,
      "learning_rate": 4.964250933636711e-05,
      "loss": 2.1559,
      "step": 2025
    },
    {
      "epoch": 0.1015,
      "grad_norm": 3.8703551292419434,
      "learning_rate": 4.963901813971831e-05,
      "loss": 1.9177,
      "step": 2030
    },
    {
      "epoch": 0.10175,
      "grad_norm": 4.478012561798096,
      "learning_rate": 4.963551010253294e-05,
      "loss": 2.023,
      "step": 2035
    },
    {
      "epoch": 0.102,
      "grad_norm": 5.8118414878845215,
      "learning_rate": 4.963198522720872e-05,
      "loss": 1.991,
      "step": 2040
    },
    {
      "epoch": 0.10225,
      "grad_norm": 4.319365978240967,
      "learning_rate": 4.962844351615485e-05,
      "loss": 2.0088,
      "step": 2045
    },
    {
      "epoch": 0.1025,
      "grad_norm": 3.755974292755127,
      "learning_rate": 4.962488497179209e-05,
      "loss": 1.9244,
      "step": 2050
    },
    {
      "epoch": 0.10275,
      "grad_norm": 4.355856895446777,
      "learning_rate": 4.9621309596552637e-05,
      "loss": 1.9578,
      "step": 2055
    },
    {
      "epoch": 0.103,
      "grad_norm": 4.16625452041626,
      "learning_rate": 4.961771739288024e-05,
      "loss": 2.0999,
      "step": 2060
    },
    {
      "epoch": 0.10325,
      "grad_norm": 4.758518695831299,
      "learning_rate": 4.9614108363230135e-05,
      "loss": 1.7478,
      "step": 2065
    },
    {
      "epoch": 0.1035,
      "grad_norm": 7.987035274505615,
      "learning_rate": 4.9610482510069064e-05,
      "loss": 2.0527,
      "step": 2070
    },
    {
      "epoch": 0.10375,
      "grad_norm": 3.3992745876312256,
      "learning_rate": 4.960683983587526e-05,
      "loss": 1.7293,
      "step": 2075
    },
    {
      "epoch": 0.104,
      "grad_norm": 3.765942096710205,
      "learning_rate": 4.960318034313845e-05,
      "loss": 2.193,
      "step": 2080
    },
    {
      "epoch": 0.10425,
      "grad_norm": 5.917516708374023,
      "learning_rate": 4.959950403435988e-05,
      "loss": 2.1289,
      "step": 2085
    },
    {
      "epoch": 0.1045,
      "grad_norm": 4.340882778167725,
      "learning_rate": 4.9595810912052265e-05,
      "loss": 2.0745,
      "step": 2090
    },
    {
      "epoch": 0.10475,
      "grad_norm": 4.755206108093262,
      "learning_rate": 4.959210097873981e-05,
      "loss": 1.852,
      "step": 2095
    },
    {
      "epoch": 0.105,
      "grad_norm": 4.309403419494629,
      "learning_rate": 4.958837423695822e-05,
      "loss": 2.0184,
      "step": 2100
    },
    {
      "epoch": 0.10525,
      "grad_norm": 7.408356666564941,
      "learning_rate": 4.95846306892547e-05,
      "loss": 2.1755,
      "step": 2105
    },
    {
      "epoch": 0.1055,
      "grad_norm": 5.234712600708008,
      "learning_rate": 4.958087033818792e-05,
      "loss": 1.9699,
      "step": 2110
    },
    {
      "epoch": 0.10575,
      "grad_norm": 6.6951375007629395,
      "learning_rate": 4.957709318632805e-05,
      "loss": 2.0702,
      "step": 2115
    },
    {
      "epoch": 0.106,
      "grad_norm": 4.313019275665283,
      "learning_rate": 4.957329923625674e-05,
      "loss": 1.8655,
      "step": 2120
    },
    {
      "epoch": 0.10625,
      "grad_norm": 3.9118385314941406,
      "learning_rate": 4.956948849056711e-05,
      "loss": 2.0354,
      "step": 2125
    },
    {
      "epoch": 0.1065,
      "grad_norm": 6.7428178787231445,
      "learning_rate": 4.956566095186377e-05,
      "loss": 2.0925,
      "step": 2130
    },
    {
      "epoch": 0.10675,
      "grad_norm": 4.877020359039307,
      "learning_rate": 4.9561816622762815e-05,
      "loss": 2.0232,
      "step": 2135
    },
    {
      "epoch": 0.107,
      "grad_norm": 4.744772911071777,
      "learning_rate": 4.9557955505891796e-05,
      "loss": 1.9593,
      "step": 2140
    },
    {
      "epoch": 0.10725,
      "grad_norm": 5.181288719177246,
      "learning_rate": 4.955407760388976e-05,
      "loss": 1.769,
      "step": 2145
    },
    {
      "epoch": 0.1075,
      "grad_norm": 4.501628398895264,
      "learning_rate": 4.955018291940721e-05,
      "loss": 1.9867,
      "step": 2150
    },
    {
      "epoch": 0.10775,
      "grad_norm": 5.178610324859619,
      "learning_rate": 4.954627145510614e-05,
      "loss": 1.8073,
      "step": 2155
    },
    {
      "epoch": 0.108,
      "grad_norm": 4.390756607055664,
      "learning_rate": 4.9542343213659974e-05,
      "loss": 2.054,
      "step": 2160
    },
    {
      "epoch": 0.10825,
      "grad_norm": 4.948946952819824,
      "learning_rate": 4.9538398197753646e-05,
      "loss": 2.0564,
      "step": 2165
    },
    {
      "epoch": 0.1085,
      "grad_norm": 4.300079822540283,
      "learning_rate": 4.953443641008354e-05,
      "loss": 1.804,
      "step": 2170
    },
    {
      "epoch": 0.10875,
      "grad_norm": 4.5610575675964355,
      "learning_rate": 4.953045785335748e-05,
      "loss": 1.88,
      "step": 2175
    },
    {
      "epoch": 0.109,
      "grad_norm": 3.4445371627807617,
      "learning_rate": 4.95264625302948e-05,
      "loss": 2.0592,
      "step": 2180
    },
    {
      "epoch": 0.10925,
      "grad_norm": 4.007230281829834,
      "learning_rate": 4.9522450443626244e-05,
      "loss": 1.8287,
      "step": 2185
    },
    {
      "epoch": 0.1095,
      "grad_norm": 5.275167942047119,
      "learning_rate": 4.9518421596094044e-05,
      "loss": 1.7703,
      "step": 2190
    },
    {
      "epoch": 0.10975,
      "grad_norm": 2.7615456581115723,
      "learning_rate": 4.9514375990451874e-05,
      "loss": 1.6934,
      "step": 2195
    },
    {
      "epoch": 0.11,
      "grad_norm": 4.5946574211120605,
      "learning_rate": 4.951031362946488e-05,
      "loss": 1.9936,
      "step": 2200
    },
    {
      "epoch": 0.11025,
      "grad_norm": 4.468874931335449,
      "learning_rate": 4.950623451590963e-05,
      "loss": 1.7966,
      "step": 2205
    },
    {
      "epoch": 0.1105,
      "grad_norm": 4.394681453704834,
      "learning_rate": 4.950213865257417e-05,
      "loss": 1.8442,
      "step": 2210
    },
    {
      "epoch": 0.11075,
      "grad_norm": 5.849232196807861,
      "learning_rate": 4.949802604225799e-05,
      "loss": 2.1301,
      "step": 2215
    },
    {
      "epoch": 0.111,
      "grad_norm": 4.378709316253662,
      "learning_rate": 4.9493896687772e-05,
      "loss": 1.9106,
      "step": 2220
    },
    {
      "epoch": 0.11125,
      "grad_norm": 5.137689590454102,
      "learning_rate": 4.94897505919386e-05,
      "loss": 2.196,
      "step": 2225
    },
    {
      "epoch": 0.1115,
      "grad_norm": 5.701934337615967,
      "learning_rate": 4.948558775759159e-05,
      "loss": 1.9737,
      "step": 2230
    },
    {
      "epoch": 0.11175,
      "grad_norm": 5.306042194366455,
      "learning_rate": 4.948140818757623e-05,
      "loss": 1.8982,
      "step": 2235
    },
    {
      "epoch": 0.112,
      "grad_norm": 4.524853706359863,
      "learning_rate": 4.947721188474922e-05,
      "loss": 1.9996,
      "step": 2240
    },
    {
      "epoch": 0.11225,
      "grad_norm": 4.439231872558594,
      "learning_rate": 4.9472998851978705e-05,
      "loss": 1.9008,
      "step": 2245
    },
    {
      "epoch": 0.1125,
      "grad_norm": 3.802072525024414,
      "learning_rate": 4.946876909214423e-05,
      "loss": 2.1914,
      "step": 2250
    },
    {
      "epoch": 0.11275,
      "grad_norm": 4.0595269203186035,
      "learning_rate": 4.9464522608136805e-05,
      "loss": 1.8511,
      "step": 2255
    },
    {
      "epoch": 0.113,
      "grad_norm": 4.822787284851074,
      "learning_rate": 4.9460259402858864e-05,
      "loss": 1.8649,
      "step": 2260
    },
    {
      "epoch": 0.11325,
      "grad_norm": 4.218528747558594,
      "learning_rate": 4.945597947922428e-05,
      "loss": 1.8957,
      "step": 2265
    },
    {
      "epoch": 0.1135,
      "grad_norm": 3.776315689086914,
      "learning_rate": 4.9451682840158316e-05,
      "loss": 1.8285,
      "step": 2270
    },
    {
      "epoch": 0.11375,
      "grad_norm": 4.548083305358887,
      "learning_rate": 4.944736948859769e-05,
      "loss": 1.9122,
      "step": 2275
    },
    {
      "epoch": 0.114,
      "grad_norm": 3.9625601768493652,
      "learning_rate": 4.944303942749056e-05,
      "loss": 1.7468,
      "step": 2280
    },
    {
      "epoch": 0.11425,
      "grad_norm": 6.8458428382873535,
      "learning_rate": 4.943869265979646e-05,
      "loss": 1.8993,
      "step": 2285
    },
    {
      "epoch": 0.1145,
      "grad_norm": 3.6497209072113037,
      "learning_rate": 4.9434329188486374e-05,
      "loss": 2.123,
      "step": 2290
    },
    {
      "epoch": 0.11475,
      "grad_norm": 5.153071403503418,
      "learning_rate": 4.9429949016542675e-05,
      "loss": 1.9383,
      "step": 2295
    },
    {
      "epoch": 0.115,
      "grad_norm": 6.659607887268066,
      "learning_rate": 4.94255521469592e-05,
      "loss": 2.0444,
      "step": 2300
    },
    {
      "epoch": 0.11525,
      "grad_norm": 6.6558003425598145,
      "learning_rate": 4.942113858274114e-05,
      "loss": 2.1424,
      "step": 2305
    },
    {
      "epoch": 0.1155,
      "grad_norm": 4.348246097564697,
      "learning_rate": 4.941670832690514e-05,
      "loss": 1.8353,
      "step": 2310
    },
    {
      "epoch": 0.11575,
      "grad_norm": 4.726480007171631,
      "learning_rate": 4.941226138247924e-05,
      "loss": 1.9806,
      "step": 2315
    },
    {
      "epoch": 0.116,
      "grad_norm": 3.8782341480255127,
      "learning_rate": 4.9407797752502874e-05,
      "loss": 2.1282,
      "step": 2320
    },
    {
      "epoch": 0.11625,
      "grad_norm": 4.481600284576416,
      "learning_rate": 4.9403317440026896e-05,
      "loss": 2.0143,
      "step": 2325
    },
    {
      "epoch": 0.1165,
      "grad_norm": 5.169986248016357,
      "learning_rate": 4.939882044811356e-05,
      "loss": 2.1287,
      "step": 2330
    },
    {
      "epoch": 0.11675,
      "grad_norm": 3.80021071434021,
      "learning_rate": 4.939430677983651e-05,
      "loss": 1.9804,
      "step": 2335
    },
    {
      "epoch": 0.117,
      "grad_norm": 4.010263442993164,
      "learning_rate": 4.93897764382808e-05,
      "loss": 2.0065,
      "step": 2340
    },
    {
      "epoch": 0.11725,
      "grad_norm": 3.388132095336914,
      "learning_rate": 4.9385229426542884e-05,
      "loss": 2.0538,
      "step": 2345
    },
    {
      "epoch": 0.1175,
      "grad_norm": 3.5005693435668945,
      "learning_rate": 4.938066574773059e-05,
      "loss": 1.9594,
      "step": 2350
    },
    {
      "epoch": 0.11775,
      "grad_norm": 3.988081216812134,
      "learning_rate": 4.9376085404963154e-05,
      "loss": 1.8648,
      "step": 2355
    },
    {
      "epoch": 0.118,
      "grad_norm": 6.687004089355469,
      "learning_rate": 4.937148840137119e-05,
      "loss": 1.9155,
      "step": 2360
    },
    {
      "epoch": 0.11825,
      "grad_norm": 5.326045036315918,
      "learning_rate": 4.936687474009672e-05,
      "loss": 2.0138,
      "step": 2365
    },
    {
      "epoch": 0.1185,
      "grad_norm": 3.8491272926330566,
      "learning_rate": 4.936224442429312e-05,
      "loss": 1.8295,
      "step": 2370
    },
    {
      "epoch": 0.11875,
      "grad_norm": 4.759214401245117,
      "learning_rate": 4.935759745712519e-05,
      "loss": 1.9892,
      "step": 2375
    },
    {
      "epoch": 0.119,
      "grad_norm": 4.99428129196167,
      "learning_rate": 4.9352933841769075e-05,
      "loss": 1.8949,
      "step": 2380
    },
    {
      "epoch": 0.11925,
      "grad_norm": 3.479767084121704,
      "learning_rate": 4.934825358141232e-05,
      "loss": 1.8526,
      "step": 2385
    },
    {
      "epoch": 0.1195,
      "grad_norm": 6.00455379486084,
      "learning_rate": 4.934355667925382e-05,
      "loss": 1.9395,
      "step": 2390
    },
    {
      "epoch": 0.11975,
      "grad_norm": 4.127881050109863,
      "learning_rate": 4.933884313850388e-05,
      "loss": 1.8829,
      "step": 2395
    },
    {
      "epoch": 0.12,
      "grad_norm": 3.431368112564087,
      "learning_rate": 4.933411296238416e-05,
      "loss": 1.9233,
      "step": 2400
    },
    {
      "epoch": 0.12025,
      "grad_norm": 4.142413139343262,
      "learning_rate": 4.932936615412769e-05,
      "loss": 1.7806,
      "step": 2405
    },
    {
      "epoch": 0.1205,
      "grad_norm": 6.311450481414795,
      "learning_rate": 4.932460271697885e-05,
      "loss": 1.9595,
      "step": 2410
    },
    {
      "epoch": 0.12075,
      "grad_norm": 7.136922836303711,
      "learning_rate": 4.931982265419344e-05,
      "loss": 2.0207,
      "step": 2415
    },
    {
      "epoch": 0.121,
      "grad_norm": 4.405117034912109,
      "learning_rate": 4.9315025969038556e-05,
      "loss": 1.8621,
      "step": 2420
    },
    {
      "epoch": 0.12125,
      "grad_norm": 3.972935199737549,
      "learning_rate": 4.93102126647927e-05,
      "loss": 1.782,
      "step": 2425
    },
    {
      "epoch": 0.1215,
      "grad_norm": 3.766510486602783,
      "learning_rate": 4.930538274474572e-05,
      "loss": 1.9559,
      "step": 2430
    },
    {
      "epoch": 0.12175,
      "grad_norm": 5.306090354919434,
      "learning_rate": 4.930053621219882e-05,
      "loss": 1.8297,
      "step": 2435
    },
    {
      "epoch": 0.122,
      "grad_norm": 4.362013816833496,
      "learning_rate": 4.929567307046456e-05,
      "loss": 1.9452,
      "step": 2440
    },
    {
      "epoch": 0.12225,
      "grad_norm": 4.1972832679748535,
      "learning_rate": 4.929079332286685e-05,
      "loss": 1.9595,
      "step": 2445
    },
    {
      "epoch": 0.1225,
      "grad_norm": 3.822537899017334,
      "learning_rate": 4.9285896972740955e-05,
      "loss": 2.0221,
      "step": 2450
    },
    {
      "epoch": 0.12275,
      "grad_norm": 4.931141376495361,
      "learning_rate": 4.928098402343348e-05,
      "loss": 2.0513,
      "step": 2455
    },
    {
      "epoch": 0.123,
      "grad_norm": 4.613540172576904,
      "learning_rate": 4.927605447830238e-05,
      "loss": 1.9673,
      "step": 2460
    },
    {
      "epoch": 0.12325,
      "grad_norm": 4.641809463500977,
      "learning_rate": 4.9271108340716955e-05,
      "loss": 1.8568,
      "step": 2465
    },
    {
      "epoch": 0.1235,
      "grad_norm": 4.135331630706787,
      "learning_rate": 4.926614561405784e-05,
      "loss": 1.9269,
      "step": 2470
    },
    {
      "epoch": 0.12375,
      "grad_norm": 4.6842546463012695,
      "learning_rate": 4.9261166301717014e-05,
      "loss": 1.8575,
      "step": 2475
    },
    {
      "epoch": 0.124,
      "grad_norm": 4.359808921813965,
      "learning_rate": 4.925617040709779e-05,
      "loss": 1.8843,
      "step": 2480
    },
    {
      "epoch": 0.12425,
      "grad_norm": 4.78517484664917,
      "learning_rate": 4.9251157933614815e-05,
      "loss": 1.9368,
      "step": 2485
    },
    {
      "epoch": 0.1245,
      "grad_norm": 3.624941110610962,
      "learning_rate": 4.924612888469407e-05,
      "loss": 1.9868,
      "step": 2490
    },
    {
      "epoch": 0.12475,
      "grad_norm": 3.9468789100646973,
      "learning_rate": 4.9241083263772855e-05,
      "loss": 1.8873,
      "step": 2495
    },
    {
      "epoch": 0.125,
      "grad_norm": 5.966180324554443,
      "learning_rate": 4.923602107429982e-05,
      "loss": 2.1115,
      "step": 2500
    },
    {
      "epoch": 0.125,
      "eval_loss": 2.1996548175811768,
      "eval_runtime": 5.3162,
      "eval_samples_per_second": 192.617,
      "eval_steps_per_second": 24.077,
      "step": 2500
    },
    {
      "epoch": 0.12525,
      "grad_norm": 3.9985108375549316,
      "learning_rate": 4.92309423197349e-05,
      "loss": 1.9075,
      "step": 2505
    },
    {
      "epoch": 0.1255,
      "grad_norm": 4.583198547363281,
      "learning_rate": 4.9225847003549394e-05,
      "loss": 2.0833,
      "step": 2510
    },
    {
      "epoch": 0.12575,
      "grad_norm": 4.306720733642578,
      "learning_rate": 4.922073512922589e-05,
      "loss": 1.7343,
      "step": 2515
    },
    {
      "epoch": 0.126,
      "grad_norm": 4.633462429046631,
      "learning_rate": 4.921560670025832e-05,
      "loss": 1.8916,
      "step": 2520
    },
    {
      "epoch": 0.12625,
      "grad_norm": 4.451355934143066,
      "learning_rate": 4.921046172015191e-05,
      "loss": 1.9243,
      "step": 2525
    },
    {
      "epoch": 0.1265,
      "grad_norm": 7.873693943023682,
      "learning_rate": 4.920530019242321e-05,
      "loss": 1.9737,
      "step": 2530
    },
    {
      "epoch": 0.12675,
      "grad_norm": 4.919652462005615,
      "learning_rate": 4.920012212060005e-05,
      "loss": 1.9374,
      "step": 2535
    },
    {
      "epoch": 0.127,
      "grad_norm": 3.9035181999206543,
      "learning_rate": 4.919492750822164e-05,
      "loss": 1.9635,
      "step": 2540
    },
    {
      "epoch": 0.12725,
      "grad_norm": 5.482311248779297,
      "learning_rate": 4.918971635883841e-05,
      "loss": 1.8354,
      "step": 2545
    },
    {
      "epoch": 0.1275,
      "grad_norm": 3.6090075969696045,
      "learning_rate": 4.918448867601215e-05,
      "loss": 1.9003,
      "step": 2550
    },
    {
      "epoch": 0.12775,
      "grad_norm": 3.820448875427246,
      "learning_rate": 4.917924446331592e-05,
      "loss": 1.9624,
      "step": 2555
    },
    {
      "epoch": 0.128,
      "grad_norm": 4.309107303619385,
      "learning_rate": 4.917398372433409e-05,
      "loss": 1.9307,
      "step": 2560
    },
    {
      "epoch": 0.12825,
      "grad_norm": 8.27322769165039,
      "learning_rate": 4.9168706462662344e-05,
      "loss": 2.1268,
      "step": 2565
    },
    {
      "epoch": 0.1285,
      "grad_norm": 4.996788501739502,
      "learning_rate": 4.9163412681907625e-05,
      "loss": 2.0077,
      "step": 2570
    },
    {
      "epoch": 0.12875,
      "grad_norm": 3.5759968757629395,
      "learning_rate": 4.915810238568819e-05,
      "loss": 2.2235,
      "step": 2575
    },
    {
      "epoch": 0.129,
      "grad_norm": 3.639644145965576,
      "learning_rate": 4.9152775577633556e-05,
      "loss": 1.8425,
      "step": 2580
    },
    {
      "epoch": 0.12925,
      "grad_norm": 4.757011413574219,
      "learning_rate": 4.914743226138457e-05,
      "loss": 1.9689,
      "step": 2585
    },
    {
      "epoch": 0.1295,
      "grad_norm": 3.098180055618286,
      "learning_rate": 4.914207244059334e-05,
      "loss": 1.9056,
      "step": 2590
    },
    {
      "epoch": 0.12975,
      "grad_norm": 4.13278341293335,
      "learning_rate": 4.913669611892323e-05,
      "loss": 1.8171,
      "step": 2595
    },
    {
      "epoch": 0.13,
      "grad_norm": 3.9144065380096436,
      "learning_rate": 4.913130330004892e-05,
      "loss": 1.899,
      "step": 2600
    },
    {
      "epoch": 0.13025,
      "grad_norm": 4.135909557342529,
      "learning_rate": 4.912589398765635e-05,
      "loss": 1.9156,
      "step": 2605
    },
    {
      "epoch": 0.1305,
      "grad_norm": 4.857423782348633,
      "learning_rate": 4.912046818544274e-05,
      "loss": 1.9442,
      "step": 2610
    },
    {
      "epoch": 0.13075,
      "grad_norm": 3.924797773361206,
      "learning_rate": 4.911502589711656e-05,
      "loss": 1.9228,
      "step": 2615
    },
    {
      "epoch": 0.131,
      "grad_norm": 6.622230529785156,
      "learning_rate": 4.910956712639757e-05,
      "loss": 1.7704,
      "step": 2620
    },
    {
      "epoch": 0.13125,
      "grad_norm": 5.524605751037598,
      "learning_rate": 4.910409187701679e-05,
      "loss": 1.8633,
      "step": 2625
    },
    {
      "epoch": 0.1315,
      "grad_norm": 3.565288782119751,
      "learning_rate": 4.90986001527165e-05,
      "loss": 1.8438,
      "step": 2630
    },
    {
      "epoch": 0.13175,
      "grad_norm": 6.867469310760498,
      "learning_rate": 4.909309195725025e-05,
      "loss": 1.8963,
      "step": 2635
    },
    {
      "epoch": 0.132,
      "grad_norm": 4.456717491149902,
      "learning_rate": 4.908756729438282e-05,
      "loss": 1.8175,
      "step": 2640
    },
    {
      "epoch": 0.13225,
      "grad_norm": 8.943986892700195,
      "learning_rate": 4.9082026167890285e-05,
      "loss": 1.8063,
      "step": 2645
    },
    {
      "epoch": 0.1325,
      "grad_norm": 3.9676740169525146,
      "learning_rate": 4.9076468581559936e-05,
      "loss": 1.6495,
      "step": 2650
    },
    {
      "epoch": 0.13275,
      "grad_norm": 6.653923511505127,
      "learning_rate": 4.9070894539190344e-05,
      "loss": 1.7319,
      "step": 2655
    },
    {
      "epoch": 0.133,
      "grad_norm": 2.9768340587615967,
      "learning_rate": 4.906530404459131e-05,
      "loss": 1.4077,
      "step": 2660
    },
    {
      "epoch": 0.13325,
      "grad_norm": 11.490910530090332,
      "learning_rate": 4.905969710158388e-05,
      "loss": 2.0055,
      "step": 2665
    },
    {
      "epoch": 0.1335,
      "grad_norm": 3.3574059009552,
      "learning_rate": 4.9054073714000346e-05,
      "loss": 1.6114,
      "step": 2670
    },
    {
      "epoch": 0.13375,
      "grad_norm": 3.3605735301971436,
      "learning_rate": 4.904843388568426e-05,
      "loss": 1.4269,
      "step": 2675
    },
    {
      "epoch": 0.134,
      "grad_norm": 4.027829170227051,
      "learning_rate": 4.904277762049037e-05,
      "loss": 1.7403,
      "step": 2680
    },
    {
      "epoch": 0.13425,
      "grad_norm": 3.1749770641326904,
      "learning_rate": 4.9037104922284685e-05,
      "loss": 1.7624,
      "step": 2685
    },
    {
      "epoch": 0.1345,
      "grad_norm": 3.438232660293579,
      "learning_rate": 4.903141579494443e-05,
      "loss": 1.7305,
      "step": 2690
    },
    {
      "epoch": 0.13475,
      "grad_norm": 5.706902027130127,
      "learning_rate": 4.9025710242358096e-05,
      "loss": 1.8391,
      "step": 2695
    },
    {
      "epoch": 0.135,
      "grad_norm": 4.734111785888672,
      "learning_rate": 4.901998826842536e-05,
      "loss": 2.1034,
      "step": 2700
    },
    {
      "epoch": 0.13525,
      "grad_norm": 7.314356803894043,
      "learning_rate": 4.901424987705713e-05,
      "loss": 1.9989,
      "step": 2705
    },
    {
      "epoch": 0.1355,
      "grad_norm": 5.427128791809082,
      "learning_rate": 4.900849507217556e-05,
      "loss": 1.9252,
      "step": 2710
    },
    {
      "epoch": 0.13575,
      "grad_norm": 5.953651428222656,
      "learning_rate": 4.9002723857713986e-05,
      "loss": 2.0222,
      "step": 2715
    },
    {
      "epoch": 0.136,
      "grad_norm": 4.058828353881836,
      "learning_rate": 4.899693623761699e-05,
      "loss": 1.7366,
      "step": 2720
    },
    {
      "epoch": 0.13625,
      "grad_norm": 5.366683006286621,
      "learning_rate": 4.899113221584036e-05,
      "loss": 1.8143,
      "step": 2725
    },
    {
      "epoch": 0.1365,
      "grad_norm": 4.066611289978027,
      "learning_rate": 4.898531179635107e-05,
      "loss": 2.0216,
      "step": 2730
    },
    {
      "epoch": 0.13675,
      "grad_norm": 5.362022399902344,
      "learning_rate": 4.897947498312735e-05,
      "loss": 1.9542,
      "step": 2735
    },
    {
      "epoch": 0.137,
      "grad_norm": 4.123788356781006,
      "learning_rate": 4.897362178015858e-05,
      "loss": 2.0151,
      "step": 2740
    },
    {
      "epoch": 0.13725,
      "grad_norm": 3.912055015563965,
      "learning_rate": 4.896775219144538e-05,
      "loss": 1.9911,
      "step": 2745
    },
    {
      "epoch": 0.1375,
      "grad_norm": 5.6968302726745605,
      "learning_rate": 4.8961866220999566e-05,
      "loss": 1.6888,
      "step": 2750
    },
    {
      "epoch": 0.13775,
      "grad_norm": 6.178469657897949,
      "learning_rate": 4.895596387284414e-05,
      "loss": 1.9305,
      "step": 2755
    },
    {
      "epoch": 0.138,
      "grad_norm": 3.1087405681610107,
      "learning_rate": 4.895004515101329e-05,
      "loss": 1.9907,
      "step": 2760
    },
    {
      "epoch": 0.13825,
      "grad_norm": 4.6020989418029785,
      "learning_rate": 4.8944110059552425e-05,
      "loss": 1.9275,
      "step": 2765
    },
    {
      "epoch": 0.1385,
      "grad_norm": 6.296562671661377,
      "learning_rate": 4.89381586025181e-05,
      "loss": 2.0724,
      "step": 2770
    },
    {
      "epoch": 0.13875,
      "grad_norm": 4.72686767578125,
      "learning_rate": 4.8932190783978105e-05,
      "loss": 1.9816,
      "step": 2775
    },
    {
      "epoch": 0.139,
      "grad_norm": 4.04862117767334,
      "learning_rate": 4.892620660801137e-05,
      "loss": 1.9248,
      "step": 2780
    },
    {
      "epoch": 0.13925,
      "grad_norm": 3.756844997406006,
      "learning_rate": 4.892020607870803e-05,
      "loss": 1.8446,
      "step": 2785
    },
    {
      "epoch": 0.1395,
      "grad_norm": 4.058226108551025,
      "learning_rate": 4.891418920016939e-05,
      "loss": 1.7768,
      "step": 2790
    },
    {
      "epoch": 0.13975,
      "grad_norm": 3.506917715072632,
      "learning_rate": 4.890815597650793e-05,
      "loss": 1.8507,
      "step": 2795
    },
    {
      "epoch": 0.14,
      "grad_norm": 5.59250020980835,
      "learning_rate": 4.8902106411847304e-05,
      "loss": 1.846,
      "step": 2800
    },
    {
      "epoch": 0.14025,
      "grad_norm": 4.840002059936523,
      "learning_rate": 4.8896040510322325e-05,
      "loss": 2.0763,
      "step": 2805
    },
    {
      "epoch": 0.1405,
      "grad_norm": 3.910008668899536,
      "learning_rate": 4.8889958276078984e-05,
      "loss": 1.7279,
      "step": 2810
    },
    {
      "epoch": 0.14075,
      "grad_norm": 5.073479652404785,
      "learning_rate": 4.8883859713274426e-05,
      "loss": 1.9621,
      "step": 2815
    },
    {
      "epoch": 0.141,
      "grad_norm": 4.416707515716553,
      "learning_rate": 4.887774482607697e-05,
      "loss": 1.8042,
      "step": 2820
    },
    {
      "epoch": 0.14125,
      "grad_norm": 4.3732500076293945,
      "learning_rate": 4.887161361866608e-05,
      "loss": 1.9923,
      "step": 2825
    },
    {
      "epoch": 0.1415,
      "grad_norm": 3.700728178024292,
      "learning_rate": 4.886546609523237e-05,
      "loss": 1.6977,
      "step": 2830
    },
    {
      "epoch": 0.14175,
      "grad_norm": 3.9607126712799072,
      "learning_rate": 4.885930225997763e-05,
      "loss": 1.6724,
      "step": 2835
    },
    {
      "epoch": 0.142,
      "grad_norm": 5.149561882019043,
      "learning_rate": 4.885312211711477e-05,
      "loss": 1.9378,
      "step": 2840
    },
    {
      "epoch": 0.14225,
      "grad_norm": 6.338295936584473,
      "learning_rate": 4.8846925670867865e-05,
      "loss": 1.9808,
      "step": 2845
    },
    {
      "epoch": 0.1425,
      "grad_norm": 3.3533804416656494,
      "learning_rate": 4.884071292547213e-05,
      "loss": 1.6718,
      "step": 2850
    },
    {
      "epoch": 0.14275,
      "grad_norm": 3.160515546798706,
      "learning_rate": 4.883448388517391e-05,
      "loss": 1.7401,
      "step": 2855
    },
    {
      "epoch": 0.143,
      "grad_norm": 3.686453342437744,
      "learning_rate": 4.882823855423071e-05,
      "loss": 1.5935,
      "step": 2860
    },
    {
      "epoch": 0.14325,
      "grad_norm": 6.72442626953125,
      "learning_rate": 4.8821976936911134e-05,
      "loss": 1.5923,
      "step": 2865
    },
    {
      "epoch": 0.1435,
      "grad_norm": 3.589254856109619,
      "learning_rate": 4.8815699037494955e-05,
      "loss": 1.8234,
      "step": 2870
    },
    {
      "epoch": 0.14375,
      "grad_norm": 5.197192668914795,
      "learning_rate": 4.880940486027305e-05,
      "loss": 1.8446,
      "step": 2875
    },
    {
      "epoch": 0.144,
      "grad_norm": 3.51377272605896,
      "learning_rate": 4.8803094409547434e-05,
      "loss": 1.715,
      "step": 2880
    },
    {
      "epoch": 0.14425,
      "grad_norm": 2.917829751968384,
      "learning_rate": 4.879676768963124e-05,
      "loss": 1.8493,
      "step": 2885
    },
    {
      "epoch": 0.1445,
      "grad_norm": 4.585383892059326,
      "learning_rate": 4.879042470484872e-05,
      "loss": 1.5593,
      "step": 2890
    },
    {
      "epoch": 0.14475,
      "grad_norm": 4.166030406951904,
      "learning_rate": 4.8784065459535246e-05,
      "loss": 1.4608,
      "step": 2895
    },
    {
      "epoch": 0.145,
      "grad_norm": 4.06920051574707,
      "learning_rate": 4.8777689958037295e-05,
      "loss": 1.6506,
      "step": 2900
    },
    {
      "epoch": 0.14525,
      "grad_norm": 2.883028745651245,
      "learning_rate": 4.877129820471247e-05,
      "loss": 1.4231,
      "step": 2905
    },
    {
      "epoch": 0.1455,
      "grad_norm": 5.8802056312561035,
      "learning_rate": 4.8764890203929474e-05,
      "loss": 1.7723,
      "step": 2910
    },
    {
      "epoch": 0.14575,
      "grad_norm": 3.0073366165161133,
      "learning_rate": 4.8758465960068113e-05,
      "loss": 1.8183,
      "step": 2915
    },
    {
      "epoch": 0.146,
      "grad_norm": 4.422276020050049,
      "learning_rate": 4.8752025477519295e-05,
      "loss": 1.5421,
      "step": 2920
    },
    {
      "epoch": 0.14625,
      "grad_norm": 4.052964687347412,
      "learning_rate": 4.8745568760685035e-05,
      "loss": 1.7904,
      "step": 2925
    },
    {
      "epoch": 0.1465,
      "grad_norm": 4.402920722961426,
      "learning_rate": 4.873909581397843e-05,
      "loss": 2.6659,
      "step": 2930
    },
    {
      "epoch": 0.14675,
      "grad_norm": 4.966674327850342,
      "learning_rate": 4.873260664182369e-05,
      "loss": 3.0102,
      "step": 2935
    },
    {
      "epoch": 0.147,
      "grad_norm": 4.108904838562012,
      "learning_rate": 4.872610124865609e-05,
      "loss": 2.8118,
      "step": 2940
    },
    {
      "epoch": 0.14725,
      "grad_norm": 7.18000602722168,
      "learning_rate": 4.871957963892201e-05,
      "loss": 2.765,
      "step": 2945
    },
    {
      "epoch": 0.1475,
      "grad_norm": 4.358675479888916,
      "learning_rate": 4.8713041817078906e-05,
      "loss": 2.6793,
      "step": 2950
    },
    {
      "epoch": 0.14775,
      "grad_norm": 4.704619884490967,
      "learning_rate": 4.870648778759532e-05,
      "loss": 2.6941,
      "step": 2955
    },
    {
      "epoch": 0.148,
      "grad_norm": 3.9932851791381836,
      "learning_rate": 4.869991755495088e-05,
      "loss": 2.613,
      "step": 2960
    },
    {
      "epoch": 0.14825,
      "grad_norm": 5.229231834411621,
      "learning_rate": 4.869333112363625e-05,
      "loss": 2.5672,
      "step": 2965
    },
    {
      "epoch": 0.1485,
      "grad_norm": 3.807551622390747,
      "learning_rate": 4.8686728498153235e-05,
      "loss": 2.6524,
      "step": 2970
    },
    {
      "epoch": 0.14875,
      "grad_norm": 6.764174461364746,
      "learning_rate": 4.868010968301463e-05,
      "loss": 2.8041,
      "step": 2975
    },
    {
      "epoch": 0.149,
      "grad_norm": 4.299892425537109,
      "learning_rate": 4.8673474682744344e-05,
      "loss": 2.6945,
      "step": 2980
    },
    {
      "epoch": 0.14925,
      "grad_norm": 3.3867175579071045,
      "learning_rate": 4.866682350187735e-05,
      "loss": 2.5568,
      "step": 2985
    },
    {
      "epoch": 0.1495,
      "grad_norm": 4.426889419555664,
      "learning_rate": 4.866015614495966e-05,
      "loss": 2.5961,
      "step": 2990
    },
    {
      "epoch": 0.14975,
      "grad_norm": 3.9033052921295166,
      "learning_rate": 4.865347261654834e-05,
      "loss": 2.5401,
      "step": 2995
    },
    {
      "epoch": 0.15,
      "grad_norm": 4.880038261413574,
      "learning_rate": 4.864677292121151e-05,
      "loss": 2.5362,
      "step": 3000
    },
    {
      "epoch": 0.15,
      "eval_loss": 2.0188329219818115,
      "eval_runtime": 5.3015,
      "eval_samples_per_second": 193.152,
      "eval_steps_per_second": 24.144,
      "step": 3000
    },
    {
      "epoch": 0.15025,
      "grad_norm": 3.4957773685455322,
      "learning_rate": 4.864005706352839e-05,
      "loss": 2.5363,
      "step": 3005
    },
    {
      "epoch": 0.1505,
      "grad_norm": 5.945515155792236,
      "learning_rate": 4.863332504808916e-05,
      "loss": 2.7121,
      "step": 3010
    },
    {
      "epoch": 0.15075,
      "grad_norm": 5.792594909667969,
      "learning_rate": 4.862657687949512e-05,
      "loss": 2.6683,
      "step": 3015
    },
    {
      "epoch": 0.151,
      "grad_norm": 4.170135974884033,
      "learning_rate": 4.861981256235857e-05,
      "loss": 2.5633,
      "step": 3020
    },
    {
      "epoch": 0.15125,
      "grad_norm": 3.676929473876953,
      "learning_rate": 4.861303210130285e-05,
      "loss": 2.5866,
      "step": 3025
    },
    {
      "epoch": 0.1515,
      "grad_norm": 4.243528366088867,
      "learning_rate": 4.860623550096235e-05,
      "loss": 2.7389,
      "step": 3030
    },
    {
      "epoch": 0.15175,
      "grad_norm": 5.200112342834473,
      "learning_rate": 4.859942276598247e-05,
      "loss": 2.6301,
      "step": 3035
    },
    {
      "epoch": 0.152,
      "grad_norm": 8.061675071716309,
      "learning_rate": 4.859259390101967e-05,
      "loss": 3.6009,
      "step": 3040
    },
    {
      "epoch": 0.15225,
      "grad_norm": 6.500259876251221,
      "learning_rate": 4.858574891074139e-05,
      "loss": 3.4599,
      "step": 3045
    },
    {
      "epoch": 0.1525,
      "grad_norm": 5.4606804847717285,
      "learning_rate": 4.857888779982614e-05,
      "loss": 3.2723,
      "step": 3050
    },
    {
      "epoch": 0.15275,
      "grad_norm": 6.087939262390137,
      "learning_rate": 4.857201057296341e-05,
      "loss": 2.623,
      "step": 3055
    },
    {
      "epoch": 0.153,
      "grad_norm": 5.16325044631958,
      "learning_rate": 4.856511723485373e-05,
      "loss": 2.8466,
      "step": 3060
    },
    {
      "epoch": 0.15325,
      "grad_norm": 4.197852611541748,
      "learning_rate": 4.855820779020862e-05,
      "loss": 2.342,
      "step": 3065
    },
    {
      "epoch": 1.00025,
      "grad_norm": 5.650729656219482,
      "learning_rate": 4.855128224375063e-05,
      "loss": 1.933,
      "step": 3070
    },
    {
      "epoch": 1.0005,
      "grad_norm": 4.917675971984863,
      "learning_rate": 4.85443406002133e-05,
      "loss": 1.9036,
      "step": 3075
    },
    {
      "epoch": 1.00075,
      "grad_norm": 4.079252243041992,
      "learning_rate": 4.8537382864341176e-05,
      "loss": 1.8518,
      "step": 3080
    },
    {
      "epoch": 1.001,
      "grad_norm": 6.321979522705078,
      "learning_rate": 4.8530409040889814e-05,
      "loss": 1.7822,
      "step": 3085
    },
    {
      "epoch": 1.00125,
      "grad_norm": 4.839600086212158,
      "learning_rate": 4.8523419134625745e-05,
      "loss": 1.9971,
      "step": 3090
    },
    {
      "epoch": 1.0015,
      "grad_norm": 6.3816986083984375,
      "learning_rate": 4.851641315032651e-05,
      "loss": 2.0191,
      "step": 3095
    },
    {
      "epoch": 1.00175,
      "grad_norm": 5.28675651550293,
      "learning_rate": 4.8509391092780644e-05,
      "loss": 1.5809,
      "step": 3100
    },
    {
      "epoch": 1.002,
      "grad_norm": 4.579629421234131,
      "learning_rate": 4.850235296678763e-05,
      "loss": 1.8257,
      "step": 3105
    },
    {
      "epoch": 1.00225,
      "grad_norm": 4.7766571044921875,
      "learning_rate": 4.849529877715799e-05,
      "loss": 1.8407,
      "step": 3110
    },
    {
      "epoch": 1.0025,
      "grad_norm": 4.621523857116699,
      "learning_rate": 4.8488228528713186e-05,
      "loss": 2.0168,
      "step": 3115
    },
    {
      "epoch": 1.00275,
      "grad_norm": 7.840010166168213,
      "learning_rate": 4.848114222628566e-05,
      "loss": 2.2287,
      "step": 3120
    },
    {
      "epoch": 1.003,
      "grad_norm": 6.118283271789551,
      "learning_rate": 4.847403987471883e-05,
      "loss": 2.0747,
      "step": 3125
    },
    {
      "epoch": 1.00325,
      "grad_norm": 4.7877888679504395,
      "learning_rate": 4.8466921478867094e-05,
      "loss": 2.1611,
      "step": 3130
    },
    {
      "epoch": 1.0035,
      "grad_norm": 4.566706657409668,
      "learning_rate": 4.845978704359581e-05,
      "loss": 1.9471,
      "step": 3135
    },
    {
      "epoch": 1.00375,
      "grad_norm": 4.128468990325928,
      "learning_rate": 4.8452636573781294e-05,
      "loss": 1.8401,
      "step": 3140
    },
    {
      "epoch": 1.004,
      "grad_norm": 11.590713500976562,
      "learning_rate": 4.8445470074310824e-05,
      "loss": 2.1865,
      "step": 3145
    },
    {
      "epoch": 1.00425,
      "grad_norm": 8.429697036743164,
      "learning_rate": 4.8438287550082635e-05,
      "loss": 2.2493,
      "step": 3150
    },
    {
      "epoch": 1.0045,
      "grad_norm": 5.53994607925415,
      "learning_rate": 4.843108900600591e-05,
      "loss": 2.1613,
      "step": 3155
    },
    {
      "epoch": 1.00475,
      "grad_norm": 6.21323299407959,
      "learning_rate": 4.8423874447000806e-05,
      "loss": 2.1872,
      "step": 3160
    },
    {
      "epoch": 1.005,
      "grad_norm": 5.4360761642456055,
      "learning_rate": 4.841664387799838e-05,
      "loss": 2.2367,
      "step": 3165
    },
    {
      "epoch": 1.00525,
      "grad_norm": 4.7074809074401855,
      "learning_rate": 4.840939730394067e-05,
      "loss": 2.1508,
      "step": 3170
    },
    {
      "epoch": 1.0055,
      "grad_norm": 5.978890895843506,
      "learning_rate": 4.8402134729780645e-05,
      "loss": 2.4492,
      "step": 3175
    },
    {
      "epoch": 1.00575,
      "grad_norm": 4.495105743408203,
      "learning_rate": 4.83948561604822e-05,
      "loss": 2.1956,
      "step": 3180
    },
    {
      "epoch": 1.006,
      "grad_norm": 5.8422369956970215,
      "learning_rate": 4.8387561601020174e-05,
      "loss": 2.3463,
      "step": 3185
    },
    {
      "epoch": 1.00625,
      "grad_norm": 5.384918689727783,
      "learning_rate": 4.838025105638031e-05,
      "loss": 2.1011,
      "step": 3190
    },
    {
      "epoch": 1.0065,
      "grad_norm": 6.35078763961792,
      "learning_rate": 4.8372924531559325e-05,
      "loss": 2.3235,
      "step": 3195
    },
    {
      "epoch": 1.00675,
      "grad_norm": 4.6856689453125,
      "learning_rate": 4.836558203156481e-05,
      "loss": 1.9775,
      "step": 3200
    },
    {
      "epoch": 1.007,
      "grad_norm": 5.825282096862793,
      "learning_rate": 4.8358223561415304e-05,
      "loss": 2.0617,
      "step": 3205
    },
    {
      "epoch": 1.00725,
      "grad_norm": 4.641531467437744,
      "learning_rate": 4.835084912614025e-05,
      "loss": 2.2078,
      "step": 3210
    },
    {
      "epoch": 1.0075,
      "grad_norm": 5.100907325744629,
      "learning_rate": 4.834345873078e-05,
      "loss": 2.2997,
      "step": 3215
    },
    {
      "epoch": 1.00775,
      "grad_norm": 6.405536651611328,
      "learning_rate": 4.833605238038582e-05,
      "loss": 2.0786,
      "step": 3220
    },
    {
      "epoch": 1.008,
      "grad_norm": 4.855493545532227,
      "learning_rate": 4.8328630080019886e-05,
      "loss": 2.1133,
      "step": 3225
    },
    {
      "epoch": 1.00825,
      "grad_norm": 5.828333854675293,
      "learning_rate": 4.8321191834755275e-05,
      "loss": 2.2757,
      "step": 3230
    },
    {
      "epoch": 1.0085,
      "grad_norm": 7.836358547210693,
      "learning_rate": 4.831373764967595e-05,
      "loss": 2.1042,
      "step": 3235
    },
    {
      "epoch": 1.00875,
      "grad_norm": 5.462383270263672,
      "learning_rate": 4.830626752987677e-05,
      "loss": 2.1279,
      "step": 3240
    },
    {
      "epoch": 1.009,
      "grad_norm": 5.467504501342773,
      "learning_rate": 4.8298781480463495e-05,
      "loss": 2.3043,
      "step": 3245
    },
    {
      "epoch": 1.00925,
      "grad_norm": 5.2045440673828125,
      "learning_rate": 4.8291279506552786e-05,
      "loss": 2.1157,
      "step": 3250
    },
    {
      "epoch": 1.0095,
      "grad_norm": 6.847127914428711,
      "learning_rate": 4.828376161327215e-05,
      "loss": 2.0767,
      "step": 3255
    },
    {
      "epoch": 1.00975,
      "grad_norm": 8.46539306640625,
      "learning_rate": 4.827622780576e-05,
      "loss": 2.2986,
      "step": 3260
    },
    {
      "epoch": 1.01,
      "grad_norm": 5.336989402770996,
      "learning_rate": 4.826867808916563e-05,
      "loss": 2.0822,
      "step": 3265
    },
    {
      "epoch": 1.01025,
      "grad_norm": 6.099551677703857,
      "learning_rate": 4.82611124686492e-05,
      "loss": 2.1412,
      "step": 3270
    },
    {
      "epoch": 1.0105,
      "grad_norm": 4.4179792404174805,
      "learning_rate": 4.825353094938173e-05,
      "loss": 2.2774,
      "step": 3275
    },
    {
      "epoch": 1.01075,
      "grad_norm": 6.528589248657227,
      "learning_rate": 4.824593353654513e-05,
      "loss": 2.1407,
      "step": 3280
    },
    {
      "epoch": 1.011,
      "grad_norm": 6.024819374084473,
      "learning_rate": 4.8238320235332156e-05,
      "loss": 2.0607,
      "step": 3285
    },
    {
      "epoch": 1.01125,
      "grad_norm": 4.311819076538086,
      "learning_rate": 4.823069105094642e-05,
      "loss": 2.1614,
      "step": 3290
    },
    {
      "epoch": 1.0115,
      "grad_norm": 5.840682029724121,
      "learning_rate": 4.822304598860241e-05,
      "loss": 2.0214,
      "step": 3295
    },
    {
      "epoch": 1.01175,
      "grad_norm": 6.061587810516357,
      "learning_rate": 4.821538505352543e-05,
      "loss": 2.1134,
      "step": 3300
    },
    {
      "epoch": 1.012,
      "grad_norm": 5.554698944091797,
      "learning_rate": 4.820770825095169e-05,
      "loss": 1.9866,
      "step": 3305
    },
    {
      "epoch": 1.01225,
      "grad_norm": 5.422181129455566,
      "learning_rate": 4.8200015586128185e-05,
      "loss": 2.2722,
      "step": 3310
    },
    {
      "epoch": 1.0125,
      "grad_norm": 5.765488624572754,
      "learning_rate": 4.81923070643128e-05,
      "loss": 1.9928,
      "step": 3315
    },
    {
      "epoch": 1.01275,
      "grad_norm": 5.1331586837768555,
      "learning_rate": 4.818458269077423e-05,
      "loss": 1.796,
      "step": 3320
    },
    {
      "epoch": 1.013,
      "grad_norm": 4.64312219619751,
      "learning_rate": 4.8176842470792e-05,
      "loss": 1.8983,
      "step": 3325
    },
    {
      "epoch": 1.01325,
      "grad_norm": 5.3146843910217285,
      "learning_rate": 4.816908640965648e-05,
      "loss": 1.9804,
      "step": 3330
    },
    {
      "epoch": 1.0135,
      "grad_norm": 6.0117292404174805,
      "learning_rate": 4.816131451266889e-05,
      "loss": 1.8409,
      "step": 3335
    },
    {
      "epoch": 1.01375,
      "grad_norm": 6.641458034515381,
      "learning_rate": 4.815352678514121e-05,
      "loss": 1.9863,
      "step": 3340
    },
    {
      "epoch": 1.014,
      "grad_norm": 5.4235758781433105,
      "learning_rate": 4.81457232323963e-05,
      "loss": 2.3862,
      "step": 3345
    },
    {
      "epoch": 1.01425,
      "grad_norm": 4.416881084442139,
      "learning_rate": 4.813790385976782e-05,
      "loss": 2.2305,
      "step": 3350
    },
    {
      "epoch": 1.0145,
      "grad_norm": 4.191169261932373,
      "learning_rate": 4.813006867260023e-05,
      "loss": 1.8794,
      "step": 3355
    },
    {
      "epoch": 1.01475,
      "grad_norm": 4.888765335083008,
      "learning_rate": 4.8122217676248796e-05,
      "loss": 2.1659,
      "step": 3360
    },
    {
      "epoch": 1.015,
      "grad_norm": 4.292660236358643,
      "learning_rate": 4.811435087607961e-05,
      "loss": 1.9989,
      "step": 3365
    },
    {
      "epoch": 1.01525,
      "grad_norm": 6.307635307312012,
      "learning_rate": 4.8106468277469566e-05,
      "loss": 1.3717,
      "step": 3370
    },
    {
      "epoch": 1.0155,
      "grad_norm": 5.933898448944092,
      "learning_rate": 4.809856988580632e-05,
      "loss": 1.4805,
      "step": 3375
    },
    {
      "epoch": 1.01575,
      "grad_norm": 4.657791614532471,
      "learning_rate": 4.8090655706488376e-05,
      "loss": 1.3398,
      "step": 3380
    },
    {
      "epoch": 1.016,
      "grad_norm": 4.024274826049805,
      "learning_rate": 4.8082725744924984e-05,
      "loss": 1.0875,
      "step": 3385
    },
    {
      "epoch": 1.01625,
      "grad_norm": 4.801994800567627,
      "learning_rate": 4.807478000653621e-05,
      "loss": 1.3414,
      "step": 3390
    },
    {
      "epoch": 1.0165,
      "grad_norm": 9.415144920349121,
      "learning_rate": 4.8066818496752875e-05,
      "loss": 1.593,
      "step": 3395
    },
    {
      "epoch": 1.01675,
      "grad_norm": 6.3346099853515625,
      "learning_rate": 4.8058841221016605e-05,
      "loss": 2.2502,
      "step": 3400
    },
    {
      "epoch": 1.017,
      "grad_norm": 5.882691860198975,
      "learning_rate": 4.805084818477979e-05,
      "loss": 2.4469,
      "step": 3405
    },
    {
      "epoch": 1.01725,
      "grad_norm": 5.344890594482422,
      "learning_rate": 4.80428393935056e-05,
      "loss": 2.1669,
      "step": 3410
    },
    {
      "epoch": 1.0175,
      "grad_norm": 6.70387077331543,
      "learning_rate": 4.8034814852667974e-05,
      "loss": 2.2047,
      "step": 3415
    },
    {
      "epoch": 1.01775,
      "grad_norm": 5.582768440246582,
      "learning_rate": 4.802677456775159e-05,
      "loss": 1.8797,
      "step": 3420
    },
    {
      "epoch": 1.018,
      "grad_norm": 5.059412956237793,
      "learning_rate": 4.801871854425193e-05,
      "loss": 1.9524,
      "step": 3425
    },
    {
      "epoch": 1.01825,
      "grad_norm": 5.930922985076904,
      "learning_rate": 4.8010646787675186e-05,
      "loss": 2.1678,
      "step": 3430
    },
    {
      "epoch": 1.0185,
      "grad_norm": 5.139240264892578,
      "learning_rate": 4.8002559303538344e-05,
      "loss": 2.3248,
      "step": 3435
    },
    {
      "epoch": 1.01875,
      "grad_norm": 5.9244513511657715,
      "learning_rate": 4.799445609736912e-05,
      "loss": 2.0682,
      "step": 3440
    },
    {
      "epoch": 1.019,
      "grad_norm": 4.683111667633057,
      "learning_rate": 4.798633717470598e-05,
      "loss": 2.2856,
      "step": 3445
    },
    {
      "epoch": 1.01925,
      "grad_norm": 5.694440841674805,
      "learning_rate": 4.7978202541098124e-05,
      "loss": 2.1148,
      "step": 3450
    },
    {
      "epoch": 1.0195,
      "grad_norm": 5.379753589630127,
      "learning_rate": 4.797005220210551e-05,
      "loss": 2.0606,
      "step": 3455
    },
    {
      "epoch": 1.01975,
      "grad_norm": 5.186297416687012,
      "learning_rate": 4.7961886163298805e-05,
      "loss": 1.8429,
      "step": 3460
    },
    {
      "epoch": 1.02,
      "grad_norm": 5.456118583679199,
      "learning_rate": 4.7953704430259424e-05,
      "loss": 2.0697,
      "step": 3465
    },
    {
      "epoch": 1.02025,
      "grad_norm": 7.120105266571045,
      "learning_rate": 4.794550700857952e-05,
      "loss": 2.3512,
      "step": 3470
    },
    {
      "epoch": 1.0205,
      "grad_norm": 4.7992401123046875,
      "learning_rate": 4.7937293903861926e-05,
      "loss": 2.2238,
      "step": 3475
    },
    {
      "epoch": 1.02075,
      "grad_norm": 6.005787372589111,
      "learning_rate": 4.7929065121720226e-05,
      "loss": 1.9676,
      "step": 3480
    },
    {
      "epoch": 1.021,
      "grad_norm": 6.7100019454956055,
      "learning_rate": 4.7920820667778744e-05,
      "loss": 2.1519,
      "step": 3485
    },
    {
      "epoch": 1.02125,
      "grad_norm": 4.202538967132568,
      "learning_rate": 4.791256054767245e-05,
      "loss": 2.1177,
      "step": 3490
    },
    {
      "epoch": 1.0215,
      "grad_norm": 5.638918399810791,
      "learning_rate": 4.790428476704708e-05,
      "loss": 1.9019,
      "step": 3495
    },
    {
      "epoch": 1.02175,
      "grad_norm": 6.81525182723999,
      "learning_rate": 4.7895993331559044e-05,
      "loss": 2.0774,
      "step": 3500
    },
    {
      "epoch": 1.02175,
      "eval_loss": 1.9086544513702393,
      "eval_runtime": 5.3126,
      "eval_samples_per_second": 192.748,
      "eval_steps_per_second": 24.093,
      "step": 3500
    },
    {
      "epoch": 1.022,
      "grad_norm": 5.157780170440674,
      "learning_rate": 4.7887686246875465e-05,
      "loss": 2.282,
      "step": 3505
    },
    {
      "epoch": 1.02225,
      "grad_norm": 6.93260383605957,
      "learning_rate": 4.787936351867416e-05,
      "loss": 2.3928,
      "step": 3510
    },
    {
      "epoch": 1.0225,
      "grad_norm": 5.580226898193359,
      "learning_rate": 4.7871025152643616e-05,
      "loss": 2.4991,
      "step": 3515
    },
    {
      "epoch": 1.02275,
      "grad_norm": 6.218936920166016,
      "learning_rate": 4.7862671154483054e-05,
      "loss": 2.1357,
      "step": 3520
    },
    {
      "epoch": 1.023,
      "grad_norm": 4.838619709014893,
      "learning_rate": 4.785430152990233e-05,
      "loss": 2.2372,
      "step": 3525
    },
    {
      "epoch": 1.02325,
      "grad_norm": 5.0787200927734375,
      "learning_rate": 4.7845916284622016e-05,
      "loss": 1.9005,
      "step": 3530
    },
    {
      "epoch": 1.0235,
      "grad_norm": 6.685436248779297,
      "learning_rate": 4.7837515424373344e-05,
      "loss": 2.2012,
      "step": 3535
    },
    {
      "epoch": 1.02375,
      "grad_norm": 4.236557483673096,
      "learning_rate": 4.782909895489823e-05,
      "loss": 1.9099,
      "step": 3540
    },
    {
      "epoch": 1.024,
      "grad_norm": 6.4910454750061035,
      "learning_rate": 4.7820666881949246e-05,
      "loss": 2.0049,
      "step": 3545
    },
    {
      "epoch": 1.02425,
      "grad_norm": 5.379177093505859,
      "learning_rate": 4.7812219211289626e-05,
      "loss": 1.9113,
      "step": 3550
    },
    {
      "epoch": 1.0245,
      "grad_norm": 8.186783790588379,
      "learning_rate": 4.780375594869329e-05,
      "loss": 2.4602,
      "step": 3555
    },
    {
      "epoch": 1.02475,
      "grad_norm": 4.874765396118164,
      "learning_rate": 4.779527709994478e-05,
      "loss": 2.0623,
      "step": 3560
    },
    {
      "epoch": 1.025,
      "grad_norm": 7.039510250091553,
      "learning_rate": 4.778678267083933e-05,
      "loss": 2.1379,
      "step": 3565
    },
    {
      "epoch": 1.02525,
      "grad_norm": 3.5718512535095215,
      "learning_rate": 4.777827266718279e-05,
      "loss": 1.9723,
      "step": 3570
    },
    {
      "epoch": 1.0255,
      "grad_norm": 5.794490337371826,
      "learning_rate": 4.776974709479166e-05,
      "loss": 2.2397,
      "step": 3575
    },
    {
      "epoch": 1.02575,
      "grad_norm": 4.6782450675964355,
      "learning_rate": 4.7761205959493105e-05,
      "loss": 2.2375,
      "step": 3580
    },
    {
      "epoch": 1.026,
      "grad_norm": 5.95806884765625,
      "learning_rate": 4.775264926712489e-05,
      "loss": 1.9378,
      "step": 3585
    },
    {
      "epoch": 1.02625,
      "grad_norm": 7.179965972900391,
      "learning_rate": 4.774407702353546e-05,
      "loss": 2.4961,
      "step": 3590
    },
    {
      "epoch": 1.0265,
      "grad_norm": 4.94431734085083,
      "learning_rate": 4.7735489234583844e-05,
      "loss": 1.9591,
      "step": 3595
    },
    {
      "epoch": 1.02675,
      "grad_norm": 6.936899662017822,
      "learning_rate": 4.772688590613971e-05,
      "loss": 2.1566,
      "step": 3600
    },
    {
      "epoch": 1.027,
      "grad_norm": 7.25170373916626,
      "learning_rate": 4.771826704408337e-05,
      "loss": 2.0716,
      "step": 3605
    },
    {
      "epoch": 1.02725,
      "grad_norm": 5.059237003326416,
      "learning_rate": 4.770963265430573e-05,
      "loss": 2.0076,
      "step": 3610
    },
    {
      "epoch": 1.0275,
      "grad_norm": 6.065010070800781,
      "learning_rate": 4.77009827427083e-05,
      "loss": 1.9142,
      "step": 3615
    },
    {
      "epoch": 1.02775,
      "grad_norm": 5.040463924407959,
      "learning_rate": 4.7692317315203224e-05,
      "loss": 2.1408,
      "step": 3620
    },
    {
      "epoch": 1.028,
      "grad_norm": 8.299524307250977,
      "learning_rate": 4.768363637771324e-05,
      "loss": 2.0311,
      "step": 3625
    },
    {
      "epoch": 1.02825,
      "grad_norm": 6.12831974029541,
      "learning_rate": 4.7674939936171686e-05,
      "loss": 2.2887,
      "step": 3630
    },
    {
      "epoch": 1.0285,
      "grad_norm": 5.486968994140625,
      "learning_rate": 4.76662279965225e-05,
      "loss": 2.0065,
      "step": 3635
    },
    {
      "epoch": 1.02875,
      "grad_norm": 7.7019195556640625,
      "learning_rate": 4.76575005647202e-05,
      "loss": 2.1939,
      "step": 3640
    },
    {
      "epoch": 1.029,
      "grad_norm": 5.9688191413879395,
      "learning_rate": 4.764875764672992e-05,
      "loss": 2.0413,
      "step": 3645
    },
    {
      "epoch": 1.02925,
      "grad_norm": 4.725334644317627,
      "learning_rate": 4.7639999248527356e-05,
      "loss": 2.008,
      "step": 3650
    },
    {
      "epoch": 1.0295,
      "grad_norm": 3.4049594402313232,
      "learning_rate": 4.7631225376098784e-05,
      "loss": 1.748,
      "step": 3655
    },
    {
      "epoch": 1.02975,
      "grad_norm": 6.779332160949707,
      "learning_rate": 4.762243603544106e-05,
      "loss": 2.0698,
      "step": 3660
    },
    {
      "epoch": 1.03,
      "grad_norm": 3.9875094890594482,
      "learning_rate": 4.761363123256163e-05,
      "loss": 1.9278,
      "step": 3665
    },
    {
      "epoch": 1.03025,
      "grad_norm": 7.1705121994018555,
      "learning_rate": 4.760481097347849e-05,
      "loss": 2.1109,
      "step": 3670
    },
    {
      "epoch": 1.0305,
      "grad_norm": 5.450742244720459,
      "learning_rate": 4.759597526422019e-05,
      "loss": 2.1123,
      "step": 3675
    },
    {
      "epoch": 1.03075,
      "grad_norm": 6.320598602294922,
      "learning_rate": 4.7587124110825875e-05,
      "loss": 2.01,
      "step": 3680
    },
    {
      "epoch": 1.031,
      "grad_norm": 4.3844404220581055,
      "learning_rate": 4.75782575193452e-05,
      "loss": 1.9227,
      "step": 3685
    },
    {
      "epoch": 1.03125,
      "grad_norm": 5.701441764831543,
      "learning_rate": 4.756937549583843e-05,
      "loss": 2.1944,
      "step": 3690
    },
    {
      "epoch": 1.0315,
      "grad_norm": 5.622453212738037,
      "learning_rate": 4.7560478046376314e-05,
      "loss": 1.9549,
      "step": 3695
    },
    {
      "epoch": 1.03175,
      "grad_norm": 4.242876052856445,
      "learning_rate": 4.755156517704018e-05,
      "loss": 1.9892,
      "step": 3700
    },
    {
      "epoch": 1.032,
      "grad_norm": 4.691072940826416,
      "learning_rate": 4.754263689392191e-05,
      "loss": 1.8929,
      "step": 3705
    },
    {
      "epoch": 1.03225,
      "grad_norm": 6.27617883682251,
      "learning_rate": 4.753369320312388e-05,
      "loss": 1.9958,
      "step": 3710
    },
    {
      "epoch": 1.0325,
      "grad_norm": 5.873416900634766,
      "learning_rate": 4.752473411075902e-05,
      "loss": 1.7586,
      "step": 3715
    },
    {
      "epoch": 1.03275,
      "grad_norm": 4.853408336639404,
      "learning_rate": 4.75157596229508e-05,
      "loss": 1.7057,
      "step": 3720
    },
    {
      "epoch": 1.033,
      "grad_norm": 5.834494590759277,
      "learning_rate": 4.7506769745833184e-05,
      "loss": 1.9435,
      "step": 3725
    },
    {
      "epoch": 1.03325,
      "grad_norm": 5.642355918884277,
      "learning_rate": 4.749776448555068e-05,
      "loss": 1.9923,
      "step": 3730
    },
    {
      "epoch": 1.0335,
      "grad_norm": 4.945495128631592,
      "learning_rate": 4.7488743848258274e-05,
      "loss": 1.9879,
      "step": 3735
    },
    {
      "epoch": 1.03375,
      "grad_norm": 6.757663249969482,
      "learning_rate": 4.747970784012152e-05,
      "loss": 2.1695,
      "step": 3740
    },
    {
      "epoch": 1.034,
      "grad_norm": 5.118980407714844,
      "learning_rate": 4.7470656467316424e-05,
      "loss": 2.2881,
      "step": 3745
    },
    {
      "epoch": 1.0342500000000001,
      "grad_norm": 5.07265567779541,
      "learning_rate": 4.746158973602952e-05,
      "loss": 1.837,
      "step": 3750
    },
    {
      "epoch": 1.0345,
      "grad_norm": 5.077028274536133,
      "learning_rate": 4.745250765245783e-05,
      "loss": 1.985,
      "step": 3755
    },
    {
      "epoch": 1.03475,
      "grad_norm": 4.20198392868042,
      "learning_rate": 4.7443410222808874e-05,
      "loss": 2.0607,
      "step": 3760
    },
    {
      "epoch": 1.035,
      "grad_norm": 4.880964756011963,
      "learning_rate": 4.743429745330066e-05,
      "loss": 2.1628,
      "step": 3765
    },
    {
      "epoch": 1.03525,
      "grad_norm": 4.348728656768799,
      "learning_rate": 4.7425169350161675e-05,
      "loss": 2.0548,
      "step": 3770
    },
    {
      "epoch": 1.0355,
      "grad_norm": 3.8417739868164062,
      "learning_rate": 4.7416025919630904e-05,
      "loss": 2.0197,
      "step": 3775
    },
    {
      "epoch": 1.03575,
      "grad_norm": 4.936638832092285,
      "learning_rate": 4.740686716795778e-05,
      "loss": 1.9931,
      "step": 3780
    },
    {
      "epoch": 1.036,
      "grad_norm": 5.359314441680908,
      "learning_rate": 4.739769310140223e-05,
      "loss": 2.3132,
      "step": 3785
    },
    {
      "epoch": 1.03625,
      "grad_norm": 4.460964202880859,
      "learning_rate": 4.738850372623465e-05,
      "loss": 2.1259,
      "step": 3790
    },
    {
      "epoch": 1.0365,
      "grad_norm": 8.426417350769043,
      "learning_rate": 4.7379299048735876e-05,
      "loss": 2.1517,
      "step": 3795
    },
    {
      "epoch": 1.03675,
      "grad_norm": 5.55516242980957,
      "learning_rate": 4.737007907519723e-05,
      "loss": 2.1512,
      "step": 3800
    },
    {
      "epoch": 1.037,
      "grad_norm": 3.5295591354370117,
      "learning_rate": 4.736084381192048e-05,
      "loss": 2.117,
      "step": 3805
    },
    {
      "epoch": 1.03725,
      "grad_norm": 4.460900783538818,
      "learning_rate": 4.735159326521782e-05,
      "loss": 2.0798,
      "step": 3810
    },
    {
      "epoch": 1.0375,
      "grad_norm": 5.18726921081543,
      "learning_rate": 4.734232744141194e-05,
      "loss": 2.262,
      "step": 3815
    },
    {
      "epoch": 1.03775,
      "grad_norm": 3.773829936981201,
      "learning_rate": 4.7333046346835924e-05,
      "loss": 1.9963,
      "step": 3820
    },
    {
      "epoch": 1.038,
      "grad_norm": 4.2963714599609375,
      "learning_rate": 4.732374998783332e-05,
      "loss": 2.0073,
      "step": 3825
    },
    {
      "epoch": 1.03825,
      "grad_norm": 6.819156646728516,
      "learning_rate": 4.73144383707581e-05,
      "loss": 2.0866,
      "step": 3830
    },
    {
      "epoch": 1.0385,
      "grad_norm": 4.7382049560546875,
      "learning_rate": 4.730511150197466e-05,
      "loss": 2.0522,
      "step": 3835
    },
    {
      "epoch": 1.03875,
      "grad_norm": 5.126642227172852,
      "learning_rate": 4.7295769387857834e-05,
      "loss": 2.2422,
      "step": 3840
    },
    {
      "epoch": 1.039,
      "grad_norm": 5.761113166809082,
      "learning_rate": 4.728641203479287e-05,
      "loss": 1.9793,
      "step": 3845
    },
    {
      "epoch": 1.03925,
      "grad_norm": 3.815767288208008,
      "learning_rate": 4.7277039449175434e-05,
      "loss": 1.9581,
      "step": 3850
    },
    {
      "epoch": 1.0395,
      "grad_norm": 5.620586395263672,
      "learning_rate": 4.726765163741159e-05,
      "loss": 2.1342,
      "step": 3855
    },
    {
      "epoch": 1.03975,
      "grad_norm": 4.04993200302124,
      "learning_rate": 4.725824860591782e-05,
      "loss": 1.9699,
      "step": 3860
    },
    {
      "epoch": 1.04,
      "grad_norm": 4.687021732330322,
      "learning_rate": 4.7248830361121014e-05,
      "loss": 1.9668,
      "step": 3865
    },
    {
      "epoch": 1.04025,
      "grad_norm": 4.363482475280762,
      "learning_rate": 4.723939690945846e-05,
      "loss": 2.0777,
      "step": 3870
    },
    {
      "epoch": 1.0405,
      "grad_norm": 5.549733638763428,
      "learning_rate": 4.72299482573778e-05,
      "loss": 1.9162,
      "step": 3875
    },
    {
      "epoch": 1.04075,
      "grad_norm": 4.040524482727051,
      "learning_rate": 4.722048441133714e-05,
      "loss": 1.8528,
      "step": 3880
    },
    {
      "epoch": 1.041,
      "grad_norm": 4.283405780792236,
      "learning_rate": 4.7211005377804904e-05,
      "loss": 2.184,
      "step": 3885
    },
    {
      "epoch": 1.04125,
      "grad_norm": 5.075758934020996,
      "learning_rate": 4.720151116325994e-05,
      "loss": 2.1626,
      "step": 3890
    },
    {
      "epoch": 1.0415,
      "grad_norm": 4.010477542877197,
      "learning_rate": 4.719200177419143e-05,
      "loss": 2.1861,
      "step": 3895
    },
    {
      "epoch": 1.04175,
      "grad_norm": 4.563173294067383,
      "learning_rate": 4.718247721709898e-05,
      "loss": 2.1947,
      "step": 3900
    },
    {
      "epoch": 1.042,
      "grad_norm": 6.117631912231445,
      "learning_rate": 4.71729374984925e-05,
      "loss": 2.0854,
      "step": 3905
    },
    {
      "epoch": 1.04225,
      "grad_norm": 5.757574081420898,
      "learning_rate": 4.716338262489233e-05,
      "loss": 2.0878,
      "step": 3910
    },
    {
      "epoch": 1.0425,
      "grad_norm": 4.162769317626953,
      "learning_rate": 4.715381260282913e-05,
      "loss": 2.1656,
      "step": 3915
    },
    {
      "epoch": 1.04275,
      "grad_norm": 5.6960930824279785,
      "learning_rate": 4.71442274388439e-05,
      "loss": 2.13,
      "step": 3920
    },
    {
      "epoch": 1.043,
      "grad_norm": 7.33437442779541,
      "learning_rate": 4.713462713948804e-05,
      "loss": 2.0057,
      "step": 3925
    },
    {
      "epoch": 1.04325,
      "grad_norm": 5.097583293914795,
      "learning_rate": 4.712501171132323e-05,
      "loss": 2.381,
      "step": 3930
    },
    {
      "epoch": 1.0435,
      "grad_norm": 6.34027624130249,
      "learning_rate": 4.7115381160921556e-05,
      "loss": 2.2775,
      "step": 3935
    },
    {
      "epoch": 1.04375,
      "grad_norm": 3.9251132011413574,
      "learning_rate": 4.710573549486539e-05,
      "loss": 1.9316,
      "step": 3940
    },
    {
      "epoch": 1.044,
      "grad_norm": 4.696828365325928,
      "learning_rate": 4.709607471974745e-05,
      "loss": 1.9767,
      "step": 3945
    },
    {
      "epoch": 1.04425,
      "grad_norm": 4.338076591491699,
      "learning_rate": 4.70863988421708e-05,
      "loss": 1.9142,
      "step": 3950
    },
    {
      "epoch": 1.0445,
      "grad_norm": 4.060908794403076,
      "learning_rate": 4.707670786874881e-05,
      "loss": 1.9911,
      "step": 3955
    },
    {
      "epoch": 1.04475,
      "grad_norm": 5.923634052276611,
      "learning_rate": 4.7067001806105147e-05,
      "loss": 2.2224,
      "step": 3960
    },
    {
      "epoch": 1.045,
      "grad_norm": 4.0284342765808105,
      "learning_rate": 4.7057280660873835e-05,
      "loss": 2.0747,
      "step": 3965
    },
    {
      "epoch": 1.04525,
      "grad_norm": 4.900084018707275,
      "learning_rate": 4.7047544439699174e-05,
      "loss": 1.9838,
      "step": 3970
    },
    {
      "epoch": 1.0455,
      "grad_norm": 5.259332180023193,
      "learning_rate": 4.70377931492358e-05,
      "loss": 1.976,
      "step": 3975
    },
    {
      "epoch": 1.04575,
      "grad_norm": 4.34686279296875,
      "learning_rate": 4.70280267961486e-05,
      "loss": 1.9929,
      "step": 3980
    },
    {
      "epoch": 1.046,
      "grad_norm": 4.3407368659973145,
      "learning_rate": 4.70182453871128e-05,
      "loss": 1.9334,
      "step": 3985
    },
    {
      "epoch": 1.04625,
      "grad_norm": 5.214223384857178,
      "learning_rate": 4.70084489288139e-05,
      "loss": 2.2082,
      "step": 3990
    },
    {
      "epoch": 1.0465,
      "grad_norm": 4.077178001403809,
      "learning_rate": 4.699863742794768e-05,
      "loss": 1.9421,
      "step": 3995
    },
    {
      "epoch": 1.04675,
      "grad_norm": 4.981807231903076,
      "learning_rate": 4.698881089122022e-05,
      "loss": 1.9208,
      "step": 4000
    },
    {
      "epoch": 1.04675,
      "eval_loss": 1.8722341060638428,
      "eval_runtime": 5.3994,
      "eval_samples_per_second": 189.65,
      "eval_steps_per_second": 23.706,
      "step": 4000
    },
    {
      "epoch": 1.047,
      "grad_norm": 5.419761657714844,
      "learning_rate": 4.697896932534784e-05,
      "loss": 1.9566,
      "step": 4005
    },
    {
      "epoch": 1.04725,
      "grad_norm": 4.2793989181518555,
      "learning_rate": 4.6969112737057195e-05,
      "loss": 2.0841,
      "step": 4010
    },
    {
      "epoch": 1.0475,
      "grad_norm": 5.420416831970215,
      "learning_rate": 4.695924113308514e-05,
      "loss": 1.8742,
      "step": 4015
    },
    {
      "epoch": 1.04775,
      "grad_norm": 5.1646528244018555,
      "learning_rate": 4.6949354520178846e-05,
      "loss": 2.0833,
      "step": 4020
    },
    {
      "epoch": 1.048,
      "grad_norm": 4.3372979164123535,
      "learning_rate": 4.6939452905095696e-05,
      "loss": 2.1216,
      "step": 4025
    },
    {
      "epoch": 1.04825,
      "grad_norm": 5.890414237976074,
      "learning_rate": 4.692953629460336e-05,
      "loss": 2.2621,
      "step": 4030
    },
    {
      "epoch": 1.0485,
      "grad_norm": 4.005143165588379,
      "learning_rate": 4.691960469547976e-05,
      "loss": 2.0316,
      "step": 4035
    },
    {
      "epoch": 1.04875,
      "grad_norm": 3.8217575550079346,
      "learning_rate": 4.6909658114513035e-05,
      "loss": 2.0377,
      "step": 4040
    },
    {
      "epoch": 1.049,
      "grad_norm": 4.776891708374023,
      "learning_rate": 4.6899696558501585e-05,
      "loss": 2.1034,
      "step": 4045
    },
    {
      "epoch": 1.04925,
      "grad_norm": 4.655837535858154,
      "learning_rate": 4.6889720034254033e-05,
      "loss": 2.0102,
      "step": 4050
    },
    {
      "epoch": 1.0495,
      "grad_norm": 7.698810577392578,
      "learning_rate": 4.687972854858925e-05,
      "loss": 2.0473,
      "step": 4055
    },
    {
      "epoch": 1.04975,
      "grad_norm": 4.082536697387695,
      "learning_rate": 4.6869722108336323e-05,
      "loss": 1.7935,
      "step": 4060
    },
    {
      "epoch": 1.05,
      "grad_norm": 5.8715410232543945,
      "learning_rate": 4.6859700720334546e-05,
      "loss": 1.9781,
      "step": 4065
    },
    {
      "epoch": 1.05025,
      "grad_norm": 5.394929885864258,
      "learning_rate": 4.6849664391433455e-05,
      "loss": 2.1044,
      "step": 4070
    },
    {
      "epoch": 1.0505,
      "grad_norm": 4.387430191040039,
      "learning_rate": 4.683961312849279e-05,
      "loss": 2.0169,
      "step": 4075
    },
    {
      "epoch": 1.05075,
      "grad_norm": 3.9075124263763428,
      "learning_rate": 4.682954693838247e-05,
      "loss": 2.1409,
      "step": 4080
    },
    {
      "epoch": 1.051,
      "grad_norm": 4.151585102081299,
      "learning_rate": 4.681946582798267e-05,
      "loss": 1.9546,
      "step": 4085
    },
    {
      "epoch": 1.05125,
      "grad_norm": 4.117992877960205,
      "learning_rate": 4.680936980418372e-05,
      "loss": 2.0512,
      "step": 4090
    },
    {
      "epoch": 1.0515,
      "grad_norm": 4.018939018249512,
      "learning_rate": 4.679925887388615e-05,
      "loss": 1.9671,
      "step": 4095
    },
    {
      "epoch": 1.05175,
      "grad_norm": 5.734249114990234,
      "learning_rate": 4.678913304400071e-05,
      "loss": 2.1059,
      "step": 4100
    },
    {
      "epoch": 1.052,
      "grad_norm": 4.410394668579102,
      "learning_rate": 4.677899232144829e-05,
      "loss": 1.8484,
      "step": 4105
    },
    {
      "epoch": 1.05225,
      "grad_norm": 4.20674467086792,
      "learning_rate": 4.676883671315997e-05,
      "loss": 2.0282,
      "step": 4110
    },
    {
      "epoch": 1.0525,
      "grad_norm": 3.776916742324829,
      "learning_rate": 4.6758666226077034e-05,
      "loss": 1.9783,
      "step": 4115
    },
    {
      "epoch": 1.05275,
      "grad_norm": 5.18597412109375,
      "learning_rate": 4.67484808671509e-05,
      "loss": 2.0517,
      "step": 4120
    },
    {
      "epoch": 1.053,
      "grad_norm": 4.181754112243652,
      "learning_rate": 4.673828064334318e-05,
      "loss": 1.9314,
      "step": 4125
    },
    {
      "epoch": 1.05325,
      "grad_norm": 3.3259003162384033,
      "learning_rate": 4.672806556162561e-05,
      "loss": 1.9656,
      "step": 4130
    },
    {
      "epoch": 1.0535,
      "grad_norm": 4.928900241851807,
      "learning_rate": 4.671783562898011e-05,
      "loss": 1.914,
      "step": 4135
    },
    {
      "epoch": 1.05375,
      "grad_norm": 3.8681187629699707,
      "learning_rate": 4.670759085239875e-05,
      "loss": 1.9301,
      "step": 4140
    },
    {
      "epoch": 1.054,
      "grad_norm": 4.402945041656494,
      "learning_rate": 4.669733123888373e-05,
      "loss": 1.75,
      "step": 4145
    },
    {
      "epoch": 1.05425,
      "grad_norm": 5.4227986335754395,
      "learning_rate": 4.66870567954474e-05,
      "loss": 1.85,
      "step": 4150
    },
    {
      "epoch": 1.0545,
      "grad_norm": 4.677880764007568,
      "learning_rate": 4.667676752911225e-05,
      "loss": 2.1244,
      "step": 4155
    },
    {
      "epoch": 1.05475,
      "grad_norm": 3.9097025394439697,
      "learning_rate": 4.66664634469109e-05,
      "loss": 2.2506,
      "step": 4160
    },
    {
      "epoch": 1.055,
      "grad_norm": 5.1922478675842285,
      "learning_rate": 4.6656144555886076e-05,
      "loss": 1.965,
      "step": 4165
    },
    {
      "epoch": 1.05525,
      "grad_norm": 3.5001306533813477,
      "learning_rate": 4.664581086309066e-05,
      "loss": 2.0056,
      "step": 4170
    },
    {
      "epoch": 1.0555,
      "grad_norm": 4.363569259643555,
      "learning_rate": 4.663546237558762e-05,
      "loss": 1.9283,
      "step": 4175
    },
    {
      "epoch": 1.05575,
      "grad_norm": 4.105342864990234,
      "learning_rate": 4.662509910045006e-05,
      "loss": 2.0127,
      "step": 4180
    },
    {
      "epoch": 1.056,
      "grad_norm": 4.650881767272949,
      "learning_rate": 4.6614721044761175e-05,
      "loss": 1.881,
      "step": 4185
    },
    {
      "epoch": 1.05625,
      "grad_norm": 5.962282657623291,
      "learning_rate": 4.660432821561428e-05,
      "loss": 2.1366,
      "step": 4190
    },
    {
      "epoch": 1.0565,
      "grad_norm": 4.006699085235596,
      "learning_rate": 4.659392062011276e-05,
      "loss": 1.8362,
      "step": 4195
    },
    {
      "epoch": 1.05675,
      "grad_norm": 5.890232563018799,
      "learning_rate": 4.6583498265370117e-05,
      "loss": 2.1075,
      "step": 4200
    },
    {
      "epoch": 1.057,
      "grad_norm": 4.050289630889893,
      "learning_rate": 4.657306115850992e-05,
      "loss": 1.7546,
      "step": 4205
    },
    {
      "epoch": 1.05725,
      "grad_norm": 3.9772226810455322,
      "learning_rate": 4.656260930666586e-05,
      "loss": 1.998,
      "step": 4210
    },
    {
      "epoch": 1.0575,
      "grad_norm": 3.8273258209228516,
      "learning_rate": 4.655214271698165e-05,
      "loss": 1.9435,
      "step": 4215
    },
    {
      "epoch": 1.05775,
      "grad_norm": 4.074473857879639,
      "learning_rate": 4.654166139661112e-05,
      "loss": 1.7877,
      "step": 4220
    },
    {
      "epoch": 1.058,
      "grad_norm": 3.9250261783599854,
      "learning_rate": 4.653116535271816e-05,
      "loss": 1.7583,
      "step": 4225
    },
    {
      "epoch": 1.05825,
      "grad_norm": 4.939465522766113,
      "learning_rate": 4.652065459247669e-05,
      "loss": 1.7227,
      "step": 4230
    },
    {
      "epoch": 1.0585,
      "grad_norm": 4.297363758087158,
      "learning_rate": 4.651012912307074e-05,
      "loss": 2.0881,
      "step": 4235
    },
    {
      "epoch": 1.05875,
      "grad_norm": 6.320962905883789,
      "learning_rate": 4.649958895169437e-05,
      "loss": 1.8752,
      "step": 4240
    },
    {
      "epoch": 1.059,
      "grad_norm": 4.60801362991333,
      "learning_rate": 4.648903408555168e-05,
      "loss": 2.111,
      "step": 4245
    },
    {
      "epoch": 1.05925,
      "grad_norm": 3.3736000061035156,
      "learning_rate": 4.647846453185681e-05,
      "loss": 1.8347,
      "step": 4250
    },
    {
      "epoch": 1.0594999999999999,
      "grad_norm": 5.0893144607543945,
      "learning_rate": 4.646788029783398e-05,
      "loss": 2.1165,
      "step": 4255
    },
    {
      "epoch": 1.05975,
      "grad_norm": 6.422370910644531,
      "learning_rate": 4.645728139071738e-05,
      "loss": 1.9727,
      "step": 4260
    },
    {
      "epoch": 1.06,
      "grad_norm": 5.219328880310059,
      "learning_rate": 4.644666781775129e-05,
      "loss": 2.1378,
      "step": 4265
    },
    {
      "epoch": 1.06025,
      "grad_norm": 4.730388164520264,
      "learning_rate": 4.643603958618997e-05,
      "loss": 1.9564,
      "step": 4270
    },
    {
      "epoch": 1.0605,
      "grad_norm": 4.917454719543457,
      "learning_rate": 4.6425396703297724e-05,
      "loss": 2.2729,
      "step": 4275
    },
    {
      "epoch": 1.06075,
      "grad_norm": 4.185992240905762,
      "learning_rate": 4.6414739176348866e-05,
      "loss": 1.8566,
      "step": 4280
    },
    {
      "epoch": 1.061,
      "grad_norm": 4.183476448059082,
      "learning_rate": 4.6404067012627704e-05,
      "loss": 1.9256,
      "step": 4285
    },
    {
      "epoch": 1.06125,
      "grad_norm": 4.442063808441162,
      "learning_rate": 4.6393380219428575e-05,
      "loss": 1.9642,
      "step": 4290
    },
    {
      "epoch": 1.0615,
      "grad_norm": 3.8223366737365723,
      "learning_rate": 4.638267880405578e-05,
      "loss": 1.9141,
      "step": 4295
    },
    {
      "epoch": 1.06175,
      "grad_norm": 4.94193172454834,
      "learning_rate": 4.6371962773823665e-05,
      "loss": 1.9004,
      "step": 4300
    },
    {
      "epoch": 1.062,
      "grad_norm": 4.276530742645264,
      "learning_rate": 4.636123213605651e-05,
      "loss": 1.9622,
      "step": 4305
    },
    {
      "epoch": 1.06225,
      "grad_norm": 5.498557090759277,
      "learning_rate": 4.6350486898088615e-05,
      "loss": 2.0265,
      "step": 4310
    },
    {
      "epoch": 1.0625,
      "grad_norm": 4.01211404800415,
      "learning_rate": 4.633972706726425e-05,
      "loss": 1.728,
      "step": 4315
    },
    {
      "epoch": 1.06275,
      "grad_norm": 5.3501458168029785,
      "learning_rate": 4.632895265093765e-05,
      "loss": 1.8675,
      "step": 4320
    },
    {
      "epoch": 1.063,
      "grad_norm": 5.1605634689331055,
      "learning_rate": 4.6318163656473043e-05,
      "loss": 2.0043,
      "step": 4325
    },
    {
      "epoch": 1.06325,
      "grad_norm": 5.8346662521362305,
      "learning_rate": 4.630736009124459e-05,
      "loss": 2.031,
      "step": 4330
    },
    {
      "epoch": 1.0635,
      "grad_norm": 3.63301420211792,
      "learning_rate": 4.629654196263643e-05,
      "loss": 1.7523,
      "step": 4335
    },
    {
      "epoch": 1.06375,
      "grad_norm": 4.274652481079102,
      "learning_rate": 4.628570927804265e-05,
      "loss": 1.8117,
      "step": 4340
    },
    {
      "epoch": 1.064,
      "grad_norm": 5.609598636627197,
      "learning_rate": 4.6274862044867304e-05,
      "loss": 1.8024,
      "step": 4345
    },
    {
      "epoch": 1.06425,
      "grad_norm": 6.669052600860596,
      "learning_rate": 4.626400027052434e-05,
      "loss": 2.0943,
      "step": 4350
    },
    {
      "epoch": 1.0645,
      "grad_norm": 3.3530704975128174,
      "learning_rate": 4.625312396243772e-05,
      "loss": 1.8511,
      "step": 4355
    },
    {
      "epoch": 1.06475,
      "grad_norm": 5.506870269775391,
      "learning_rate": 4.624223312804126e-05,
      "loss": 2.0634,
      "step": 4360
    },
    {
      "epoch": 1.065,
      "grad_norm": 6.082592964172363,
      "learning_rate": 4.6231327774778766e-05,
      "loss": 2.1106,
      "step": 4365
    },
    {
      "epoch": 1.06525,
      "grad_norm": 5.798084735870361,
      "learning_rate": 4.6220407910103943e-05,
      "loss": 2.1332,
      "step": 4370
    },
    {
      "epoch": 1.0655000000000001,
      "grad_norm": 4.998977184295654,
      "learning_rate": 4.62094735414804e-05,
      "loss": 1.956,
      "step": 4375
    },
    {
      "epoch": 1.06575,
      "grad_norm": 4.88196325302124,
      "learning_rate": 4.6198524676381694e-05,
      "loss": 1.9593,
      "step": 4380
    },
    {
      "epoch": 1.066,
      "grad_norm": 5.304676532745361,
      "learning_rate": 4.618756132229125e-05,
      "loss": 1.9058,
      "step": 4385
    },
    {
      "epoch": 1.06625,
      "grad_norm": 4.647281169891357,
      "learning_rate": 4.617658348670244e-05,
      "loss": 2.15,
      "step": 4390
    },
    {
      "epoch": 1.0665,
      "grad_norm": 4.620741844177246,
      "learning_rate": 4.616559117711849e-05,
      "loss": 2.0021,
      "step": 4395
    },
    {
      "epoch": 1.06675,
      "grad_norm": 5.678706169128418,
      "learning_rate": 4.615458440105256e-05,
      "loss": 1.9233,
      "step": 4400
    },
    {
      "epoch": 1.067,
      "grad_norm": 4.459969997406006,
      "learning_rate": 4.614356316602766e-05,
      "loss": 1.9922,
      "step": 4405
    },
    {
      "epoch": 1.06725,
      "grad_norm": 4.538761138916016,
      "learning_rate": 4.613252747957671e-05,
      "loss": 1.7594,
      "step": 4410
    },
    {
      "epoch": 1.0675,
      "grad_norm": 4.694785118103027,
      "learning_rate": 4.6121477349242505e-05,
      "loss": 1.7887,
      "step": 4415
    },
    {
      "epoch": 1.06775,
      "grad_norm": 4.414416313171387,
      "learning_rate": 4.611041278257768e-05,
      "loss": 1.8371,
      "step": 4420
    },
    {
      "epoch": 1.068,
      "grad_norm": 3.046703577041626,
      "learning_rate": 4.6099333787144795e-05,
      "loss": 1.9186,
      "step": 4425
    },
    {
      "epoch": 1.06825,
      "grad_norm": 3.746440887451172,
      "learning_rate": 4.608824037051621e-05,
      "loss": 1.8573,
      "step": 4430
    },
    {
      "epoch": 1.0685,
      "grad_norm": 3.844529867172241,
      "learning_rate": 4.607713254027418e-05,
      "loss": 1.8133,
      "step": 4435
    },
    {
      "epoch": 1.06875,
      "grad_norm": 3.6625304222106934,
      "learning_rate": 4.606601030401081e-05,
      "loss": 1.7459,
      "step": 4440
    },
    {
      "epoch": 1.069,
      "grad_norm": 2.98250412940979,
      "learning_rate": 4.6054873669328035e-05,
      "loss": 1.6795,
      "step": 4445
    },
    {
      "epoch": 1.06925,
      "grad_norm": 4.797673225402832,
      "learning_rate": 4.604372264383764e-05,
      "loss": 1.7813,
      "step": 4450
    },
    {
      "epoch": 1.0695000000000001,
      "grad_norm": 2.865541696548462,
      "learning_rate": 4.6032557235161255e-05,
      "loss": 1.6438,
      "step": 4455
    },
    {
      "epoch": 1.06975,
      "grad_norm": 2.757150173187256,
      "learning_rate": 4.602137745093032e-05,
      "loss": 1.7239,
      "step": 4460
    },
    {
      "epoch": 1.07,
      "grad_norm": 4.107379913330078,
      "learning_rate": 4.601018329878611e-05,
      "loss": 1.8414,
      "step": 4465
    },
    {
      "epoch": 1.07025,
      "grad_norm": 3.8767716884613037,
      "learning_rate": 4.5998974786379744e-05,
      "loss": 1.5372,
      "step": 4470
    },
    {
      "epoch": 1.0705,
      "grad_norm": 3.1281259059906006,
      "learning_rate": 4.5987751921372116e-05,
      "loss": 1.6966,
      "step": 4475
    },
    {
      "epoch": 1.07075,
      "grad_norm": 3.5663375854492188,
      "learning_rate": 4.597651471143395e-05,
      "loss": 1.6446,
      "step": 4480
    },
    {
      "epoch": 1.071,
      "grad_norm": 3.5435259342193604,
      "learning_rate": 4.596526316424578e-05,
      "loss": 1.5141,
      "step": 4485
    },
    {
      "epoch": 1.07125,
      "grad_norm": 3.047060251235962,
      "learning_rate": 4.5953997287497916e-05,
      "loss": 1.629,
      "step": 4490
    },
    {
      "epoch": 1.0715,
      "grad_norm": 3.818629741668701,
      "learning_rate": 4.59427170888905e-05,
      "loss": 1.7393,
      "step": 4495
    },
    {
      "epoch": 1.07175,
      "grad_norm": 4.264594078063965,
      "learning_rate": 4.593142257613343e-05,
      "loss": 1.9764,
      "step": 4500
    },
    {
      "epoch": 1.07175,
      "eval_loss": 1.93582022190094,
      "eval_runtime": 5.1643,
      "eval_samples_per_second": 198.284,
      "eval_steps_per_second": 24.785,
      "step": 4500
    },
    {
      "epoch": 1.072,
      "grad_norm": 3.4849753379821777,
      "learning_rate": 4.5920113756946396e-05,
      "loss": 1.7958,
      "step": 4505
    },
    {
      "epoch": 1.07225,
      "grad_norm": 3.2161712646484375,
      "learning_rate": 4.5908790639058884e-05,
      "loss": 1.6951,
      "step": 4510
    },
    {
      "epoch": 1.0725,
      "grad_norm": 3.531079053878784,
      "learning_rate": 4.589745323021012e-05,
      "loss": 1.5001,
      "step": 4515
    },
    {
      "epoch": 1.07275,
      "grad_norm": 3.903332471847534,
      "learning_rate": 4.5886101538149126e-05,
      "loss": 1.7516,
      "step": 4520
    },
    {
      "epoch": 1.073,
      "grad_norm": 3.484504461288452,
      "learning_rate": 4.587473557063468e-05,
      "loss": 1.7298,
      "step": 4525
    },
    {
      "epoch": 1.07325,
      "grad_norm": 4.695910453796387,
      "learning_rate": 4.5863355335435295e-05,
      "loss": 1.7018,
      "step": 4530
    },
    {
      "epoch": 1.0735,
      "grad_norm": 4.44573974609375,
      "learning_rate": 4.585196084032928e-05,
      "loss": 1.5304,
      "step": 4535
    },
    {
      "epoch": 1.07375,
      "grad_norm": 3.0215213298797607,
      "learning_rate": 4.584055209310465e-05,
      "loss": 1.7813,
      "step": 4540
    },
    {
      "epoch": 1.074,
      "grad_norm": 3.8151867389678955,
      "learning_rate": 4.582912910155918e-05,
      "loss": 1.6221,
      "step": 4545
    },
    {
      "epoch": 1.07425,
      "grad_norm": 3.0081217288970947,
      "learning_rate": 4.581769187350038e-05,
      "loss": 1.6912,
      "step": 4550
    },
    {
      "epoch": 1.0745,
      "grad_norm": 4.7459306716918945,
      "learning_rate": 4.580624041674547e-05,
      "loss": 1.8175,
      "step": 4555
    },
    {
      "epoch": 1.07475,
      "grad_norm": 2.590607166290283,
      "learning_rate": 4.579477473912144e-05,
      "loss": 1.5407,
      "step": 4560
    },
    {
      "epoch": 1.075,
      "grad_norm": 2.9012176990509033,
      "learning_rate": 4.578329484846495e-05,
      "loss": 1.498,
      "step": 4565
    },
    {
      "epoch": 1.07525,
      "grad_norm": 3.8135111331939697,
      "learning_rate": 4.577180075262241e-05,
      "loss": 1.7926,
      "step": 4570
    },
    {
      "epoch": 1.0755,
      "grad_norm": 3.307680368423462,
      "learning_rate": 4.576029245944993e-05,
      "loss": 1.7739,
      "step": 4575
    },
    {
      "epoch": 1.07575,
      "grad_norm": 4.308133125305176,
      "learning_rate": 4.57487699768133e-05,
      "loss": 1.7146,
      "step": 4580
    },
    {
      "epoch": 1.076,
      "grad_norm": 4.004507064819336,
      "learning_rate": 4.573723331258804e-05,
      "loss": 1.8364,
      "step": 4585
    },
    {
      "epoch": 1.07625,
      "grad_norm": 3.3620262145996094,
      "learning_rate": 4.572568247465935e-05,
      "loss": 1.6518,
      "step": 4590
    },
    {
      "epoch": 1.0765,
      "grad_norm": 3.3721745014190674,
      "learning_rate": 4.571411747092213e-05,
      "loss": 1.6119,
      "step": 4595
    },
    {
      "epoch": 1.07675,
      "grad_norm": 3.8430206775665283,
      "learning_rate": 4.570253830928094e-05,
      "loss": 1.633,
      "step": 4600
    },
    {
      "epoch": 1.077,
      "grad_norm": 3.9339239597320557,
      "learning_rate": 4.569094499765002e-05,
      "loss": 1.5372,
      "step": 4605
    },
    {
      "epoch": 1.07725,
      "grad_norm": 3.3259096145629883,
      "learning_rate": 4.5679337543953306e-05,
      "loss": 1.6663,
      "step": 4610
    },
    {
      "epoch": 1.0775,
      "grad_norm": 3.8993513584136963,
      "learning_rate": 4.566771595612437e-05,
      "loss": 1.6942,
      "step": 4615
    },
    {
      "epoch": 1.07775,
      "grad_norm": 3.5801916122436523,
      "learning_rate": 4.5656080242106455e-05,
      "loss": 1.5506,
      "step": 4620
    },
    {
      "epoch": 1.078,
      "grad_norm": 3.3749828338623047,
      "learning_rate": 4.564443040985249e-05,
      "loss": 1.6455,
      "step": 4625
    },
    {
      "epoch": 1.07825,
      "grad_norm": 3.1090173721313477,
      "learning_rate": 4.563276646732499e-05,
      "loss": 1.5572,
      "step": 4630
    },
    {
      "epoch": 1.0785,
      "grad_norm": 3.9866669178009033,
      "learning_rate": 4.5621088422496175e-05,
      "loss": 1.5895,
      "step": 4635
    },
    {
      "epoch": 1.07875,
      "grad_norm": 3.461456298828125,
      "learning_rate": 4.560939628334786e-05,
      "loss": 1.5799,
      "step": 4640
    },
    {
      "epoch": 1.079,
      "grad_norm": 5.070949554443359,
      "learning_rate": 4.5597690057871525e-05,
      "loss": 1.6377,
      "step": 4645
    },
    {
      "epoch": 1.07925,
      "grad_norm": 3.9355008602142334,
      "learning_rate": 4.558596975406826e-05,
      "loss": 1.6994,
      "step": 4650
    },
    {
      "epoch": 1.0795,
      "grad_norm": 3.300117015838623,
      "learning_rate": 4.5574235379948776e-05,
      "loss": 1.5971,
      "step": 4655
    },
    {
      "epoch": 1.07975,
      "grad_norm": 4.908795356750488,
      "learning_rate": 4.5562486943533423e-05,
      "loss": 1.6883,
      "step": 4660
    },
    {
      "epoch": 1.08,
      "grad_norm": 2.840364933013916,
      "learning_rate": 4.555072445285213e-05,
      "loss": 1.5831,
      "step": 4665
    },
    {
      "epoch": 1.08025,
      "grad_norm": 3.2211973667144775,
      "learning_rate": 4.553894791594445e-05,
      "loss": 1.7231,
      "step": 4670
    },
    {
      "epoch": 1.0805,
      "grad_norm": 3.703331470489502,
      "learning_rate": 4.552715734085955e-05,
      "loss": 1.5677,
      "step": 4675
    },
    {
      "epoch": 1.08075,
      "grad_norm": 5.291966915130615,
      "learning_rate": 4.551535273565616e-05,
      "loss": 1.6836,
      "step": 4680
    },
    {
      "epoch": 1.081,
      "grad_norm": 3.4194083213806152,
      "learning_rate": 4.5503534108402635e-05,
      "loss": 1.6681,
      "step": 4685
    },
    {
      "epoch": 1.08125,
      "grad_norm": 3.7608985900878906,
      "learning_rate": 4.5491701467176875e-05,
      "loss": 1.7158,
      "step": 4690
    },
    {
      "epoch": 1.0815,
      "grad_norm": 3.2399110794067383,
      "learning_rate": 4.5479854820066385e-05,
      "loss": 1.6527,
      "step": 4695
    },
    {
      "epoch": 1.08175,
      "grad_norm": 2.8777942657470703,
      "learning_rate": 4.5467994175168246e-05,
      "loss": 1.6318,
      "step": 4700
    },
    {
      "epoch": 1.082,
      "grad_norm": 3.021608829498291,
      "learning_rate": 4.545611954058909e-05,
      "loss": 1.7079,
      "step": 4705
    },
    {
      "epoch": 1.08225,
      "grad_norm": 3.4349005222320557,
      "learning_rate": 4.544423092444511e-05,
      "loss": 1.5973,
      "step": 4710
    },
    {
      "epoch": 1.0825,
      "grad_norm": 3.487009048461914,
      "learning_rate": 4.5432328334862074e-05,
      "loss": 1.6924,
      "step": 4715
    },
    {
      "epoch": 1.08275,
      "grad_norm": 3.43142032623291,
      "learning_rate": 4.542041177997529e-05,
      "loss": 1.6263,
      "step": 4720
    },
    {
      "epoch": 1.083,
      "grad_norm": 3.453925132751465,
      "learning_rate": 4.5408481267929605e-05,
      "loss": 1.6072,
      "step": 4725
    },
    {
      "epoch": 1.08325,
      "grad_norm": 4.111630916595459,
      "learning_rate": 4.539653680687941e-05,
      "loss": 1.6097,
      "step": 4730
    },
    {
      "epoch": 1.0835,
      "grad_norm": 4.692383289337158,
      "learning_rate": 4.5384578404988637e-05,
      "loss": 1.7791,
      "step": 4735
    },
    {
      "epoch": 1.08375,
      "grad_norm": 3.074538230895996,
      "learning_rate": 4.5372606070430745e-05,
      "loss": 1.5235,
      "step": 4740
    },
    {
      "epoch": 1.084,
      "grad_norm": 4.155384540557861,
      "learning_rate": 4.53606198113887e-05,
      "loss": 1.4729,
      "step": 4745
    },
    {
      "epoch": 1.08425,
      "grad_norm": 2.8819973468780518,
      "learning_rate": 4.5348619636055004e-05,
      "loss": 1.4062,
      "step": 4750
    },
    {
      "epoch": 1.0845,
      "grad_norm": 5.038691997528076,
      "learning_rate": 4.533660555263166e-05,
      "loss": 1.709,
      "step": 4755
    },
    {
      "epoch": 1.08475,
      "grad_norm": 3.2347469329833984,
      "learning_rate": 4.532457756933019e-05,
      "loss": 1.4836,
      "step": 4760
    },
    {
      "epoch": 1.085,
      "grad_norm": 3.980374336242676,
      "learning_rate": 4.53125356943716e-05,
      "loss": 1.4538,
      "step": 4765
    },
    {
      "epoch": 1.08525,
      "grad_norm": 3.754215717315674,
      "learning_rate": 4.5300479935986396e-05,
      "loss": 1.5026,
      "step": 4770
    },
    {
      "epoch": 1.0855,
      "grad_norm": 3.1842422485351562,
      "learning_rate": 4.528841030241458e-05,
      "loss": 1.4409,
      "step": 4775
    },
    {
      "epoch": 1.08575,
      "grad_norm": 3.5200135707855225,
      "learning_rate": 4.527632680190563e-05,
      "loss": 1.533,
      "step": 4780
    },
    {
      "epoch": 1.086,
      "grad_norm": 3.4490599632263184,
      "learning_rate": 4.52642294427185e-05,
      "loss": 1.6684,
      "step": 4785
    },
    {
      "epoch": 1.08625,
      "grad_norm": 3.7956504821777344,
      "learning_rate": 4.525211823312163e-05,
      "loss": 1.4075,
      "step": 4790
    },
    {
      "epoch": 1.0865,
      "grad_norm": 3.1324305534362793,
      "learning_rate": 4.5239993181392913e-05,
      "loss": 1.4321,
      "step": 4795
    },
    {
      "epoch": 1.0867499999999999,
      "grad_norm": 3.3575289249420166,
      "learning_rate": 4.522785429581971e-05,
      "loss": 1.481,
      "step": 4800
    },
    {
      "epoch": 1.087,
      "grad_norm": 4.737222671508789,
      "learning_rate": 4.521570158469883e-05,
      "loss": 1.6642,
      "step": 4805
    },
    {
      "epoch": 1.08725,
      "grad_norm": 3.583317518234253,
      "learning_rate": 4.520353505633655e-05,
      "loss": 1.5447,
      "step": 4810
    },
    {
      "epoch": 1.0875,
      "grad_norm": 4.53584098815918,
      "learning_rate": 4.5191354719048556e-05,
      "loss": 1.6171,
      "step": 4815
    },
    {
      "epoch": 1.08775,
      "grad_norm": 4.127008438110352,
      "learning_rate": 4.5179160581160005e-05,
      "loss": 1.5021,
      "step": 4820
    },
    {
      "epoch": 1.088,
      "grad_norm": 3.919621467590332,
      "learning_rate": 4.5166952651005486e-05,
      "loss": 1.49,
      "step": 4825
    },
    {
      "epoch": 1.08825,
      "grad_norm": 3.712592124938965,
      "learning_rate": 4.515473093692898e-05,
      "loss": 1.5239,
      "step": 4830
    },
    {
      "epoch": 1.0885,
      "grad_norm": 3.876936435699463,
      "learning_rate": 4.514249544728393e-05,
      "loss": 1.3948,
      "step": 4835
    },
    {
      "epoch": 1.08875,
      "grad_norm": 3.791917324066162,
      "learning_rate": 4.5130246190433176e-05,
      "loss": 1.598,
      "step": 4840
    },
    {
      "epoch": 1.089,
      "grad_norm": 3.397174596786499,
      "learning_rate": 4.5117983174748966e-05,
      "loss": 1.5646,
      "step": 4845
    },
    {
      "epoch": 1.08925,
      "grad_norm": 3.8309688568115234,
      "learning_rate": 4.5105706408612957e-05,
      "loss": 1.5431,
      "step": 4850
    },
    {
      "epoch": 1.0895,
      "grad_norm": 4.368788242340088,
      "learning_rate": 4.50934159004162e-05,
      "loss": 1.5605,
      "step": 4855
    },
    {
      "epoch": 1.08975,
      "grad_norm": 4.061962604522705,
      "learning_rate": 4.508111165855915e-05,
      "loss": 1.4127,
      "step": 4860
    },
    {
      "epoch": 1.09,
      "grad_norm": 3.730299949645996,
      "learning_rate": 4.5068793691451635e-05,
      "loss": 1.5641,
      "step": 4865
    },
    {
      "epoch": 1.09025,
      "grad_norm": 3.321775197982788,
      "learning_rate": 4.505646200751287e-05,
      "loss": 1.5581,
      "step": 4870
    },
    {
      "epoch": 1.0905,
      "grad_norm": 3.9858102798461914,
      "learning_rate": 4.504411661517144e-05,
      "loss": 1.4059,
      "step": 4875
    },
    {
      "epoch": 1.0907499999999999,
      "grad_norm": 3.87475323677063,
      "learning_rate": 4.5031757522865314e-05,
      "loss": 1.2812,
      "step": 4880
    },
    {
      "epoch": 1.091,
      "grad_norm": 3.1079070568084717,
      "learning_rate": 4.5019384739041804e-05,
      "loss": 1.4973,
      "step": 4885
    },
    {
      "epoch": 1.09125,
      "grad_norm": 3.4038779735565186,
      "learning_rate": 4.5006998272157594e-05,
      "loss": 1.5034,
      "step": 4890
    },
    {
      "epoch": 1.0915,
      "grad_norm": 3.880481719970703,
      "learning_rate": 4.499459813067873e-05,
      "loss": 1.6934,
      "step": 4895
    },
    {
      "epoch": 1.09175,
      "grad_norm": 3.194786310195923,
      "learning_rate": 4.498218432308058e-05,
      "loss": 1.7209,
      "step": 4900
    },
    {
      "epoch": 1.092,
      "grad_norm": 3.3032357692718506,
      "learning_rate": 4.4969756857847854e-05,
      "loss": 1.4691,
      "step": 4905
    },
    {
      "epoch": 1.09225,
      "grad_norm": 3.456852674484253,
      "learning_rate": 4.495731574347462e-05,
      "loss": 1.6539,
      "step": 4910
    },
    {
      "epoch": 1.0925,
      "grad_norm": 3.7419190406799316,
      "learning_rate": 4.4944860988464276e-05,
      "loss": 1.4563,
      "step": 4915
    },
    {
      "epoch": 1.09275,
      "grad_norm": 3.3801889419555664,
      "learning_rate": 4.493239260132951e-05,
      "loss": 1.6066,
      "step": 4920
    },
    {
      "epoch": 1.093,
      "grad_norm": 4.346551895141602,
      "learning_rate": 4.491991059059235e-05,
      "loss": 1.5262,
      "step": 4925
    },
    {
      "epoch": 1.09325,
      "grad_norm": 3.011253595352173,
      "learning_rate": 4.4907414964784135e-05,
      "loss": 1.6514,
      "step": 4930
    },
    {
      "epoch": 1.0935,
      "grad_norm": 4.14349365234375,
      "learning_rate": 4.48949057324455e-05,
      "loss": 1.703,
      "step": 4935
    },
    {
      "epoch": 1.09375,
      "grad_norm": 3.8955419063568115,
      "learning_rate": 4.488238290212641e-05,
      "loss": 1.7426,
      "step": 4940
    },
    {
      "epoch": 1.094,
      "grad_norm": 3.3793601989746094,
      "learning_rate": 4.486984648238607e-05,
      "loss": 1.5003,
      "step": 4945
    },
    {
      "epoch": 1.09425,
      "grad_norm": 4.496316432952881,
      "learning_rate": 4.485729648179303e-05,
      "loss": 1.537,
      "step": 4950
    },
    {
      "epoch": 1.0945,
      "grad_norm": 2.7266664505004883,
      "learning_rate": 4.4844732908925084e-05,
      "loss": 1.4191,
      "step": 4955
    },
    {
      "epoch": 1.09475,
      "grad_norm": 3.5908684730529785,
      "learning_rate": 4.483215577236932e-05,
      "loss": 1.6473,
      "step": 4960
    },
    {
      "epoch": 1.095,
      "grad_norm": 4.489529609680176,
      "learning_rate": 4.4819565080722095e-05,
      "loss": 1.6514,
      "step": 4965
    },
    {
      "epoch": 1.09525,
      "grad_norm": 3.9849066734313965,
      "learning_rate": 4.4806960842589016e-05,
      "loss": 1.5761,
      "step": 4970
    },
    {
      "epoch": 1.0955,
      "grad_norm": 3.6190974712371826,
      "learning_rate": 4.479434306658498e-05,
      "loss": 1.6338,
      "step": 4975
    },
    {
      "epoch": 1.09575,
      "grad_norm": 4.029214859008789,
      "learning_rate": 4.4781711761334106e-05,
      "loss": 1.6285,
      "step": 4980
    },
    {
      "epoch": 1.096,
      "grad_norm": 4.199014663696289,
      "learning_rate": 4.476906693546977e-05,
      "loss": 1.6338,
      "step": 4985
    },
    {
      "epoch": 1.09625,
      "grad_norm": 3.5012056827545166,
      "learning_rate": 4.47564085976346e-05,
      "loss": 1.591,
      "step": 4990
    },
    {
      "epoch": 1.0965,
      "grad_norm": 3.2599923610687256,
      "learning_rate": 4.474373675648044e-05,
      "loss": 1.5069,
      "step": 4995
    },
    {
      "epoch": 1.0967500000000001,
      "grad_norm": 3.175414800643921,
      "learning_rate": 4.473105142066838e-05,
      "loss": 1.5463,
      "step": 5000
    },
    {
      "epoch": 1.0967500000000001,
      "eval_loss": 2.0399317741394043,
      "eval_runtime": 5.3502,
      "eval_samples_per_second": 191.396,
      "eval_steps_per_second": 23.925,
      "step": 5000
    },
    {
      "epoch": 1.097,
      "grad_norm": 3.965646266937256,
      "learning_rate": 4.4718352598868723e-05,
      "loss": 1.5263,
      "step": 5005
    },
    {
      "epoch": 1.09725,
      "grad_norm": 3.439690351486206,
      "learning_rate": 4.4705640299761007e-05,
      "loss": 1.478,
      "step": 5010
    },
    {
      "epoch": 1.0975,
      "grad_norm": 8.87918472290039,
      "learning_rate": 4.469291453203395e-05,
      "loss": 1.6565,
      "step": 5015
    },
    {
      "epoch": 1.09775,
      "grad_norm": 5.40011739730835,
      "learning_rate": 4.468017530438551e-05,
      "loss": 1.5777,
      "step": 5020
    },
    {
      "epoch": 1.098,
      "grad_norm": 3.8222219944000244,
      "learning_rate": 4.466742262552281e-05,
      "loss": 1.6345,
      "step": 5025
    },
    {
      "epoch": 1.09825,
      "grad_norm": 3.3322908878326416,
      "learning_rate": 4.4654656504162214e-05,
      "loss": 1.4437,
      "step": 5030
    },
    {
      "epoch": 1.0985,
      "grad_norm": 3.717231035232544,
      "learning_rate": 4.4641876949029224e-05,
      "loss": 1.5707,
      "step": 5035
    },
    {
      "epoch": 1.09875,
      "grad_norm": 3.378216028213501,
      "learning_rate": 4.4629083968858555e-05,
      "loss": 1.5849,
      "step": 5040
    },
    {
      "epoch": 1.099,
      "grad_norm": 2.8108742237091064,
      "learning_rate": 4.461627757239407e-05,
      "loss": 1.4505,
      "step": 5045
    },
    {
      "epoch": 1.09925,
      "grad_norm": 6.004570007324219,
      "learning_rate": 4.460345776838885e-05,
      "loss": 1.711,
      "step": 5050
    },
    {
      "epoch": 1.0995,
      "grad_norm": 2.5549368858337402,
      "learning_rate": 4.459062456560509e-05,
      "loss": 1.5241,
      "step": 5055
    },
    {
      "epoch": 1.09975,
      "grad_norm": 3.2860043048858643,
      "learning_rate": 4.457777797281417e-05,
      "loss": 1.4397,
      "step": 5060
    },
    {
      "epoch": 1.1,
      "grad_norm": 3.321780204772949,
      "learning_rate": 4.456491799879661e-05,
      "loss": 1.6536,
      "step": 5065
    },
    {
      "epoch": 1.10025,
      "grad_norm": 4.824212074279785,
      "learning_rate": 4.455204465234208e-05,
      "loss": 1.6869,
      "step": 5070
    },
    {
      "epoch": 1.1005,
      "grad_norm": 3.374572515487671,
      "learning_rate": 4.4539157942249396e-05,
      "loss": 1.552,
      "step": 5075
    },
    {
      "epoch": 1.1007500000000001,
      "grad_norm": 4.0737223625183105,
      "learning_rate": 4.4526257877326515e-05,
      "loss": 1.556,
      "step": 5080
    },
    {
      "epoch": 1.101,
      "grad_norm": 4.928226947784424,
      "learning_rate": 4.451334446639048e-05,
      "loss": 1.688,
      "step": 5085
    },
    {
      "epoch": 1.10125,
      "grad_norm": 6.874153137207031,
      "learning_rate": 4.450041771826751e-05,
      "loss": 1.6927,
      "step": 5090
    },
    {
      "epoch": 1.1015,
      "grad_norm": 2.933840274810791,
      "learning_rate": 4.448747764179291e-05,
      "loss": 1.4417,
      "step": 5095
    },
    {
      "epoch": 1.10175,
      "grad_norm": 3.1355690956115723,
      "learning_rate": 4.447452424581109e-05,
      "loss": 1.508,
      "step": 5100
    },
    {
      "epoch": 1.102,
      "grad_norm": 4.030665874481201,
      "learning_rate": 4.4461557539175594e-05,
      "loss": 1.4901,
      "step": 5105
    },
    {
      "epoch": 1.10225,
      "grad_norm": 3.7816247940063477,
      "learning_rate": 4.444857753074901e-05,
      "loss": 1.5334,
      "step": 5110
    },
    {
      "epoch": 1.1025,
      "grad_norm": 3.01961612701416,
      "learning_rate": 4.443558422940309e-05,
      "loss": 1.4889,
      "step": 5115
    },
    {
      "epoch": 1.10275,
      "grad_norm": 3.8019180297851562,
      "learning_rate": 4.442257764401861e-05,
      "loss": 1.4642,
      "step": 5120
    },
    {
      "epoch": 1.103,
      "grad_norm": 3.8167366981506348,
      "learning_rate": 4.4409557783485464e-05,
      "loss": 1.6369,
      "step": 5125
    },
    {
      "epoch": 1.10325,
      "grad_norm": 3.6335556507110596,
      "learning_rate": 4.4396524656702584e-05,
      "loss": 1.3222,
      "step": 5130
    },
    {
      "epoch": 1.1035,
      "grad_norm": 4.944820880889893,
      "learning_rate": 4.438347827257801e-05,
      "loss": 1.5627,
      "step": 5135
    },
    {
      "epoch": 1.10375,
      "grad_norm": 2.6292834281921387,
      "learning_rate": 4.4370418640028795e-05,
      "loss": 1.2873,
      "step": 5140
    },
    {
      "epoch": 1.104,
      "grad_norm": 3.1514668464660645,
      "learning_rate": 4.43573457679811e-05,
      "loss": 1.6683,
      "step": 5145
    },
    {
      "epoch": 1.10425,
      "grad_norm": 4.401288032531738,
      "learning_rate": 4.4344259665370106e-05,
      "loss": 1.6357,
      "step": 5150
    },
    {
      "epoch": 1.1045,
      "grad_norm": 3.295553684234619,
      "learning_rate": 4.433116034114003e-05,
      "loss": 1.5389,
      "step": 5155
    },
    {
      "epoch": 1.10475,
      "grad_norm": 3.804469347000122,
      "learning_rate": 4.431804780424414e-05,
      "loss": 1.4155,
      "step": 5160
    },
    {
      "epoch": 1.105,
      "grad_norm": 3.7724082469940186,
      "learning_rate": 4.430492206364474e-05,
      "loss": 1.545,
      "step": 5165
    },
    {
      "epoch": 1.10525,
      "grad_norm": 4.544516563415527,
      "learning_rate": 4.429178312831315e-05,
      "loss": 1.5986,
      "step": 5170
    },
    {
      "epoch": 1.1055,
      "grad_norm": 4.628349304199219,
      "learning_rate": 4.427863100722969e-05,
      "loss": 1.479,
      "step": 5175
    },
    {
      "epoch": 1.10575,
      "grad_norm": 5.117056369781494,
      "learning_rate": 4.426546570938373e-05,
      "loss": 1.5269,
      "step": 5180
    },
    {
      "epoch": 1.106,
      "grad_norm": 3.7559845447540283,
      "learning_rate": 4.4252287243773617e-05,
      "loss": 1.4126,
      "step": 5185
    },
    {
      "epoch": 1.10625,
      "grad_norm": 3.2551510334014893,
      "learning_rate": 4.4239095619406715e-05,
      "loss": 1.5138,
      "step": 5190
    },
    {
      "epoch": 1.1065,
      "grad_norm": 5.1682963371276855,
      "learning_rate": 4.422589084529937e-05,
      "loss": 1.5769,
      "step": 5195
    },
    {
      "epoch": 1.10675,
      "grad_norm": 3.8308281898498535,
      "learning_rate": 4.4212672930476915e-05,
      "loss": 1.5652,
      "step": 5200
    },
    {
      "epoch": 1.107,
      "grad_norm": 3.653142213821411,
      "learning_rate": 4.419944188397369e-05,
      "loss": 1.5399,
      "step": 5205
    },
    {
      "epoch": 1.10725,
      "grad_norm": 3.9739511013031006,
      "learning_rate": 4.4186197714832963e-05,
      "loss": 1.3386,
      "step": 5210
    },
    {
      "epoch": 1.1075,
      "grad_norm": 3.440263271331787,
      "learning_rate": 4.4172940432107015e-05,
      "loss": 1.542,
      "step": 5215
    },
    {
      "epoch": 1.10775,
      "grad_norm": 3.961378335952759,
      "learning_rate": 4.4159670044857084e-05,
      "loss": 1.3949,
      "step": 5220
    },
    {
      "epoch": 1.108,
      "grad_norm": 4.118093490600586,
      "learning_rate": 4.414638656215333e-05,
      "loss": 1.5263,
      "step": 5225
    },
    {
      "epoch": 1.10825,
      "grad_norm": 3.7023651599884033,
      "learning_rate": 4.413308999307491e-05,
      "loss": 1.5389,
      "step": 5230
    },
    {
      "epoch": 1.1085,
      "grad_norm": 3.445111036300659,
      "learning_rate": 4.411978034670988e-05,
      "loss": 1.39,
      "step": 5235
    },
    {
      "epoch": 1.10875,
      "grad_norm": 3.7818267345428467,
      "learning_rate": 4.410645763215528e-05,
      "loss": 1.4458,
      "step": 5240
    },
    {
      "epoch": 1.109,
      "grad_norm": 2.891883134841919,
      "learning_rate": 4.409312185851706e-05,
      "loss": 1.5536,
      "step": 5245
    },
    {
      "epoch": 1.10925,
      "grad_norm": 3.0145137310028076,
      "learning_rate": 4.4079773034910074e-05,
      "loss": 1.4159,
      "step": 5250
    },
    {
      "epoch": 1.1095,
      "grad_norm": 3.50247859954834,
      "learning_rate": 4.4066411170458136e-05,
      "loss": 1.3326,
      "step": 5255
    },
    {
      "epoch": 1.10975,
      "grad_norm": 2.4391684532165527,
      "learning_rate": 4.4053036274293946e-05,
      "loss": 1.2918,
      "step": 5260
    },
    {
      "epoch": 1.11,
      "grad_norm": 3.770916223526001,
      "learning_rate": 4.4039648355559124e-05,
      "loss": 1.4955,
      "step": 5265
    },
    {
      "epoch": 1.11025,
      "grad_norm": 4.941360950469971,
      "learning_rate": 4.402624742340419e-05,
      "loss": 1.3997,
      "step": 5270
    },
    {
      "epoch": 1.1105,
      "grad_norm": 3.607698678970337,
      "learning_rate": 4.401283348698854e-05,
      "loss": 1.4521,
      "step": 5275
    },
    {
      "epoch": 1.11075,
      "grad_norm": 4.231293678283691,
      "learning_rate": 4.399940655548048e-05,
      "loss": 1.7095,
      "step": 5280
    },
    {
      "epoch": 1.111,
      "grad_norm": 3.5768826007843018,
      "learning_rate": 4.3985966638057196e-05,
      "loss": 1.5058,
      "step": 5285
    },
    {
      "epoch": 1.11125,
      "grad_norm": 4.071849822998047,
      "learning_rate": 4.397251374390474e-05,
      "loss": 1.734,
      "step": 5290
    },
    {
      "epoch": 1.1115,
      "grad_norm": 4.492532730102539,
      "learning_rate": 4.395904788221805e-05,
      "loss": 1.5741,
      "step": 5295
    },
    {
      "epoch": 1.11175,
      "grad_norm": 3.9940741062164307,
      "learning_rate": 4.39455690622009e-05,
      "loss": 1.5064,
      "step": 5300
    },
    {
      "epoch": 1.112,
      "grad_norm": 3.322718620300293,
      "learning_rate": 4.393207729306594e-05,
      "loss": 1.5883,
      "step": 5305
    },
    {
      "epoch": 1.11225,
      "grad_norm": 3.2864322662353516,
      "learning_rate": 4.3918572584034665e-05,
      "loss": 1.5161,
      "step": 5310
    },
    {
      "epoch": 1.1125,
      "grad_norm": 2.780078172683716,
      "learning_rate": 4.390505494433743e-05,
      "loss": 1.7965,
      "step": 5315
    },
    {
      "epoch": 1.11275,
      "grad_norm": 3.1289072036743164,
      "learning_rate": 4.38915243832134e-05,
      "loss": 1.4989,
      "step": 5320
    },
    {
      "epoch": 1.113,
      "grad_norm": 3.6629605293273926,
      "learning_rate": 4.387798090991059e-05,
      "loss": 1.519,
      "step": 5325
    },
    {
      "epoch": 1.11325,
      "grad_norm": 3.0820846557617188,
      "learning_rate": 4.386442453368583e-05,
      "loss": 1.5135,
      "step": 5330
    },
    {
      "epoch": 1.1135,
      "grad_norm": 2.9819719791412354,
      "learning_rate": 4.3850855263804794e-05,
      "loss": 1.4761,
      "step": 5335
    },
    {
      "epoch": 1.11375,
      "grad_norm": 3.7501935958862305,
      "learning_rate": 4.3837273109541934e-05,
      "loss": 1.5176,
      "step": 5340
    },
    {
      "epoch": 1.114,
      "grad_norm": 3.1171722412109375,
      "learning_rate": 4.382367808018053e-05,
      "loss": 1.4256,
      "step": 5345
    },
    {
      "epoch": 1.11425,
      "grad_norm": 5.282496452331543,
      "learning_rate": 4.381007018501265e-05,
      "loss": 1.5286,
      "step": 5350
    },
    {
      "epoch": 1.1145,
      "grad_norm": 3.267444133758545,
      "learning_rate": 4.379644943333917e-05,
      "loss": 1.6964,
      "step": 5355
    },
    {
      "epoch": 1.11475,
      "grad_norm": 4.010468482971191,
      "learning_rate": 4.3782815834469746e-05,
      "loss": 1.5228,
      "step": 5360
    },
    {
      "epoch": 1.115,
      "grad_norm": 4.611567974090576,
      "learning_rate": 4.3769169397722805e-05,
      "loss": 1.6611,
      "step": 5365
    },
    {
      "epoch": 1.11525,
      "grad_norm": 4.7730207443237305,
      "learning_rate": 4.3755510132425563e-05,
      "loss": 1.6692,
      "step": 5370
    },
    {
      "epoch": 1.1155,
      "grad_norm": 3.540018320083618,
      "learning_rate": 4.3741838047914003e-05,
      "loss": 1.3555,
      "step": 5375
    },
    {
      "epoch": 1.11575,
      "grad_norm": 4.143580436706543,
      "learning_rate": 4.372815315353286e-05,
      "loss": 1.5114,
      "step": 5380
    },
    {
      "epoch": 1.116,
      "grad_norm": 3.19844126701355,
      "learning_rate": 4.3714455458635625e-05,
      "loss": 1.6362,
      "step": 5385
    },
    {
      "epoch": 1.11625,
      "grad_norm": 3.7426719665527344,
      "learning_rate": 4.370074497258456e-05,
      "loss": 1.5427,
      "step": 5390
    },
    {
      "epoch": 1.1165,
      "grad_norm": 3.739435911178589,
      "learning_rate": 4.3687021704750644e-05,
      "loss": 1.5816,
      "step": 5395
    },
    {
      "epoch": 1.11675,
      "grad_norm": 3.285506010055542,
      "learning_rate": 4.36732856645136e-05,
      "loss": 1.5192,
      "step": 5400
    },
    {
      "epoch": 1.117,
      "grad_norm": 3.4782371520996094,
      "learning_rate": 4.3659536861261884e-05,
      "loss": 1.533,
      "step": 5405
    },
    {
      "epoch": 1.11725,
      "grad_norm": 2.8415212631225586,
      "learning_rate": 4.364577530439267e-05,
      "loss": 1.5878,
      "step": 5410
    },
    {
      "epoch": 1.1175,
      "grad_norm": 3.1669158935546875,
      "learning_rate": 4.363200100331186e-05,
      "loss": 1.499,
      "step": 5415
    },
    {
      "epoch": 1.11775,
      "grad_norm": 3.252351999282837,
      "learning_rate": 4.361821396743404e-05,
      "loss": 1.4354,
      "step": 5420
    },
    {
      "epoch": 1.1179999999999999,
      "grad_norm": 5.09337854385376,
      "learning_rate": 4.360441420618254e-05,
      "loss": 1.4662,
      "step": 5425
    },
    {
      "epoch": 1.11825,
      "grad_norm": 3.739830493927002,
      "learning_rate": 4.3590601728989363e-05,
      "loss": 1.5206,
      "step": 5430
    },
    {
      "epoch": 1.1185,
      "grad_norm": 3.1852571964263916,
      "learning_rate": 4.357677654529521e-05,
      "loss": 1.3617,
      "step": 5435
    },
    {
      "epoch": 1.11875,
      "grad_norm": 3.517584800720215,
      "learning_rate": 4.356293866454944e-05,
      "loss": 1.5455,
      "step": 5440
    },
    {
      "epoch": 1.119,
      "grad_norm": 4.219455242156982,
      "learning_rate": 4.3549088096210136e-05,
      "loss": 1.5049,
      "step": 5445
    },
    {
      "epoch": 1.11925,
      "grad_norm": 2.943101167678833,
      "learning_rate": 4.353522484974403e-05,
      "loss": 1.4548,
      "step": 5450
    },
    {
      "epoch": 1.1195,
      "grad_norm": 4.866213798522949,
      "learning_rate": 4.352134893462651e-05,
      "loss": 1.4934,
      "step": 5455
    },
    {
      "epoch": 1.11975,
      "grad_norm": 3.3534274101257324,
      "learning_rate": 4.3507460360341646e-05,
      "loss": 1.5083,
      "step": 5460
    },
    {
      "epoch": 1.12,
      "grad_norm": 3.119929790496826,
      "learning_rate": 4.349355913638213e-05,
      "loss": 1.4896,
      "step": 5465
    },
    {
      "epoch": 1.12025,
      "grad_norm": 3.5968360900878906,
      "learning_rate": 4.3479645272249345e-05,
      "loss": 1.3497,
      "step": 5470
    },
    {
      "epoch": 1.1205,
      "grad_norm": 5.096608638763428,
      "learning_rate": 4.346571877745326e-05,
      "loss": 1.5273,
      "step": 5475
    },
    {
      "epoch": 1.12075,
      "grad_norm": 6.080724239349365,
      "learning_rate": 4.3451779661512503e-05,
      "loss": 1.5896,
      "step": 5480
    },
    {
      "epoch": 1.121,
      "grad_norm": 3.163904905319214,
      "learning_rate": 4.343782793395435e-05,
      "loss": 1.5021,
      "step": 5485
    },
    {
      "epoch": 1.12125,
      "grad_norm": 3.1921632289886475,
      "learning_rate": 4.3423863604314655e-05,
      "loss": 1.3876,
      "step": 5490
    },
    {
      "epoch": 1.1215,
      "grad_norm": 3.585191488265991,
      "learning_rate": 4.340988668213792e-05,
      "loss": 1.5594,
      "step": 5495
    },
    {
      "epoch": 1.12175,
      "grad_norm": 4.233048915863037,
      "learning_rate": 4.339589717697723e-05,
      "loss": 1.4445,
      "step": 5500
    },
    {
      "epoch": 1.12175,
      "eval_loss": 2.0158255100250244,
      "eval_runtime": 5.2428,
      "eval_samples_per_second": 195.315,
      "eval_steps_per_second": 24.414,
      "step": 5500
    },
    {
      "epoch": 1.1219999999999999,
      "grad_norm": 3.359717607498169,
      "learning_rate": 4.338189509839427e-05,
      "loss": 1.5659,
      "step": 5505
    },
    {
      "epoch": 1.12225,
      "grad_norm": 3.692406415939331,
      "learning_rate": 4.336788045595937e-05,
      "loss": 1.5273,
      "step": 5510
    },
    {
      "epoch": 1.1225,
      "grad_norm": 3.3265469074249268,
      "learning_rate": 4.335385325925135e-05,
      "loss": 1.5751,
      "step": 5515
    },
    {
      "epoch": 1.12275,
      "grad_norm": 4.033473491668701,
      "learning_rate": 4.3339813517857706e-05,
      "loss": 1.5317,
      "step": 5520
    },
    {
      "epoch": 1.123,
      "grad_norm": 3.909996747970581,
      "learning_rate": 4.332576124137445e-05,
      "loss": 1.4807,
      "step": 5525
    },
    {
      "epoch": 1.12325,
      "grad_norm": 4.175907611846924,
      "learning_rate": 4.331169643940619e-05,
      "loss": 1.383,
      "step": 5530
    },
    {
      "epoch": 1.1235,
      "grad_norm": 3.114474058151245,
      "learning_rate": 4.329761912156609e-05,
      "loss": 1.4793,
      "step": 5535
    },
    {
      "epoch": 1.12375,
      "grad_norm": 3.9472506046295166,
      "learning_rate": 4.328352929747584e-05,
      "loss": 1.4403,
      "step": 5540
    },
    {
      "epoch": 1.124,
      "grad_norm": 3.483933925628662,
      "learning_rate": 4.3269426976765734e-05,
      "loss": 1.4442,
      "step": 5545
    },
    {
      "epoch": 1.12425,
      "grad_norm": 3.7506189346313477,
      "learning_rate": 4.325531216907456e-05,
      "loss": 1.4628,
      "step": 5550
    },
    {
      "epoch": 1.1245,
      "grad_norm": 3.438319206237793,
      "learning_rate": 4.324118488404965e-05,
      "loss": 1.5042,
      "step": 5555
    },
    {
      "epoch": 1.12475,
      "grad_norm": 3.64228892326355,
      "learning_rate": 4.322704513134687e-05,
      "loss": 1.3842,
      "step": 5560
    },
    {
      "epoch": 1.125,
      "grad_norm": 4.745358467102051,
      "learning_rate": 4.321289292063062e-05,
      "loss": 1.6195,
      "step": 5565
    },
    {
      "epoch": 1.12525,
      "grad_norm": 3.2237014770507812,
      "learning_rate": 4.3198728261573796e-05,
      "loss": 1.4705,
      "step": 5570
    },
    {
      "epoch": 1.1255,
      "grad_norm": 3.871901273727417,
      "learning_rate": 4.318455116385781e-05,
      "loss": 1.574,
      "step": 5575
    },
    {
      "epoch": 1.12575,
      "grad_norm": 4.092315196990967,
      "learning_rate": 4.3170361637172575e-05,
      "loss": 1.3344,
      "step": 5580
    },
    {
      "epoch": 1.126,
      "grad_norm": 3.6278023719787598,
      "learning_rate": 4.315615969121649e-05,
      "loss": 1.457,
      "step": 5585
    },
    {
      "epoch": 1.12625,
      "grad_norm": 3.5942656993865967,
      "learning_rate": 4.314194533569647e-05,
      "loss": 1.4817,
      "step": 5590
    },
    {
      "epoch": 1.1265,
      "grad_norm": 6.7075090408325195,
      "learning_rate": 4.312771858032787e-05,
      "loss": 1.499,
      "step": 5595
    },
    {
      "epoch": 1.12675,
      "grad_norm": 4.761797904968262,
      "learning_rate": 4.311347943483456e-05,
      "loss": 1.4763,
      "step": 5600
    },
    {
      "epoch": 1.127,
      "grad_norm": 3.367255926132202,
      "learning_rate": 4.309922790894885e-05,
      "loss": 1.4781,
      "step": 5605
    },
    {
      "epoch": 1.12725,
      "grad_norm": 4.473830223083496,
      "learning_rate": 4.308496401241154e-05,
      "loss": 1.3841,
      "step": 5610
    },
    {
      "epoch": 1.1275,
      "grad_norm": 3.3326478004455566,
      "learning_rate": 4.307068775497185e-05,
      "loss": 1.439,
      "step": 5615
    },
    {
      "epoch": 1.12775,
      "grad_norm": 3.3282127380371094,
      "learning_rate": 4.305639914638748e-05,
      "loss": 1.5374,
      "step": 5620
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 3.7862021923065186,
      "learning_rate": 4.304209819642455e-05,
      "loss": 1.4587,
      "step": 5625
    },
    {
      "epoch": 1.12825,
      "grad_norm": 5.752108097076416,
      "learning_rate": 4.302778491485764e-05,
      "loss": 1.6198,
      "step": 5630
    },
    {
      "epoch": 1.1285,
      "grad_norm": 3.632767915725708,
      "learning_rate": 4.301345931146973e-05,
      "loss": 1.5089,
      "step": 5635
    },
    {
      "epoch": 1.12875,
      "grad_norm": 3.31538462638855,
      "learning_rate": 4.2999121396052234e-05,
      "loss": 1.7662,
      "step": 5640
    },
    {
      "epoch": 1.129,
      "grad_norm": 3.0356359481811523,
      "learning_rate": 4.298477117840498e-05,
      "loss": 1.3772,
      "step": 5645
    },
    {
      "epoch": 1.12925,
      "grad_norm": 3.895453691482544,
      "learning_rate": 4.2970408668336215e-05,
      "loss": 1.4625,
      "step": 5650
    },
    {
      "epoch": 1.1295,
      "grad_norm": 2.756258249282837,
      "learning_rate": 4.295603387566258e-05,
      "loss": 1.4543,
      "step": 5655
    },
    {
      "epoch": 1.12975,
      "grad_norm": 3.704470634460449,
      "learning_rate": 4.29416468102091e-05,
      "loss": 1.3522,
      "step": 5660
    },
    {
      "epoch": 1.13,
      "grad_norm": 3.9065771102905273,
      "learning_rate": 4.292724748180921e-05,
      "loss": 1.4693,
      "step": 5665
    },
    {
      "epoch": 1.13025,
      "grad_norm": 3.852479934692383,
      "learning_rate": 4.291283590030471e-05,
      "loss": 1.4723,
      "step": 5670
    },
    {
      "epoch": 1.1305,
      "grad_norm": 3.942232131958008,
      "learning_rate": 4.289841207554578e-05,
      "loss": 1.5226,
      "step": 5675
    },
    {
      "epoch": 1.13075,
      "grad_norm": 3.526204824447632,
      "learning_rate": 4.288397601739097e-05,
      "loss": 1.4816,
      "step": 5680
    },
    {
      "epoch": 1.131,
      "grad_norm": 5.5281548500061035,
      "learning_rate": 4.2869527735707184e-05,
      "loss": 1.3511,
      "step": 5685
    },
    {
      "epoch": 1.13125,
      "grad_norm": 4.10409688949585,
      "learning_rate": 4.2855067240369694e-05,
      "loss": 1.419,
      "step": 5690
    },
    {
      "epoch": 1.1315,
      "grad_norm": 3.15427565574646,
      "learning_rate": 4.2840594541262105e-05,
      "loss": 1.3794,
      "step": 5695
    },
    {
      "epoch": 1.13175,
      "grad_norm": 5.227628707885742,
      "learning_rate": 4.282610964827637e-05,
      "loss": 1.4264,
      "step": 5700
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 3.6007566452026367,
      "learning_rate": 4.281161257131279e-05,
      "loss": 1.3913,
      "step": 5705
    },
    {
      "epoch": 1.13225,
      "grad_norm": 4.663416385650635,
      "learning_rate": 4.279710332027996e-05,
      "loss": 1.3647,
      "step": 5710
    },
    {
      "epoch": 1.1325,
      "grad_norm": 3.090851306915283,
      "learning_rate": 4.278258190509484e-05,
      "loss": 1.2775,
      "step": 5715
    },
    {
      "epoch": 1.13275,
      "grad_norm": 4.840991020202637,
      "learning_rate": 4.276804833568266e-05,
      "loss": 1.327,
      "step": 5720
    },
    {
      "epoch": 1.133,
      "grad_norm": 2.71724271774292,
      "learning_rate": 4.2753502621976986e-05,
      "loss": 1.0822,
      "step": 5725
    },
    {
      "epoch": 1.13325,
      "grad_norm": 8.587010383605957,
      "learning_rate": 4.273894477391968e-05,
      "loss": 1.5469,
      "step": 5730
    },
    {
      "epoch": 1.1335,
      "grad_norm": 2.948481798171997,
      "learning_rate": 4.272437480146088e-05,
      "loss": 1.1965,
      "step": 5735
    },
    {
      "epoch": 1.13375,
      "grad_norm": 2.681004524230957,
      "learning_rate": 4.270979271455904e-05,
      "loss": 1.0856,
      "step": 5740
    },
    {
      "epoch": 1.134,
      "grad_norm": 2.9778308868408203,
      "learning_rate": 4.2695198523180876e-05,
      "loss": 1.346,
      "step": 5745
    },
    {
      "epoch": 1.13425,
      "grad_norm": 2.9171950817108154,
      "learning_rate": 4.268059223730137e-05,
      "loss": 1.3559,
      "step": 5750
    },
    {
      "epoch": 1.1345,
      "grad_norm": 3.0229547023773193,
      "learning_rate": 4.266597386690379e-05,
      "loss": 1.3443,
      "step": 5755
    },
    {
      "epoch": 1.13475,
      "grad_norm": 4.613873481750488,
      "learning_rate": 4.265134342197966e-05,
      "loss": 1.4288,
      "step": 5760
    },
    {
      "epoch": 1.135,
      "grad_norm": 3.5987699031829834,
      "learning_rate": 4.263670091252873e-05,
      "loss": 1.5854,
      "step": 5765
    },
    {
      "epoch": 1.13525,
      "grad_norm": 5.628066062927246,
      "learning_rate": 4.262204634855904e-05,
      "loss": 1.5586,
      "step": 5770
    },
    {
      "epoch": 1.1355,
      "grad_norm": 4.293406963348389,
      "learning_rate": 4.260737974008683e-05,
      "loss": 1.4962,
      "step": 5775
    },
    {
      "epoch": 1.13575,
      "grad_norm": 5.088069438934326,
      "learning_rate": 4.25927010971366e-05,
      "loss": 1.58,
      "step": 5780
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 3.6495401859283447,
      "learning_rate": 4.257801042974105e-05,
      "loss": 1.3114,
      "step": 5785
    },
    {
      "epoch": 1.13625,
      "grad_norm": 4.543731689453125,
      "learning_rate": 4.2563307747941136e-05,
      "loss": 1.3922,
      "step": 5790
    },
    {
      "epoch": 1.1365,
      "grad_norm": 4.061283111572266,
      "learning_rate": 4.254859306178599e-05,
      "loss": 1.5697,
      "step": 5795
    },
    {
      "epoch": 1.13675,
      "grad_norm": 4.08359432220459,
      "learning_rate": 4.253386638133294e-05,
      "loss": 1.5166,
      "step": 5800
    },
    {
      "epoch": 1.137,
      "grad_norm": 3.835284471511841,
      "learning_rate": 4.251912771664758e-05,
      "loss": 1.5872,
      "step": 5805
    },
    {
      "epoch": 1.13725,
      "grad_norm": 3.6963117122650146,
      "learning_rate": 4.2504377077803604e-05,
      "loss": 1.5707,
      "step": 5810
    },
    {
      "epoch": 1.1375,
      "grad_norm": 4.881359577178955,
      "learning_rate": 4.248961447488295e-05,
      "loss": 1.2916,
      "step": 5815
    },
    {
      "epoch": 1.13775,
      "grad_norm": 5.504133224487305,
      "learning_rate": 4.247483991797572e-05,
      "loss": 1.4788,
      "step": 5820
    },
    {
      "epoch": 1.138,
      "grad_norm": 2.7841110229492188,
      "learning_rate": 4.246005341718019e-05,
      "loss": 1.5338,
      "step": 5825
    },
    {
      "epoch": 1.13825,
      "grad_norm": 4.190717697143555,
      "learning_rate": 4.244525498260279e-05,
      "loss": 1.4761,
      "step": 5830
    },
    {
      "epoch": 1.1385,
      "grad_norm": 4.967435836791992,
      "learning_rate": 4.24304446243581e-05,
      "loss": 1.6318,
      "step": 5835
    },
    {
      "epoch": 1.13875,
      "grad_norm": 3.9086906909942627,
      "learning_rate": 4.241562235256887e-05,
      "loss": 1.5815,
      "step": 5840
    },
    {
      "epoch": 1.139,
      "grad_norm": 3.451383113861084,
      "learning_rate": 4.2400788177365985e-05,
      "loss": 1.4771,
      "step": 5845
    },
    {
      "epoch": 1.13925,
      "grad_norm": 3.217575788497925,
      "learning_rate": 4.238594210888846e-05,
      "loss": 1.4622,
      "step": 5850
    },
    {
      "epoch": 1.1395,
      "grad_norm": 3.8814761638641357,
      "learning_rate": 4.237108415728345e-05,
      "loss": 1.3838,
      "step": 5855
    },
    {
      "epoch": 1.13975,
      "grad_norm": 3.0072360038757324,
      "learning_rate": 4.23562143327062e-05,
      "loss": 1.4477,
      "step": 5860
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 5.424773693084717,
      "learning_rate": 4.234133264532012e-05,
      "loss": 1.3822,
      "step": 5865
    },
    {
      "epoch": 1.14025,
      "grad_norm": 4.541780948638916,
      "learning_rate": 4.232643910529671e-05,
      "loss": 1.5796,
      "step": 5870
    },
    {
      "epoch": 1.1405,
      "grad_norm": 3.3818962574005127,
      "learning_rate": 4.231153372281554e-05,
      "loss": 1.3289,
      "step": 5875
    },
    {
      "epoch": 1.14075,
      "grad_norm": 4.42661714553833,
      "learning_rate": 4.22966165080643e-05,
      "loss": 1.5262,
      "step": 5880
    },
    {
      "epoch": 1.141,
      "grad_norm": 3.9261856079101562,
      "learning_rate": 4.228168747123879e-05,
      "loss": 1.3807,
      "step": 5885
    },
    {
      "epoch": 1.1412499999999999,
      "grad_norm": 3.833519458770752,
      "learning_rate": 4.2266746622542835e-05,
      "loss": 1.5315,
      "step": 5890
    },
    {
      "epoch": 1.1415,
      "grad_norm": 3.3553507328033447,
      "learning_rate": 4.225179397218839e-05,
      "loss": 1.2891,
      "step": 5895
    },
    {
      "epoch": 1.14175,
      "grad_norm": 3.380324125289917,
      "learning_rate": 4.223682953039544e-05,
      "loss": 1.2871,
      "step": 5900
    },
    {
      "epoch": 1.142,
      "grad_norm": 4.312615871429443,
      "learning_rate": 4.222185330739203e-05,
      "loss": 1.5276,
      "step": 5905
    },
    {
      "epoch": 1.14225,
      "grad_norm": 5.766417503356934,
      "learning_rate": 4.220686531341428e-05,
      "loss": 1.5215,
      "step": 5910
    },
    {
      "epoch": 1.1425,
      "grad_norm": 3.1938209533691406,
      "learning_rate": 4.219186555870634e-05,
      "loss": 1.3075,
      "step": 5915
    },
    {
      "epoch": 1.14275,
      "grad_norm": 2.838794708251953,
      "learning_rate": 4.21768540535204e-05,
      "loss": 1.3505,
      "step": 5920
    },
    {
      "epoch": 1.143,
      "grad_norm": 3.2588679790496826,
      "learning_rate": 4.2161830808116685e-05,
      "loss": 1.1845,
      "step": 5925
    },
    {
      "epoch": 1.14325,
      "grad_norm": 4.833232879638672,
      "learning_rate": 4.2146795832763445e-05,
      "loss": 1.2293,
      "step": 5930
    },
    {
      "epoch": 1.1435,
      "grad_norm": 3.5987672805786133,
      "learning_rate": 4.2131749137736925e-05,
      "loss": 1.4461,
      "step": 5935
    },
    {
      "epoch": 1.14375,
      "grad_norm": 5.170583724975586,
      "learning_rate": 4.211669073332142e-05,
      "loss": 1.4092,
      "step": 5940
    },
    {
      "epoch": 1.144,
      "grad_norm": 3.3460381031036377,
      "learning_rate": 4.2101620629809205e-05,
      "loss": 1.3047,
      "step": 5945
    },
    {
      "epoch": 1.14425,
      "grad_norm": 2.857536792755127,
      "learning_rate": 4.2086538837500535e-05,
      "loss": 1.4231,
      "step": 5950
    },
    {
      "epoch": 1.1445,
      "grad_norm": 3.6724634170532227,
      "learning_rate": 4.2071445366703685e-05,
      "loss": 1.1843,
      "step": 5955
    },
    {
      "epoch": 1.14475,
      "grad_norm": 3.605231523513794,
      "learning_rate": 4.205634022773491e-05,
      "loss": 1.1394,
      "step": 5960
    },
    {
      "epoch": 1.145,
      "grad_norm": 3.7887909412384033,
      "learning_rate": 4.204122343091842e-05,
      "loss": 1.2931,
      "step": 5965
    },
    {
      "epoch": 1.1452499999999999,
      "grad_norm": 2.6056010723114014,
      "learning_rate": 4.202609498658641e-05,
      "loss": 1.0738,
      "step": 5970
    },
    {
      "epoch": 1.1455,
      "grad_norm": 4.415247440338135,
      "learning_rate": 4.201095490507901e-05,
      "loss": 1.3353,
      "step": 5975
    },
    {
      "epoch": 1.14575,
      "grad_norm": 2.593935489654541,
      "learning_rate": 4.1995803196744344e-05,
      "loss": 1.4106,
      "step": 5980
    },
    {
      "epoch": 1.146,
      "grad_norm": 3.698383092880249,
      "learning_rate": 4.198063987193845e-05,
      "loss": 1.2003,
      "step": 5985
    },
    {
      "epoch": 1.14625,
      "grad_norm": 3.7068355083465576,
      "learning_rate": 4.1965464941025325e-05,
      "loss": 1.3649,
      "step": 5990
    },
    {
      "epoch": 1.1465,
      "grad_norm": 5.245954513549805,
      "learning_rate": 4.1950278414376883e-05,
      "loss": 2.2324,
      "step": 5995
    },
    {
      "epoch": 1.14675,
      "grad_norm": 4.499756336212158,
      "learning_rate": 4.1935080302372984e-05,
      "loss": 2.5262,
      "step": 6000
    },
    {
      "epoch": 1.14675,
      "eval_loss": 1.9444515705108643,
      "eval_runtime": 5.3274,
      "eval_samples_per_second": 192.213,
      "eval_steps_per_second": 24.027,
      "step": 6000
    },
    {
      "epoch": 1.147,
      "grad_norm": 4.763808250427246,
      "learning_rate": 4.191987061540138e-05,
      "loss": 2.3463,
      "step": 6005
    },
    {
      "epoch": 1.14725,
      "grad_norm": 6.382129192352295,
      "learning_rate": 4.1904649363857754e-05,
      "loss": 2.2306,
      "step": 6010
    },
    {
      "epoch": 1.1475,
      "grad_norm": 4.312391757965088,
      "learning_rate": 4.18894165581457e-05,
      "loss": 2.1524,
      "step": 6015
    },
    {
      "epoch": 1.14775,
      "grad_norm": 4.950083255767822,
      "learning_rate": 4.187417220867667e-05,
      "loss": 2.1597,
      "step": 6020
    },
    {
      "epoch": 1.148,
      "grad_norm": 4.1195807456970215,
      "learning_rate": 4.1858916325870065e-05,
      "loss": 2.1018,
      "step": 6025
    },
    {
      "epoch": 1.14825,
      "grad_norm": 5.000475883483887,
      "learning_rate": 4.184364892015311e-05,
      "loss": 2.0759,
      "step": 6030
    },
    {
      "epoch": 1.1485,
      "grad_norm": 3.643063545227051,
      "learning_rate": 4.182837000196095e-05,
      "loss": 2.159,
      "step": 6035
    },
    {
      "epoch": 1.14875,
      "grad_norm": 7.528382301330566,
      "learning_rate": 4.1813079581736574e-05,
      "loss": 2.2547,
      "step": 6040
    },
    {
      "epoch": 1.149,
      "grad_norm": 4.169739246368408,
      "learning_rate": 4.179777766993084e-05,
      "loss": 2.1893,
      "step": 6045
    },
    {
      "epoch": 1.1492499999999999,
      "grad_norm": 3.36360502243042,
      "learning_rate": 4.1782464277002464e-05,
      "loss": 2.0672,
      "step": 6050
    },
    {
      "epoch": 1.1495,
      "grad_norm": 4.171354293823242,
      "learning_rate": 4.1767139413418e-05,
      "loss": 2.1309,
      "step": 6055
    },
    {
      "epoch": 1.14975,
      "grad_norm": 3.8275675773620605,
      "learning_rate": 4.1751803089651855e-05,
      "loss": 2.0467,
      "step": 6060
    },
    {
      "epoch": 1.15,
      "grad_norm": 4.369592666625977,
      "learning_rate": 4.1736455316186265e-05,
      "loss": 2.0545,
      "step": 6065
    },
    {
      "epoch": 1.15025,
      "grad_norm": 3.7324016094207764,
      "learning_rate": 4.1721096103511273e-05,
      "loss": 2.0387,
      "step": 6070
    },
    {
      "epoch": 1.1505,
      "grad_norm": 5.548137187957764,
      "learning_rate": 4.170572546212477e-05,
      "loss": 2.2055,
      "step": 6075
    },
    {
      "epoch": 1.15075,
      "grad_norm": 5.7310285568237305,
      "learning_rate": 4.1690343402532426e-05,
      "loss": 2.1596,
      "step": 6080
    },
    {
      "epoch": 1.151,
      "grad_norm": 4.04592752456665,
      "learning_rate": 4.1674949935247767e-05,
      "loss": 2.1038,
      "step": 6085
    },
    {
      "epoch": 1.15125,
      "grad_norm": 3.7305145263671875,
      "learning_rate": 4.165954507079205e-05,
      "loss": 2.1692,
      "step": 6090
    },
    {
      "epoch": 1.1515,
      "grad_norm": 3.9501099586486816,
      "learning_rate": 4.164412881969437e-05,
      "loss": 2.2015,
      "step": 6095
    },
    {
      "epoch": 1.15175,
      "grad_norm": 4.76682186126709,
      "learning_rate": 4.1628701192491584e-05,
      "loss": 2.1212,
      "step": 6100
    },
    {
      "epoch": 1.152,
      "grad_norm": 8.61727237701416,
      "learning_rate": 4.161326219972833e-05,
      "loss": 2.8035,
      "step": 6105
    },
    {
      "epoch": 1.15225,
      "grad_norm": 7.279456615447998,
      "learning_rate": 4.159781185195703e-05,
      "loss": 2.6297,
      "step": 6110
    },
    {
      "epoch": 1.1525,
      "grad_norm": 6.982289791107178,
      "learning_rate": 4.1582350159737824e-05,
      "loss": 2.4953,
      "step": 6115
    },
    {
      "epoch": 1.15275,
      "grad_norm": 6.662633419036865,
      "learning_rate": 4.156687713363865e-05,
      "loss": 2.1348,
      "step": 6120
    },
    {
      "epoch": 1.153,
      "grad_norm": 5.053005218505859,
      "learning_rate": 4.155139278423518e-05,
      "loss": 2.3381,
      "step": 6125
    },
    {
      "epoch": 1.1532499999999999,
      "grad_norm": 3.9865410327911377,
      "learning_rate": 4.153589712211079e-05,
      "loss": 1.8146,
      "step": 6130
    },
    {
      "epoch": 2.00025,
      "grad_norm": 4.721142768859863,
      "learning_rate": 4.152039015785667e-05,
      "loss": 1.462,
      "step": 6135
    },
    {
      "epoch": 2.0005,
      "grad_norm": 4.926209926605225,
      "learning_rate": 4.1504871902071634e-05,
      "loss": 1.3997,
      "step": 6140
    },
    {
      "epoch": 2.00075,
      "grad_norm": 4.123579025268555,
      "learning_rate": 4.148934236536229e-05,
      "loss": 1.4167,
      "step": 6145
    },
    {
      "epoch": 2.001,
      "grad_norm": 5.926658630371094,
      "learning_rate": 4.147380155834293e-05,
      "loss": 1.3141,
      "step": 6150
    },
    {
      "epoch": 2.00125,
      "grad_norm": 4.849578380584717,
      "learning_rate": 4.145824949163554e-05,
      "loss": 1.5065,
      "step": 6155
    },
    {
      "epoch": 2.0015,
      "grad_norm": 5.0482258796691895,
      "learning_rate": 4.144268617586981e-05,
      "loss": 1.5451,
      "step": 6160
    },
    {
      "epoch": 2.00175,
      "grad_norm": 3.8800480365753174,
      "learning_rate": 4.1427111621683123e-05,
      "loss": 1.1979,
      "step": 6165
    },
    {
      "epoch": 2.002,
      "grad_norm": 4.228359699249268,
      "learning_rate": 4.141152583972054e-05,
      "loss": 1.3587,
      "step": 6170
    },
    {
      "epoch": 2.00225,
      "grad_norm": 4.249489784240723,
      "learning_rate": 4.139592884063481e-05,
      "loss": 1.3635,
      "step": 6175
    },
    {
      "epoch": 2.0025,
      "grad_norm": 4.9958295822143555,
      "learning_rate": 4.138032063508631e-05,
      "loss": 1.5969,
      "step": 6180
    },
    {
      "epoch": 2.00275,
      "grad_norm": 7.447131633758545,
      "learning_rate": 4.136470123374312e-05,
      "loss": 1.7289,
      "step": 6185
    },
    {
      "epoch": 2.003,
      "grad_norm": 5.312364101409912,
      "learning_rate": 4.1349070647280947e-05,
      "loss": 1.6497,
      "step": 6190
    },
    {
      "epoch": 2.00325,
      "grad_norm": 4.469681262969971,
      "learning_rate": 4.133342888638314e-05,
      "loss": 1.6947,
      "step": 6195
    },
    {
      "epoch": 2.0035,
      "grad_norm": 4.286854267120361,
      "learning_rate": 4.1317775961740715e-05,
      "loss": 1.5112,
      "step": 6200
    },
    {
      "epoch": 2.00375,
      "grad_norm": 3.8573760986328125,
      "learning_rate": 4.130211188405229e-05,
      "loss": 1.3975,
      "step": 6205
    },
    {
      "epoch": 2.004,
      "grad_norm": 9.258516311645508,
      "learning_rate": 4.1286436664024096e-05,
      "loss": 1.6208,
      "step": 6210
    },
    {
      "epoch": 2.00425,
      "grad_norm": 8.044137954711914,
      "learning_rate": 4.1270750312370024e-05,
      "loss": 1.7614,
      "step": 6215
    },
    {
      "epoch": 2.0045,
      "grad_norm": 5.924137115478516,
      "learning_rate": 4.125505283981154e-05,
      "loss": 1.6769,
      "step": 6220
    },
    {
      "epoch": 2.00475,
      "grad_norm": 6.3989763259887695,
      "learning_rate": 4.123934425707771e-05,
      "loss": 1.7464,
      "step": 6225
    },
    {
      "epoch": 2.005,
      "grad_norm": 5.139994144439697,
      "learning_rate": 4.1223624574905204e-05,
      "loss": 1.7822,
      "step": 6230
    },
    {
      "epoch": 2.00525,
      "grad_norm": 4.489633083343506,
      "learning_rate": 4.120789380403829e-05,
      "loss": 1.685,
      "step": 6235
    },
    {
      "epoch": 2.0055,
      "grad_norm": 5.903286457061768,
      "learning_rate": 4.119215195522878e-05,
      "loss": 1.9184,
      "step": 6240
    },
    {
      "epoch": 2.00575,
      "grad_norm": 4.64190673828125,
      "learning_rate": 4.1176399039236116e-05,
      "loss": 1.7198,
      "step": 6245
    },
    {
      "epoch": 2.006,
      "grad_norm": 5.3406219482421875,
      "learning_rate": 4.116063506682722e-05,
      "loss": 1.8643,
      "step": 6250
    },
    {
      "epoch": 2.00625,
      "grad_norm": 5.260170936584473,
      "learning_rate": 4.1144860048776653e-05,
      "loss": 1.6821,
      "step": 6255
    },
    {
      "epoch": 2.0065,
      "grad_norm": 5.67748498916626,
      "learning_rate": 4.1129073995866474e-05,
      "loss": 1.8426,
      "step": 6260
    },
    {
      "epoch": 2.00675,
      "grad_norm": 4.811794281005859,
      "learning_rate": 4.111327691888631e-05,
      "loss": 1.5695,
      "step": 6265
    },
    {
      "epoch": 2.007,
      "grad_norm": 5.701094627380371,
      "learning_rate": 4.1097468828633315e-05,
      "loss": 1.5867,
      "step": 6270
    },
    {
      "epoch": 2.00725,
      "grad_norm": 4.672128200531006,
      "learning_rate": 4.1081649735912156e-05,
      "loss": 1.6972,
      "step": 6275
    },
    {
      "epoch": 2.0075,
      "grad_norm": 5.122629165649414,
      "learning_rate": 4.106581965153505e-05,
      "loss": 1.8012,
      "step": 6280
    },
    {
      "epoch": 2.00775,
      "grad_norm": 6.325993537902832,
      "learning_rate": 4.1049978586321696e-05,
      "loss": 1.6312,
      "step": 6285
    },
    {
      "epoch": 2.008,
      "grad_norm": 4.742122173309326,
      "learning_rate": 4.1034126551099326e-05,
      "loss": 1.6302,
      "step": 6290
    },
    {
      "epoch": 2.00825,
      "grad_norm": 5.602465629577637,
      "learning_rate": 4.101826355670265e-05,
      "loss": 1.7307,
      "step": 6295
    },
    {
      "epoch": 2.0085,
      "grad_norm": 7.433533191680908,
      "learning_rate": 4.100238961397388e-05,
      "loss": 1.603,
      "step": 6300
    },
    {
      "epoch": 2.00875,
      "grad_norm": 5.0557122230529785,
      "learning_rate": 4.0986504733762696e-05,
      "loss": 1.6124,
      "step": 6305
    },
    {
      "epoch": 2.009,
      "grad_norm": 5.739558219909668,
      "learning_rate": 4.097060892692628e-05,
      "loss": 1.7495,
      "step": 6310
    },
    {
      "epoch": 2.00925,
      "grad_norm": 5.432423114776611,
      "learning_rate": 4.095470220432925e-05,
      "loss": 1.5979,
      "step": 6315
    },
    {
      "epoch": 2.0095,
      "grad_norm": 6.2294087409973145,
      "learning_rate": 4.0938784576843706e-05,
      "loss": 1.5312,
      "step": 6320
    },
    {
      "epoch": 2.00975,
      "grad_norm": 7.982913970947266,
      "learning_rate": 4.09228560553492e-05,
      "loss": 1.7704,
      "step": 6325
    },
    {
      "epoch": 2.01,
      "grad_norm": 4.819182872772217,
      "learning_rate": 4.090691665073272e-05,
      "loss": 1.5996,
      "step": 6330
    },
    {
      "epoch": 2.01025,
      "grad_norm": 5.703671932220459,
      "learning_rate": 4.0890966373888704e-05,
      "loss": 1.6584,
      "step": 6335
    },
    {
      "epoch": 2.0105,
      "grad_norm": 4.557087421417236,
      "learning_rate": 4.087500523571902e-05,
      "loss": 1.7391,
      "step": 6340
    },
    {
      "epoch": 2.01075,
      "grad_norm": 6.752840518951416,
      "learning_rate": 4.085903324713294e-05,
      "loss": 1.5821,
      "step": 6345
    },
    {
      "epoch": 2.011,
      "grad_norm": 5.946703910827637,
      "learning_rate": 4.084305041904719e-05,
      "loss": 1.5509,
      "step": 6350
    },
    {
      "epoch": 2.01125,
      "grad_norm": 4.631699085235596,
      "learning_rate": 4.0827056762385855e-05,
      "loss": 1.6386,
      "step": 6355
    },
    {
      "epoch": 2.0115,
      "grad_norm": 5.135982036590576,
      "learning_rate": 4.0811052288080474e-05,
      "loss": 1.5677,
      "step": 6360
    },
    {
      "epoch": 2.01175,
      "grad_norm": 5.749668121337891,
      "learning_rate": 4.079503700706994e-05,
      "loss": 1.6702,
      "step": 6365
    },
    {
      "epoch": 2.012,
      "grad_norm": 4.89791202545166,
      "learning_rate": 4.0779010930300546e-05,
      "loss": 1.5725,
      "step": 6370
    },
    {
      "epoch": 2.01225,
      "grad_norm": 4.774654865264893,
      "learning_rate": 4.076297406872598e-05,
      "loss": 1.8222,
      "step": 6375
    },
    {
      "epoch": 2.0125,
      "grad_norm": 5.335406303405762,
      "learning_rate": 4.0746926433307265e-05,
      "loss": 1.5379,
      "step": 6380
    },
    {
      "epoch": 2.01275,
      "grad_norm": 4.899043083190918,
      "learning_rate": 4.073086803501282e-05,
      "loss": 1.3942,
      "step": 6385
    },
    {
      "epoch": 2.013,
      "grad_norm": 5.021248817443848,
      "learning_rate": 4.07147988848184e-05,
      "loss": 1.5098,
      "step": 6390
    },
    {
      "epoch": 2.01325,
      "grad_norm": 5.350771427154541,
      "learning_rate": 4.069871899370713e-05,
      "loss": 1.5608,
      "step": 6395
    },
    {
      "epoch": 2.0135,
      "grad_norm": 6.010500907897949,
      "learning_rate": 4.068262837266945e-05,
      "loss": 1.4009,
      "step": 6400
    },
    {
      "epoch": 2.01375,
      "grad_norm": 5.949312210083008,
      "learning_rate": 4.0666527032703164e-05,
      "loss": 1.43,
      "step": 6405
    },
    {
      "epoch": 2.014,
      "grad_norm": 6.263274192810059,
      "learning_rate": 4.065041498481338e-05,
      "loss": 1.6705,
      "step": 6410
    },
    {
      "epoch": 2.01425,
      "grad_norm": 4.7441582679748535,
      "learning_rate": 4.063429224001252e-05,
      "loss": 1.4928,
      "step": 6415
    },
    {
      "epoch": 2.0145,
      "grad_norm": 4.075540542602539,
      "learning_rate": 4.061815880932034e-05,
      "loss": 1.2166,
      "step": 6420
    },
    {
      "epoch": 2.01475,
      "grad_norm": 6.144962787628174,
      "learning_rate": 4.060201470376389e-05,
      "loss": 1.3633,
      "step": 6425
    },
    {
      "epoch": 2.015,
      "grad_norm": 4.055844783782959,
      "learning_rate": 4.05858599343775e-05,
      "loss": 1.2306,
      "step": 6430
    },
    {
      "epoch": 2.01525,
      "grad_norm": 5.681750774383545,
      "learning_rate": 4.056969451220282e-05,
      "loss": 0.8659,
      "step": 6435
    },
    {
      "epoch": 2.0155,
      "grad_norm": 6.097544193267822,
      "learning_rate": 4.055351844828874e-05,
      "loss": 0.8774,
      "step": 6440
    },
    {
      "epoch": 2.01575,
      "grad_norm": 3.3078269958496094,
      "learning_rate": 4.0537331753691475e-05,
      "loss": 0.7263,
      "step": 6445
    },
    {
      "epoch": 2.016,
      "grad_norm": 3.2710654735565186,
      "learning_rate": 4.052113443947446e-05,
      "loss": 0.6182,
      "step": 6450
    },
    {
      "epoch": 2.01625,
      "grad_norm": 6.5024261474609375,
      "learning_rate": 4.050492651670841e-05,
      "loss": 0.7266,
      "step": 6455
    },
    {
      "epoch": 2.0165,
      "grad_norm": 10.859475135803223,
      "learning_rate": 4.048870799647129e-05,
      "loss": 0.9958,
      "step": 6460
    },
    {
      "epoch": 2.01675,
      "grad_norm": 7.389084339141846,
      "learning_rate": 4.04724788898483e-05,
      "loss": 1.8223,
      "step": 6465
    },
    {
      "epoch": 2.017,
      "grad_norm": 6.948849678039551,
      "learning_rate": 4.045623920793189e-05,
      "loss": 1.9194,
      "step": 6470
    },
    {
      "epoch": 2.01725,
      "grad_norm": 5.25331974029541,
      "learning_rate": 4.043998896182172e-05,
      "loss": 1.7965,
      "step": 6475
    },
    {
      "epoch": 2.0175,
      "grad_norm": 6.3399128913879395,
      "learning_rate": 4.0423728162624695e-05,
      "loss": 1.7441,
      "step": 6480
    },
    {
      "epoch": 2.01775,
      "grad_norm": 5.469205856323242,
      "learning_rate": 4.0407456821454906e-05,
      "loss": 1.5321,
      "step": 6485
    },
    {
      "epoch": 2.018,
      "grad_norm": 4.91494607925415,
      "learning_rate": 4.039117494943365e-05,
      "loss": 1.5857,
      "step": 6490
    },
    {
      "epoch": 2.01825,
      "grad_norm": 6.592794418334961,
      "learning_rate": 4.037488255768945e-05,
      "loss": 1.7299,
      "step": 6495
    },
    {
      "epoch": 2.0185,
      "grad_norm": 5.0095720291137695,
      "learning_rate": 4.035857965735801e-05,
      "loss": 1.8016,
      "step": 6500
    },
    {
      "epoch": 2.0185,
      "eval_loss": 1.8357408046722412,
      "eval_runtime": 5.2963,
      "eval_samples_per_second": 193.343,
      "eval_steps_per_second": 24.168,
      "step": 6500
    },
    {
      "epoch": 2.01875,
      "grad_norm": 5.648088455200195,
      "learning_rate": 4.034226625958219e-05,
      "loss": 1.5855,
      "step": 6505
    },
    {
      "epoch": 2.019,
      "grad_norm": 5.145895957946777,
      "learning_rate": 4.032594237551205e-05,
      "loss": 1.7187,
      "step": 6510
    },
    {
      "epoch": 2.01925,
      "grad_norm": 5.174917221069336,
      "learning_rate": 4.0309608016304815e-05,
      "loss": 1.6392,
      "step": 6515
    },
    {
      "epoch": 2.0195,
      "grad_norm": 5.41826868057251,
      "learning_rate": 4.0293263193124855e-05,
      "loss": 1.6267,
      "step": 6520
    },
    {
      "epoch": 2.01975,
      "grad_norm": 5.21962833404541,
      "learning_rate": 4.0276907917143714e-05,
      "loss": 1.4021,
      "step": 6525
    },
    {
      "epoch": 2.02,
      "grad_norm": 5.6611409187316895,
      "learning_rate": 4.0260542199540064e-05,
      "loss": 1.5953,
      "step": 6530
    },
    {
      "epoch": 2.02025,
      "grad_norm": 7.675441741943359,
      "learning_rate": 4.024416605149972e-05,
      "loss": 1.8509,
      "step": 6535
    },
    {
      "epoch": 2.0205,
      "grad_norm": 4.814138889312744,
      "learning_rate": 4.0227779484215624e-05,
      "loss": 1.743,
      "step": 6540
    },
    {
      "epoch": 2.02075,
      "grad_norm": 6.571059703826904,
      "learning_rate": 4.021138250888784e-05,
      "loss": 1.5249,
      "step": 6545
    },
    {
      "epoch": 2.021,
      "grad_norm": 7.036901950836182,
      "learning_rate": 4.019497513672355e-05,
      "loss": 1.7284,
      "step": 6550
    },
    {
      "epoch": 2.02125,
      "grad_norm": 4.666823863983154,
      "learning_rate": 4.017855737893702e-05,
      "loss": 1.6196,
      "step": 6555
    },
    {
      "epoch": 2.0215,
      "grad_norm": 5.689564228057861,
      "learning_rate": 4.0162129246749655e-05,
      "loss": 1.5141,
      "step": 6560
    },
    {
      "epoch": 2.02175,
      "grad_norm": 7.132659912109375,
      "learning_rate": 4.014569075138992e-05,
      "loss": 1.6539,
      "step": 6565
    },
    {
      "epoch": 2.022,
      "grad_norm": 5.407246112823486,
      "learning_rate": 4.012924190409337e-05,
      "loss": 1.7349,
      "step": 6570
    },
    {
      "epoch": 2.02225,
      "grad_norm": 7.537326812744141,
      "learning_rate": 4.0112782716102644e-05,
      "loss": 1.882,
      "step": 6575
    },
    {
      "epoch": 2.0225,
      "grad_norm": 5.518167972564697,
      "learning_rate": 4.009631319866744e-05,
      "loss": 1.9196,
      "step": 6580
    },
    {
      "epoch": 2.02275,
      "grad_norm": 5.861344814300537,
      "learning_rate": 4.007983336304449e-05,
      "loss": 1.68,
      "step": 6585
    },
    {
      "epoch": 2.023,
      "grad_norm": 5.128395080566406,
      "learning_rate": 4.0063343220497656e-05,
      "loss": 1.7007,
      "step": 6590
    },
    {
      "epoch": 2.02325,
      "grad_norm": 4.993957996368408,
      "learning_rate": 4.004684278229776e-05,
      "loss": 1.4596,
      "step": 6595
    },
    {
      "epoch": 2.0235,
      "grad_norm": 6.450989246368408,
      "learning_rate": 4.003033205972271e-05,
      "loss": 1.6893,
      "step": 6600
    },
    {
      "epoch": 2.02375,
      "grad_norm": 4.374735355377197,
      "learning_rate": 4.001381106405742e-05,
      "loss": 1.4713,
      "step": 6605
    },
    {
      "epoch": 2.024,
      "grad_norm": 5.949417591094971,
      "learning_rate": 3.9997279806593846e-05,
      "loss": 1.5863,
      "step": 6610
    },
    {
      "epoch": 2.02425,
      "grad_norm": 5.422788143157959,
      "learning_rate": 3.998073829863093e-05,
      "loss": 1.4543,
      "step": 6615
    },
    {
      "epoch": 2.0245,
      "grad_norm": 7.194383144378662,
      "learning_rate": 3.996418655147466e-05,
      "loss": 1.8586,
      "step": 6620
    },
    {
      "epoch": 2.02475,
      "grad_norm": 5.199779033660889,
      "learning_rate": 3.9947624576437975e-05,
      "loss": 1.5946,
      "step": 6625
    },
    {
      "epoch": 2.025,
      "grad_norm": 7.014333248138428,
      "learning_rate": 3.993105238484084e-05,
      "loss": 1.6652,
      "step": 6630
    },
    {
      "epoch": 2.02525,
      "grad_norm": 3.617401361465454,
      "learning_rate": 3.99144699880102e-05,
      "loss": 1.5806,
      "step": 6635
    },
    {
      "epoch": 2.0255,
      "grad_norm": 5.892564296722412,
      "learning_rate": 3.989787739727995e-05,
      "loss": 1.7825,
      "step": 6640
    },
    {
      "epoch": 2.02575,
      "grad_norm": 5.412377834320068,
      "learning_rate": 3.988127462399098e-05,
      "loss": 1.7286,
      "step": 6645
    },
    {
      "epoch": 2.026,
      "grad_norm": 5.809926509857178,
      "learning_rate": 3.986466167949113e-05,
      "loss": 1.5184,
      "step": 6650
    },
    {
      "epoch": 2.02625,
      "grad_norm": 6.906655311584473,
      "learning_rate": 3.984803857513518e-05,
      "loss": 1.9174,
      "step": 6655
    },
    {
      "epoch": 2.0265,
      "grad_norm": 4.918240070343018,
      "learning_rate": 3.9831405322284876e-05,
      "loss": 1.495,
      "step": 6660
    },
    {
      "epoch": 2.02675,
      "grad_norm": 7.222111701965332,
      "learning_rate": 3.981476193230889e-05,
      "loss": 1.651,
      "step": 6665
    },
    {
      "epoch": 2.027,
      "grad_norm": 6.979577541351318,
      "learning_rate": 3.9798108416582814e-05,
      "loss": 1.5634,
      "step": 6670
    },
    {
      "epoch": 2.02725,
      "grad_norm": 5.603128910064697,
      "learning_rate": 3.978144478648917e-05,
      "loss": 1.5838,
      "step": 6675
    },
    {
      "epoch": 2.0275,
      "grad_norm": 5.49391508102417,
      "learning_rate": 3.9764771053417404e-05,
      "loss": 1.4428,
      "step": 6680
    },
    {
      "epoch": 2.02775,
      "grad_norm": 5.060539722442627,
      "learning_rate": 3.9748087228763844e-05,
      "loss": 1.6515,
      "step": 6685
    },
    {
      "epoch": 2.028,
      "grad_norm": 8.350167274475098,
      "learning_rate": 3.973139332393172e-05,
      "loss": 1.6028,
      "step": 6690
    },
    {
      "epoch": 2.02825,
      "grad_norm": 6.251204967498779,
      "learning_rate": 3.971468935033118e-05,
      "loss": 1.7654,
      "step": 6695
    },
    {
      "epoch": 2.0285,
      "grad_norm": 5.378579139709473,
      "learning_rate": 3.969797531937921e-05,
      "loss": 1.5495,
      "step": 6700
    },
    {
      "epoch": 2.02875,
      "grad_norm": 7.5916829109191895,
      "learning_rate": 3.968125124249971e-05,
      "loss": 1.6964,
      "step": 6705
    },
    {
      "epoch": 2.029,
      "grad_norm": 5.724853038787842,
      "learning_rate": 3.966451713112342e-05,
      "loss": 1.6058,
      "step": 6710
    },
    {
      "epoch": 2.02925,
      "grad_norm": 4.853397369384766,
      "learning_rate": 3.964777299668795e-05,
      "loss": 1.5708,
      "step": 6715
    },
    {
      "epoch": 2.0295,
      "grad_norm": 3.4928412437438965,
      "learning_rate": 3.963101885063776e-05,
      "loss": 1.3803,
      "step": 6720
    },
    {
      "epoch": 2.02975,
      "grad_norm": 7.107460975646973,
      "learning_rate": 3.9614254704424134e-05,
      "loss": 1.6026,
      "step": 6725
    },
    {
      "epoch": 2.03,
      "grad_norm": 4.314650535583496,
      "learning_rate": 3.959748056950523e-05,
      "loss": 1.5053,
      "step": 6730
    },
    {
      "epoch": 2.03025,
      "grad_norm": 7.227870941162109,
      "learning_rate": 3.958069645734601e-05,
      "loss": 1.6533,
      "step": 6735
    },
    {
      "epoch": 2.0305,
      "grad_norm": 4.930027961730957,
      "learning_rate": 3.9563902379418246e-05,
      "loss": 1.6468,
      "step": 6740
    },
    {
      "epoch": 2.03075,
      "grad_norm": 6.357033729553223,
      "learning_rate": 3.954709834720053e-05,
      "loss": 1.6149,
      "step": 6745
    },
    {
      "epoch": 2.031,
      "grad_norm": 4.526196002960205,
      "learning_rate": 3.953028437217826e-05,
      "loss": 1.5071,
      "step": 6750
    },
    {
      "epoch": 2.03125,
      "grad_norm": 5.435182571411133,
      "learning_rate": 3.951346046584364e-05,
      "loss": 1.7157,
      "step": 6755
    },
    {
      "epoch": 2.0315,
      "grad_norm": 6.042541027069092,
      "learning_rate": 3.9496626639695655e-05,
      "loss": 1.5375,
      "step": 6760
    },
    {
      "epoch": 2.03175,
      "grad_norm": 4.53904914855957,
      "learning_rate": 3.947978290524005e-05,
      "loss": 1.5524,
      "step": 6765
    },
    {
      "epoch": 2.032,
      "grad_norm": 5.224324703216553,
      "learning_rate": 3.946292927398936e-05,
      "loss": 1.4307,
      "step": 6770
    },
    {
      "epoch": 2.03225,
      "grad_norm": 6.190934181213379,
      "learning_rate": 3.944606575746289e-05,
      "loss": 1.4888,
      "step": 6775
    },
    {
      "epoch": 2.0325,
      "grad_norm": 6.163344860076904,
      "learning_rate": 3.94291923671867e-05,
      "loss": 1.3889,
      "step": 6780
    },
    {
      "epoch": 2.03275,
      "grad_norm": 5.403548240661621,
      "learning_rate": 3.941230911469358e-05,
      "loss": 1.3399,
      "step": 6785
    },
    {
      "epoch": 2.033,
      "grad_norm": 5.796809673309326,
      "learning_rate": 3.939541601152308e-05,
      "loss": 1.5508,
      "step": 6790
    },
    {
      "epoch": 2.03325,
      "grad_norm": 5.1159348487854,
      "learning_rate": 3.9378513069221475e-05,
      "loss": 1.5618,
      "step": 6795
    },
    {
      "epoch": 2.0335,
      "grad_norm": 5.159745216369629,
      "learning_rate": 3.9361600299341775e-05,
      "loss": 1.5527,
      "step": 6800
    },
    {
      "epoch": 2.03375,
      "grad_norm": 6.604774475097656,
      "learning_rate": 3.934467771344368e-05,
      "loss": 1.6604,
      "step": 6805
    },
    {
      "epoch": 2.034,
      "grad_norm": 5.4029154777526855,
      "learning_rate": 3.9327745323093646e-05,
      "loss": 1.8201,
      "step": 6810
    },
    {
      "epoch": 2.03425,
      "grad_norm": 5.522858619689941,
      "learning_rate": 3.9310803139864775e-05,
      "loss": 1.4352,
      "step": 6815
    },
    {
      "epoch": 2.0345,
      "grad_norm": 5.243228912353516,
      "learning_rate": 3.9293851175336904e-05,
      "loss": 1.6004,
      "step": 6820
    },
    {
      "epoch": 2.03475,
      "grad_norm": 4.209684371948242,
      "learning_rate": 3.9276889441096534e-05,
      "loss": 1.6483,
      "step": 6825
    },
    {
      "epoch": 2.035,
      "grad_norm": 4.632286548614502,
      "learning_rate": 3.925991794873687e-05,
      "loss": 1.7547,
      "step": 6830
    },
    {
      "epoch": 2.03525,
      "grad_norm": 4.654284954071045,
      "learning_rate": 3.924293670985774e-05,
      "loss": 1.6516,
      "step": 6835
    },
    {
      "epoch": 2.0355,
      "grad_norm": 4.219083309173584,
      "learning_rate": 3.9225945736065664e-05,
      "loss": 1.6009,
      "step": 6840
    },
    {
      "epoch": 2.03575,
      "grad_norm": 4.975121974945068,
      "learning_rate": 3.920894503897382e-05,
      "loss": 1.5946,
      "step": 6845
    },
    {
      "epoch": 2.036,
      "grad_norm": 5.750126838684082,
      "learning_rate": 3.919193463020203e-05,
      "loss": 1.8773,
      "step": 6850
    },
    {
      "epoch": 2.03625,
      "grad_norm": 4.680241584777832,
      "learning_rate": 3.9174914521376746e-05,
      "loss": 1.6737,
      "step": 6855
    },
    {
      "epoch": 2.0365,
      "grad_norm": 8.1240873336792,
      "learning_rate": 3.915788472413105e-05,
      "loss": 1.6744,
      "step": 6860
    },
    {
      "epoch": 2.03675,
      "grad_norm": 5.807589054107666,
      "learning_rate": 3.914084525010464e-05,
      "loss": 1.6924,
      "step": 6865
    },
    {
      "epoch": 2.037,
      "grad_norm": 3.8108837604522705,
      "learning_rate": 3.912379611094384e-05,
      "loss": 1.7208,
      "step": 6870
    },
    {
      "epoch": 2.03725,
      "grad_norm": 4.420182704925537,
      "learning_rate": 3.9106737318301575e-05,
      "loss": 1.6853,
      "step": 6875
    },
    {
      "epoch": 2.0375,
      "grad_norm": 5.878048896789551,
      "learning_rate": 3.908966888383739e-05,
      "loss": 1.856,
      "step": 6880
    },
    {
      "epoch": 2.03775,
      "grad_norm": 4.071852684020996,
      "learning_rate": 3.907259081921736e-05,
      "loss": 1.6266,
      "step": 6885
    },
    {
      "epoch": 2.038,
      "grad_norm": 4.11579704284668,
      "learning_rate": 3.905550313611421e-05,
      "loss": 1.6274,
      "step": 6890
    },
    {
      "epoch": 2.03825,
      "grad_norm": 6.46403169631958,
      "learning_rate": 3.90384058462072e-05,
      "loss": 1.6904,
      "step": 6895
    },
    {
      "epoch": 2.0385,
      "grad_norm": 4.551231384277344,
      "learning_rate": 3.902129896118218e-05,
      "loss": 1.6671,
      "step": 6900
    },
    {
      "epoch": 2.03875,
      "grad_norm": 5.317965507507324,
      "learning_rate": 3.900418249273152e-05,
      "loss": 1.8126,
      "step": 6905
    },
    {
      "epoch": 2.039,
      "grad_norm": 5.309608459472656,
      "learning_rate": 3.898705645255418e-05,
      "loss": 1.6051,
      "step": 6910
    },
    {
      "epoch": 2.03925,
      "grad_norm": 3.927107095718384,
      "learning_rate": 3.8969920852355644e-05,
      "loss": 1.6089,
      "step": 6915
    },
    {
      "epoch": 2.0395,
      "grad_norm": 5.8936767578125,
      "learning_rate": 3.895277570384793e-05,
      "loss": 1.6856,
      "step": 6920
    },
    {
      "epoch": 2.03975,
      "grad_norm": 4.091733455657959,
      "learning_rate": 3.89356210187496e-05,
      "loss": 1.5552,
      "step": 6925
    },
    {
      "epoch": 2.04,
      "grad_norm": 5.197279930114746,
      "learning_rate": 3.89184568087857e-05,
      "loss": 1.5856,
      "step": 6930
    },
    {
      "epoch": 2.04025,
      "grad_norm": 4.284107685089111,
      "learning_rate": 3.890128308568782e-05,
      "loss": 1.6577,
      "step": 6935
    },
    {
      "epoch": 2.0405,
      "grad_norm": 5.2548723220825195,
      "learning_rate": 3.888409986119403e-05,
      "loss": 1.525,
      "step": 6940
    },
    {
      "epoch": 2.04075,
      "grad_norm": 3.9654247760772705,
      "learning_rate": 3.8866907147048895e-05,
      "loss": 1.4727,
      "step": 6945
    },
    {
      "epoch": 2.041,
      "grad_norm": 4.783564567565918,
      "learning_rate": 3.88497049550035e-05,
      "loss": 1.7746,
      "step": 6950
    },
    {
      "epoch": 2.04125,
      "grad_norm": 4.968181610107422,
      "learning_rate": 3.883249329681535e-05,
      "loss": 1.6743,
      "step": 6955
    },
    {
      "epoch": 2.0415,
      "grad_norm": 4.498364448547363,
      "learning_rate": 3.881527218424847e-05,
      "loss": 1.71,
      "step": 6960
    },
    {
      "epoch": 2.04175,
      "grad_norm": 4.507329940795898,
      "learning_rate": 3.879804162907332e-05,
      "loss": 1.7524,
      "step": 6965
    },
    {
      "epoch": 2.042,
      "grad_norm": 6.339420795440674,
      "learning_rate": 3.878080164306682e-05,
      "loss": 1.6805,
      "step": 6970
    },
    {
      "epoch": 2.04225,
      "grad_norm": 5.851861953735352,
      "learning_rate": 3.876355223801235e-05,
      "loss": 1.6576,
      "step": 6975
    },
    {
      "epoch": 2.0425,
      "grad_norm": 4.149860382080078,
      "learning_rate": 3.874629342569971e-05,
      "loss": 1.7062,
      "step": 6980
    },
    {
      "epoch": 2.04275,
      "grad_norm": 5.958932876586914,
      "learning_rate": 3.872902521792513e-05,
      "loss": 1.6817,
      "step": 6985
    },
    {
      "epoch": 2.043,
      "grad_norm": 7.683624744415283,
      "learning_rate": 3.871174762649126e-05,
      "loss": 1.6242,
      "step": 6990
    },
    {
      "epoch": 2.04325,
      "grad_norm": 5.202857494354248,
      "learning_rate": 3.869446066320719e-05,
      "loss": 1.8913,
      "step": 6995
    },
    {
      "epoch": 2.0435,
      "grad_norm": 6.354589939117432,
      "learning_rate": 3.8677164339888394e-05,
      "loss": 1.8528,
      "step": 7000
    },
    {
      "epoch": 2.0435,
      "eval_loss": 1.8247697353363037,
      "eval_runtime": 5.2241,
      "eval_samples_per_second": 196.015,
      "eval_steps_per_second": 24.502,
      "step": 7000
    },
    {
      "epoch": 2.04375,
      "grad_norm": 4.674135208129883,
      "learning_rate": 3.865985866835673e-05,
      "loss": 1.5464,
      "step": 7005
    },
    {
      "epoch": 2.044,
      "grad_norm": 4.800473213195801,
      "learning_rate": 3.864254366044048e-05,
      "loss": 1.5914,
      "step": 7010
    },
    {
      "epoch": 2.04425,
      "grad_norm": 4.237758159637451,
      "learning_rate": 3.862521932797427e-05,
      "loss": 1.5425,
      "step": 7015
    },
    {
      "epoch": 2.0445,
      "grad_norm": 4.6275458335876465,
      "learning_rate": 3.8607885682799126e-05,
      "loss": 1.5688,
      "step": 7020
    },
    {
      "epoch": 2.04475,
      "grad_norm": 5.950570106506348,
      "learning_rate": 3.859054273676244e-05,
      "loss": 1.7329,
      "step": 7025
    },
    {
      "epoch": 2.045,
      "grad_norm": 4.475754261016846,
      "learning_rate": 3.857319050171794e-05,
      "loss": 1.6441,
      "step": 7030
    },
    {
      "epoch": 2.04525,
      "grad_norm": 5.35487174987793,
      "learning_rate": 3.855582898952572e-05,
      "loss": 1.6017,
      "step": 7035
    },
    {
      "epoch": 2.0455,
      "grad_norm": 5.607812404632568,
      "learning_rate": 3.853845821205222e-05,
      "loss": 1.5731,
      "step": 7040
    },
    {
      "epoch": 2.04575,
      "grad_norm": 4.281394004821777,
      "learning_rate": 3.8521078181170176e-05,
      "loss": 1.5909,
      "step": 7045
    },
    {
      "epoch": 2.046,
      "grad_norm": 4.269002437591553,
      "learning_rate": 3.85036889087587e-05,
      "loss": 1.5472,
      "step": 7050
    },
    {
      "epoch": 2.04625,
      "grad_norm": 5.417506217956543,
      "learning_rate": 3.848629040670319e-05,
      "loss": 1.7793,
      "step": 7055
    },
    {
      "epoch": 2.0465,
      "grad_norm": 4.502756118774414,
      "learning_rate": 3.846888268689535e-05,
      "loss": 1.5407,
      "step": 7060
    },
    {
      "epoch": 2.04675,
      "grad_norm": 4.941746234893799,
      "learning_rate": 3.84514657612332e-05,
      "loss": 1.525,
      "step": 7065
    },
    {
      "epoch": 2.047,
      "grad_norm": 5.444134712219238,
      "learning_rate": 3.8434039641621045e-05,
      "loss": 1.5709,
      "step": 7070
    },
    {
      "epoch": 2.04725,
      "grad_norm": 4.77407693862915,
      "learning_rate": 3.841660433996946e-05,
      "loss": 1.6747,
      "step": 7075
    },
    {
      "epoch": 2.0475,
      "grad_norm": 5.163534164428711,
      "learning_rate": 3.8399159868195325e-05,
      "loss": 1.5113,
      "step": 7080
    },
    {
      "epoch": 2.04775,
      "grad_norm": 5.642294883728027,
      "learning_rate": 3.8381706238221764e-05,
      "loss": 1.7211,
      "step": 7085
    },
    {
      "epoch": 2.048,
      "grad_norm": 4.928919792175293,
      "learning_rate": 3.8364243461978185e-05,
      "loss": 1.7371,
      "step": 7090
    },
    {
      "epoch": 2.04825,
      "grad_norm": 6.107212543487549,
      "learning_rate": 3.8346771551400204e-05,
      "loss": 1.8098,
      "step": 7095
    },
    {
      "epoch": 2.0485,
      "grad_norm": 4.328073501586914,
      "learning_rate": 3.832929051842972e-05,
      "loss": 1.6739,
      "step": 7100
    },
    {
      "epoch": 2.04875,
      "grad_norm": 3.9339182376861572,
      "learning_rate": 3.8311800375014853e-05,
      "loss": 1.696,
      "step": 7105
    },
    {
      "epoch": 2.049,
      "grad_norm": 4.81417989730835,
      "learning_rate": 3.8294301133109953e-05,
      "loss": 1.7114,
      "step": 7110
    },
    {
      "epoch": 2.04925,
      "grad_norm": 4.645638942718506,
      "learning_rate": 3.82767928046756e-05,
      "loss": 1.6596,
      "step": 7115
    },
    {
      "epoch": 2.0495,
      "grad_norm": 7.914134979248047,
      "learning_rate": 3.825927540167854e-05,
      "loss": 1.6654,
      "step": 7120
    },
    {
      "epoch": 2.04975,
      "grad_norm": 4.256889343261719,
      "learning_rate": 3.8241748936091775e-05,
      "loss": 1.4771,
      "step": 7125
    },
    {
      "epoch": 2.05,
      "grad_norm": 5.882663249969482,
      "learning_rate": 3.822421341989448e-05,
      "loss": 1.5738,
      "step": 7130
    },
    {
      "epoch": 2.05025,
      "grad_norm": 5.3451104164123535,
      "learning_rate": 3.820666886507201e-05,
      "loss": 1.6796,
      "step": 7135
    },
    {
      "epoch": 2.0505,
      "grad_norm": 4.964216709136963,
      "learning_rate": 3.81891152836159e-05,
      "loss": 1.6556,
      "step": 7140
    },
    {
      "epoch": 2.05075,
      "grad_norm": 4.268764972686768,
      "learning_rate": 3.817155268752385e-05,
      "loss": 1.7566,
      "step": 7145
    },
    {
      "epoch": 2.051,
      "grad_norm": 4.285876750946045,
      "learning_rate": 3.815398108879975e-05,
      "loss": 1.557,
      "step": 7150
    },
    {
      "epoch": 2.05125,
      "grad_norm": 4.483398914337158,
      "learning_rate": 3.8136400499453604e-05,
      "loss": 1.6601,
      "step": 7155
    },
    {
      "epoch": 2.0515,
      "grad_norm": 4.030073642730713,
      "learning_rate": 3.811881093150159e-05,
      "loss": 1.511,
      "step": 7160
    },
    {
      "epoch": 2.05175,
      "grad_norm": 5.5562286376953125,
      "learning_rate": 3.8101212396966016e-05,
      "loss": 1.682,
      "step": 7165
    },
    {
      "epoch": 2.052,
      "grad_norm": 4.457246780395508,
      "learning_rate": 3.808360490787528e-05,
      "loss": 1.4973,
      "step": 7170
    },
    {
      "epoch": 2.05225,
      "grad_norm": 4.490303039550781,
      "learning_rate": 3.806598847626398e-05,
      "loss": 1.5676,
      "step": 7175
    },
    {
      "epoch": 2.0525,
      "grad_norm": 4.605641841888428,
      "learning_rate": 3.804836311417276e-05,
      "loss": 1.6105,
      "step": 7180
    },
    {
      "epoch": 2.05275,
      "grad_norm": 5.644389629364014,
      "learning_rate": 3.803072883364839e-05,
      "loss": 1.5902,
      "step": 7185
    },
    {
      "epoch": 2.053,
      "grad_norm": 4.9907379150390625,
      "learning_rate": 3.8013085646743736e-05,
      "loss": 1.5408,
      "step": 7190
    },
    {
      "epoch": 2.05325,
      "grad_norm": 4.167134761810303,
      "learning_rate": 3.7995433565517735e-05,
      "loss": 1.5589,
      "step": 7195
    },
    {
      "epoch": 2.0535,
      "grad_norm": 5.157418727874756,
      "learning_rate": 3.797777260203544e-05,
      "loss": 1.466,
      "step": 7200
    },
    {
      "epoch": 2.05375,
      "grad_norm": 4.256748676300049,
      "learning_rate": 3.7960102768367955e-05,
      "loss": 1.4815,
      "step": 7205
    },
    {
      "epoch": 2.054,
      "grad_norm": 4.578940391540527,
      "learning_rate": 3.794242407659242e-05,
      "loss": 1.4055,
      "step": 7210
    },
    {
      "epoch": 2.05425,
      "grad_norm": 5.670350551605225,
      "learning_rate": 3.792473653879208e-05,
      "loss": 1.4596,
      "step": 7215
    },
    {
      "epoch": 2.0545,
      "grad_norm": 5.143523693084717,
      "learning_rate": 3.790704016705617e-05,
      "loss": 1.7048,
      "step": 7220
    },
    {
      "epoch": 2.05475,
      "grad_norm": 4.033654689788818,
      "learning_rate": 3.7889334973480034e-05,
      "loss": 1.76,
      "step": 7225
    },
    {
      "epoch": 2.055,
      "grad_norm": 5.419588088989258,
      "learning_rate": 3.7871620970164975e-05,
      "loss": 1.5549,
      "step": 7230
    },
    {
      "epoch": 2.05525,
      "grad_norm": 3.765789747238159,
      "learning_rate": 3.7853898169218346e-05,
      "loss": 1.5926,
      "step": 7235
    },
    {
      "epoch": 2.0555,
      "grad_norm": 4.8190460205078125,
      "learning_rate": 3.783616658275353e-05,
      "loss": 1.5083,
      "step": 7240
    },
    {
      "epoch": 2.05575,
      "grad_norm": 4.425070762634277,
      "learning_rate": 3.781842622288989e-05,
      "loss": 1.5978,
      "step": 7245
    },
    {
      "epoch": 2.056,
      "grad_norm": 4.63145637512207,
      "learning_rate": 3.780067710175281e-05,
      "loss": 1.5528,
      "step": 7250
    },
    {
      "epoch": 2.05625,
      "grad_norm": 6.129134178161621,
      "learning_rate": 3.7782919231473626e-05,
      "loss": 1.6878,
      "step": 7255
    },
    {
      "epoch": 2.0565,
      "grad_norm": 4.031959533691406,
      "learning_rate": 3.7765152624189676e-05,
      "loss": 1.4206,
      "step": 7260
    },
    {
      "epoch": 2.05675,
      "grad_norm": 6.800467014312744,
      "learning_rate": 3.7747377292044276e-05,
      "loss": 1.6345,
      "step": 7265
    },
    {
      "epoch": 2.057,
      "grad_norm": 4.205437183380127,
      "learning_rate": 3.7729593247186704e-05,
      "loss": 1.3854,
      "step": 7270
    },
    {
      "epoch": 2.05725,
      "grad_norm": 3.8621878623962402,
      "learning_rate": 3.771180050177218e-05,
      "loss": 1.5631,
      "step": 7275
    },
    {
      "epoch": 2.0575,
      "grad_norm": 4.625857830047607,
      "learning_rate": 3.7693999067961884e-05,
      "loss": 1.573,
      "step": 7280
    },
    {
      "epoch": 2.05775,
      "grad_norm": 4.785152435302734,
      "learning_rate": 3.767618895792291e-05,
      "loss": 1.3971,
      "step": 7285
    },
    {
      "epoch": 2.058,
      "grad_norm": 4.384203910827637,
      "learning_rate": 3.765837018382831e-05,
      "loss": 1.377,
      "step": 7290
    },
    {
      "epoch": 2.05825,
      "grad_norm": 5.0889434814453125,
      "learning_rate": 3.764054275785707e-05,
      "loss": 1.3795,
      "step": 7295
    },
    {
      "epoch": 2.0585,
      "grad_norm": 4.499428749084473,
      "learning_rate": 3.7622706692194035e-05,
      "loss": 1.6034,
      "step": 7300
    },
    {
      "epoch": 2.05875,
      "grad_norm": 6.633622646331787,
      "learning_rate": 3.760486199903e-05,
      "loss": 1.4956,
      "step": 7305
    },
    {
      "epoch": 2.059,
      "grad_norm": 5.377922534942627,
      "learning_rate": 3.758700869056164e-05,
      "loss": 1.695,
      "step": 7310
    },
    {
      "epoch": 2.05925,
      "grad_norm": 3.538004159927368,
      "learning_rate": 3.7569146778991534e-05,
      "loss": 1.4698,
      "step": 7315
    },
    {
      "epoch": 2.0595,
      "grad_norm": 5.149505138397217,
      "learning_rate": 3.755127627652814e-05,
      "loss": 1.6861,
      "step": 7320
    },
    {
      "epoch": 2.05975,
      "grad_norm": 6.176723957061768,
      "learning_rate": 3.753339719538574e-05,
      "loss": 1.5734,
      "step": 7325
    },
    {
      "epoch": 2.06,
      "grad_norm": 5.191143989562988,
      "learning_rate": 3.751550954778456e-05,
      "loss": 1.7246,
      "step": 7330
    },
    {
      "epoch": 2.06025,
      "grad_norm": 4.77510404586792,
      "learning_rate": 3.74976133459506e-05,
      "loss": 1.577,
      "step": 7335
    },
    {
      "epoch": 2.0605,
      "grad_norm": 5.5144147872924805,
      "learning_rate": 3.747970860211577e-05,
      "loss": 1.8045,
      "step": 7340
    },
    {
      "epoch": 2.06075,
      "grad_norm": 4.56530237197876,
      "learning_rate": 3.746179532851779e-05,
      "loss": 1.5082,
      "step": 7345
    },
    {
      "epoch": 2.061,
      "grad_norm": 4.335904121398926,
      "learning_rate": 3.744387353740021e-05,
      "loss": 1.5377,
      "step": 7350
    },
    {
      "epoch": 2.06125,
      "grad_norm": 5.002376556396484,
      "learning_rate": 3.7425943241012405e-05,
      "loss": 1.5231,
      "step": 7355
    },
    {
      "epoch": 2.0615,
      "grad_norm": 4.201204776763916,
      "learning_rate": 3.740800445160956e-05,
      "loss": 1.559,
      "step": 7360
    },
    {
      "epoch": 2.06175,
      "grad_norm": 5.164073467254639,
      "learning_rate": 3.7390057181452674e-05,
      "loss": 1.4991,
      "step": 7365
    },
    {
      "epoch": 2.062,
      "grad_norm": 4.594114780426025,
      "learning_rate": 3.737210144280854e-05,
      "loss": 1.5813,
      "step": 7370
    },
    {
      "epoch": 2.06225,
      "grad_norm": 6.071612358093262,
      "learning_rate": 3.7354137247949735e-05,
      "loss": 1.631,
      "step": 7375
    },
    {
      "epoch": 2.0625,
      "grad_norm": 4.1479339599609375,
      "learning_rate": 3.7336164609154605e-05,
      "loss": 1.3715,
      "step": 7380
    },
    {
      "epoch": 2.06275,
      "grad_norm": 6.2557196617126465,
      "learning_rate": 3.731818353870729e-05,
      "loss": 1.453,
      "step": 7385
    },
    {
      "epoch": 2.063,
      "grad_norm": 5.2845635414123535,
      "learning_rate": 3.730019404889768e-05,
      "loss": 1.5915,
      "step": 7390
    },
    {
      "epoch": 2.06325,
      "grad_norm": 5.161455154418945,
      "learning_rate": 3.728219615202143e-05,
      "loss": 1.652,
      "step": 7395
    },
    {
      "epoch": 2.0635,
      "grad_norm": 3.8369598388671875,
      "learning_rate": 3.7264189860379914e-05,
      "loss": 1.381,
      "step": 7400
    },
    {
      "epoch": 2.06375,
      "grad_norm": 4.768105506896973,
      "learning_rate": 3.724617518628027e-05,
      "loss": 1.479,
      "step": 7405
    },
    {
      "epoch": 2.064,
      "grad_norm": 5.716830730438232,
      "learning_rate": 3.722815214203537e-05,
      "loss": 1.4429,
      "step": 7410
    },
    {
      "epoch": 2.06425,
      "grad_norm": 6.82916259765625,
      "learning_rate": 3.721012073996378e-05,
      "loss": 1.6453,
      "step": 7415
    },
    {
      "epoch": 2.0645,
      "grad_norm": 3.591109037399292,
      "learning_rate": 3.71920809923898e-05,
      "loss": 1.4613,
      "step": 7420
    },
    {
      "epoch": 2.06475,
      "grad_norm": 5.576654434204102,
      "learning_rate": 3.717403291164343e-05,
      "loss": 1.6239,
      "step": 7425
    },
    {
      "epoch": 2.065,
      "grad_norm": 6.4860944747924805,
      "learning_rate": 3.715597651006036e-05,
      "loss": 1.669,
      "step": 7430
    },
    {
      "epoch": 2.06525,
      "grad_norm": 6.1380205154418945,
      "learning_rate": 3.713791179998197e-05,
      "loss": 1.7049,
      "step": 7435
    },
    {
      "epoch": 2.0655,
      "grad_norm": 5.496415615081787,
      "learning_rate": 3.711983879375531e-05,
      "loss": 1.5353,
      "step": 7440
    },
    {
      "epoch": 2.06575,
      "grad_norm": 5.203444004058838,
      "learning_rate": 3.710175750373312e-05,
      "loss": 1.517,
      "step": 7445
    },
    {
      "epoch": 2.066,
      "grad_norm": 5.47744607925415,
      "learning_rate": 3.708366794227379e-05,
      "loss": 1.5368,
      "step": 7450
    },
    {
      "epoch": 2.06625,
      "grad_norm": 6.189070701599121,
      "learning_rate": 3.7065570121741366e-05,
      "loss": 1.761,
      "step": 7455
    },
    {
      "epoch": 2.0665,
      "grad_norm": 5.655669212341309,
      "learning_rate": 3.704746405450553e-05,
      "loss": 1.5478,
      "step": 7460
    },
    {
      "epoch": 2.06675,
      "grad_norm": 5.884957790374756,
      "learning_rate": 3.7029349752941616e-05,
      "loss": 1.5256,
      "step": 7465
    },
    {
      "epoch": 2.067,
      "grad_norm": 4.780405521392822,
      "learning_rate": 3.701122722943058e-05,
      "loss": 1.514,
      "step": 7470
    },
    {
      "epoch": 2.06725,
      "grad_norm": 4.585354328155518,
      "learning_rate": 3.699309649635899e-05,
      "loss": 1.353,
      "step": 7475
    },
    {
      "epoch": 2.0675,
      "grad_norm": 4.838861465454102,
      "learning_rate": 3.697495756611903e-05,
      "loss": 1.38,
      "step": 7480
    },
    {
      "epoch": 2.06775,
      "grad_norm": 4.845606803894043,
      "learning_rate": 3.695681045110849e-05,
      "loss": 1.3854,
      "step": 7485
    },
    {
      "epoch": 2.068,
      "grad_norm": 3.4433441162109375,
      "learning_rate": 3.693865516373075e-05,
      "loss": 1.551,
      "step": 7490
    },
    {
      "epoch": 2.06825,
      "grad_norm": 3.8527069091796875,
      "learning_rate": 3.692049171639479e-05,
      "loss": 1.4642,
      "step": 7495
    },
    {
      "epoch": 2.0685000000000002,
      "grad_norm": 4.11982536315918,
      "learning_rate": 3.690232012151514e-05,
      "loss": 1.4218,
      "step": 7500
    },
    {
      "epoch": 2.0685000000000002,
      "eval_loss": 1.8814682960510254,
      "eval_runtime": 5.265,
      "eval_samples_per_second": 194.492,
      "eval_steps_per_second": 24.312,
      "step": 7500
    },
    {
      "epoch": 2.06875,
      "grad_norm": 3.626978635787964,
      "learning_rate": 3.688414039151192e-05,
      "loss": 1.3623,
      "step": 7505
    },
    {
      "epoch": 2.069,
      "grad_norm": 3.0572941303253174,
      "learning_rate": 3.686595253881081e-05,
      "loss": 1.3293,
      "step": 7510
    },
    {
      "epoch": 2.06925,
      "grad_norm": 5.071407794952393,
      "learning_rate": 3.684775657584302e-05,
      "loss": 1.368,
      "step": 7515
    },
    {
      "epoch": 2.0695,
      "grad_norm": 3.163193941116333,
      "learning_rate": 3.682955251504533e-05,
      "loss": 1.293,
      "step": 7520
    },
    {
      "epoch": 2.06975,
      "grad_norm": 3.0639946460723877,
      "learning_rate": 3.681134036886003e-05,
      "loss": 1.3685,
      "step": 7525
    },
    {
      "epoch": 2.07,
      "grad_norm": 4.284790992736816,
      "learning_rate": 3.679312014973498e-05,
      "loss": 1.4664,
      "step": 7530
    },
    {
      "epoch": 2.07025,
      "grad_norm": 3.841416120529175,
      "learning_rate": 3.677489187012351e-05,
      "loss": 1.2422,
      "step": 7535
    },
    {
      "epoch": 2.0705,
      "grad_norm": 3.3282153606414795,
      "learning_rate": 3.6756655542484464e-05,
      "loss": 1.3599,
      "step": 7540
    },
    {
      "epoch": 2.07075,
      "grad_norm": 3.789240598678589,
      "learning_rate": 3.6738411179282225e-05,
      "loss": 1.2908,
      "step": 7545
    },
    {
      "epoch": 2.071,
      "grad_norm": 3.9486019611358643,
      "learning_rate": 3.672015879298663e-05,
      "loss": 1.1963,
      "step": 7550
    },
    {
      "epoch": 2.07125,
      "grad_norm": 3.4545416831970215,
      "learning_rate": 3.670189839607301e-05,
      "loss": 1.2579,
      "step": 7555
    },
    {
      "epoch": 2.0715,
      "grad_norm": 4.037593364715576,
      "learning_rate": 3.668363000102218e-05,
      "loss": 1.3328,
      "step": 7560
    },
    {
      "epoch": 2.07175,
      "grad_norm": 4.499300003051758,
      "learning_rate": 3.666535362032041e-05,
      "loss": 1.5832,
      "step": 7565
    },
    {
      "epoch": 2.072,
      "grad_norm": 3.8571999073028564,
      "learning_rate": 3.664706926645945e-05,
      "loss": 1.4191,
      "step": 7570
    },
    {
      "epoch": 2.07225,
      "grad_norm": 3.841790199279785,
      "learning_rate": 3.662877695193646e-05,
      "loss": 1.3157,
      "step": 7575
    },
    {
      "epoch": 2.0725,
      "grad_norm": 3.8172669410705566,
      "learning_rate": 3.661047668925408e-05,
      "loss": 1.1698,
      "step": 7580
    },
    {
      "epoch": 2.07275,
      "grad_norm": 4.154205799102783,
      "learning_rate": 3.6592168490920365e-05,
      "loss": 1.3697,
      "step": 7585
    },
    {
      "epoch": 2.073,
      "grad_norm": 3.963622570037842,
      "learning_rate": 3.657385236944879e-05,
      "loss": 1.3514,
      "step": 7590
    },
    {
      "epoch": 2.07325,
      "grad_norm": 5.19257116317749,
      "learning_rate": 3.6555528337358263e-05,
      "loss": 1.2905,
      "step": 7595
    },
    {
      "epoch": 2.0735,
      "grad_norm": 4.368563652038574,
      "learning_rate": 3.653719640717308e-05,
      "loss": 1.1745,
      "step": 7600
    },
    {
      "epoch": 2.07375,
      "grad_norm": 3.4554319381713867,
      "learning_rate": 3.6518856591422934e-05,
      "loss": 1.4088,
      "step": 7605
    },
    {
      "epoch": 2.074,
      "grad_norm": 4.245859146118164,
      "learning_rate": 3.6500508902642926e-05,
      "loss": 1.269,
      "step": 7610
    },
    {
      "epoch": 2.07425,
      "grad_norm": 3.633298873901367,
      "learning_rate": 3.6482153353373525e-05,
      "loss": 1.2925,
      "step": 7615
    },
    {
      "epoch": 2.0745,
      "grad_norm": 4.797056198120117,
      "learning_rate": 3.6463789956160584e-05,
      "loss": 1.4291,
      "step": 7620
    },
    {
      "epoch": 2.07475,
      "grad_norm": 2.758906126022339,
      "learning_rate": 3.64454187235553e-05,
      "loss": 1.1758,
      "step": 7625
    },
    {
      "epoch": 2.075,
      "grad_norm": 3.3991522789001465,
      "learning_rate": 3.642703966811424e-05,
      "loss": 1.1689,
      "step": 7630
    },
    {
      "epoch": 2.07525,
      "grad_norm": 4.120734691619873,
      "learning_rate": 3.640865280239932e-05,
      "loss": 1.336,
      "step": 7635
    },
    {
      "epoch": 2.0755,
      "grad_norm": 3.829988479614258,
      "learning_rate": 3.6390258138977786e-05,
      "loss": 1.4071,
      "step": 7640
    },
    {
      "epoch": 2.07575,
      "grad_norm": 4.591850757598877,
      "learning_rate": 3.637185569042222e-05,
      "loss": 1.3112,
      "step": 7645
    },
    {
      "epoch": 2.076,
      "grad_norm": 4.313765525817871,
      "learning_rate": 3.6353445469310524e-05,
      "loss": 1.45,
      "step": 7650
    },
    {
      "epoch": 2.07625,
      "grad_norm": 3.5526201725006104,
      "learning_rate": 3.633502748822589e-05,
      "loss": 1.3305,
      "step": 7655
    },
    {
      "epoch": 2.0765,
      "grad_norm": 4.221548557281494,
      "learning_rate": 3.631660175975687e-05,
      "loss": 1.2675,
      "step": 7660
    },
    {
      "epoch": 2.07675,
      "grad_norm": 4.216616153717041,
      "learning_rate": 3.6298168296497246e-05,
      "loss": 1.2982,
      "step": 7665
    },
    {
      "epoch": 2.077,
      "grad_norm": 4.118960857391357,
      "learning_rate": 3.627972711104613e-05,
      "loss": 1.2223,
      "step": 7670
    },
    {
      "epoch": 2.07725,
      "grad_norm": 3.7441391944885254,
      "learning_rate": 3.6261278216007894e-05,
      "loss": 1.3025,
      "step": 7675
    },
    {
      "epoch": 2.0775,
      "grad_norm": 4.066390514373779,
      "learning_rate": 3.6242821623992186e-05,
      "loss": 1.3516,
      "step": 7680
    },
    {
      "epoch": 2.07775,
      "grad_norm": 3.4284605979919434,
      "learning_rate": 3.6224357347613906e-05,
      "loss": 1.2462,
      "step": 7685
    },
    {
      "epoch": 2.078,
      "grad_norm": 3.6135149002075195,
      "learning_rate": 3.620588539949322e-05,
      "loss": 1.3232,
      "step": 7690
    },
    {
      "epoch": 2.07825,
      "grad_norm": 3.3177437782287598,
      "learning_rate": 3.6187405792255535e-05,
      "loss": 1.2381,
      "step": 7695
    },
    {
      "epoch": 2.0785,
      "grad_norm": 4.460937023162842,
      "learning_rate": 3.6168918538531474e-05,
      "loss": 1.212,
      "step": 7700
    },
    {
      "epoch": 2.07875,
      "grad_norm": 3.9861772060394287,
      "learning_rate": 3.615042365095692e-05,
      "loss": 1.2355,
      "step": 7705
    },
    {
      "epoch": 2.079,
      "grad_norm": 5.440366268157959,
      "learning_rate": 3.613192114217293e-05,
      "loss": 1.2659,
      "step": 7710
    },
    {
      "epoch": 2.07925,
      "grad_norm": 4.776958465576172,
      "learning_rate": 3.611341102482581e-05,
      "loss": 1.3684,
      "step": 7715
    },
    {
      "epoch": 2.0795,
      "grad_norm": 3.660489559173584,
      "learning_rate": 3.609489331156705e-05,
      "loss": 1.2499,
      "step": 7720
    },
    {
      "epoch": 2.07975,
      "grad_norm": 5.059250354766846,
      "learning_rate": 3.607636801505334e-05,
      "loss": 1.3218,
      "step": 7725
    },
    {
      "epoch": 2.08,
      "grad_norm": 2.9279632568359375,
      "learning_rate": 3.6057835147946516e-05,
      "loss": 1.2785,
      "step": 7730
    },
    {
      "epoch": 2.08025,
      "grad_norm": 3.85426926612854,
      "learning_rate": 3.6039294722913655e-05,
      "loss": 1.3558,
      "step": 7735
    },
    {
      "epoch": 2.0805,
      "grad_norm": 4.10345458984375,
      "learning_rate": 3.6020746752626936e-05,
      "loss": 1.2843,
      "step": 7740
    },
    {
      "epoch": 2.08075,
      "grad_norm": 5.375462055206299,
      "learning_rate": 3.6002191249763725e-05,
      "loss": 1.338,
      "step": 7745
    },
    {
      "epoch": 2.081,
      "grad_norm": 3.3921866416931152,
      "learning_rate": 3.5983628227006546e-05,
      "loss": 1.3246,
      "step": 7750
    },
    {
      "epoch": 2.08125,
      "grad_norm": 3.663011312484741,
      "learning_rate": 3.5965057697043044e-05,
      "loss": 1.3754,
      "step": 7755
    },
    {
      "epoch": 2.0815,
      "grad_norm": 3.5290167331695557,
      "learning_rate": 3.5946479672566e-05,
      "loss": 1.2914,
      "step": 7760
    },
    {
      "epoch": 2.08175,
      "grad_norm": 3.229116439819336,
      "learning_rate": 3.592789416627332e-05,
      "loss": 1.3045,
      "step": 7765
    },
    {
      "epoch": 2.082,
      "grad_norm": 3.543581962585449,
      "learning_rate": 3.590930119086803e-05,
      "loss": 1.368,
      "step": 7770
    },
    {
      "epoch": 2.08225,
      "grad_norm": 3.595573902130127,
      "learning_rate": 3.5890700759058234e-05,
      "loss": 1.2671,
      "step": 7775
    },
    {
      "epoch": 2.0825,
      "grad_norm": 3.6853671073913574,
      "learning_rate": 3.587209288355716e-05,
      "loss": 1.3321,
      "step": 7780
    },
    {
      "epoch": 2.08275,
      "grad_norm": 3.3741958141326904,
      "learning_rate": 3.585347757708313e-05,
      "loss": 1.2857,
      "step": 7785
    },
    {
      "epoch": 2.083,
      "grad_norm": 3.864981174468994,
      "learning_rate": 3.58348548523595e-05,
      "loss": 1.267,
      "step": 7790
    },
    {
      "epoch": 2.08325,
      "grad_norm": 4.294504642486572,
      "learning_rate": 3.5816224722114754e-05,
      "loss": 1.2547,
      "step": 7795
    },
    {
      "epoch": 2.0835,
      "grad_norm": 5.013479709625244,
      "learning_rate": 3.5797587199082404e-05,
      "loss": 1.4123,
      "step": 7800
    },
    {
      "epoch": 2.08375,
      "grad_norm": 3.231860637664795,
      "learning_rate": 3.5778942296001015e-05,
      "loss": 1.1836,
      "step": 7805
    },
    {
      "epoch": 2.084,
      "grad_norm": 4.601972579956055,
      "learning_rate": 3.576029002561421e-05,
      "loss": 1.1297,
      "step": 7810
    },
    {
      "epoch": 2.08425,
      "grad_norm": 3.1297097206115723,
      "learning_rate": 3.574163040067063e-05,
      "loss": 1.0641,
      "step": 7815
    },
    {
      "epoch": 2.0845,
      "grad_norm": 5.907943248748779,
      "learning_rate": 3.572296343392396e-05,
      "loss": 1.334,
      "step": 7820
    },
    {
      "epoch": 2.08475,
      "grad_norm": 3.3431918621063232,
      "learning_rate": 3.57042891381329e-05,
      "loss": 1.1338,
      "step": 7825
    },
    {
      "epoch": 2.085,
      "grad_norm": 4.118099212646484,
      "learning_rate": 3.5685607526061144e-05,
      "loss": 1.1077,
      "step": 7830
    },
    {
      "epoch": 2.08525,
      "grad_norm": 3.8238956928253174,
      "learning_rate": 3.566691861047741e-05,
      "loss": 1.1413,
      "step": 7835
    },
    {
      "epoch": 2.0855,
      "grad_norm": 3.353344202041626,
      "learning_rate": 3.564822240415538e-05,
      "loss": 1.0738,
      "step": 7840
    },
    {
      "epoch": 2.08575,
      "grad_norm": 3.7872252464294434,
      "learning_rate": 3.562951891987375e-05,
      "loss": 1.21,
      "step": 7845
    },
    {
      "epoch": 2.086,
      "grad_norm": 4.126880645751953,
      "learning_rate": 3.561080817041617e-05,
      "loss": 1.2868,
      "step": 7850
    },
    {
      "epoch": 2.08625,
      "grad_norm": 3.885319948196411,
      "learning_rate": 3.559209016857125e-05,
      "loss": 1.0952,
      "step": 7855
    },
    {
      "epoch": 2.0865,
      "grad_norm": 3.3507235050201416,
      "learning_rate": 3.557336492713258e-05,
      "loss": 1.1132,
      "step": 7860
    },
    {
      "epoch": 2.08675,
      "grad_norm": 3.5582315921783447,
      "learning_rate": 3.555463245889869e-05,
      "loss": 1.1547,
      "step": 7865
    },
    {
      "epoch": 2.087,
      "grad_norm": 4.760918140411377,
      "learning_rate": 3.553589277667303e-05,
      "loss": 1.2576,
      "step": 7870
    },
    {
      "epoch": 2.08725,
      "grad_norm": 3.8929762840270996,
      "learning_rate": 3.5517145893264015e-05,
      "loss": 1.1846,
      "step": 7875
    },
    {
      "epoch": 2.0875,
      "grad_norm": 4.547021865844727,
      "learning_rate": 3.5498391821484944e-05,
      "loss": 1.2179,
      "step": 7880
    },
    {
      "epoch": 2.0877499999999998,
      "grad_norm": 4.023122310638428,
      "learning_rate": 3.547963057415406e-05,
      "loss": 1.113,
      "step": 7885
    },
    {
      "epoch": 2.088,
      "grad_norm": 4.405113697052002,
      "learning_rate": 3.54608621640945e-05,
      "loss": 1.1358,
      "step": 7890
    },
    {
      "epoch": 2.08825,
      "grad_norm": 3.879204273223877,
      "learning_rate": 3.544208660413428e-05,
      "loss": 1.1541,
      "step": 7895
    },
    {
      "epoch": 2.0885,
      "grad_norm": 4.402085781097412,
      "learning_rate": 3.542330390710634e-05,
      "loss": 1.0751,
      "step": 7900
    },
    {
      "epoch": 2.08875,
      "grad_norm": 3.7799923419952393,
      "learning_rate": 3.5404514085848464e-05,
      "loss": 1.2309,
      "step": 7905
    },
    {
      "epoch": 2.089,
      "grad_norm": 3.7439966201782227,
      "learning_rate": 3.538571715320331e-05,
      "loss": 1.1952,
      "step": 7910
    },
    {
      "epoch": 2.08925,
      "grad_norm": 4.157066822052002,
      "learning_rate": 3.536691312201842e-05,
      "loss": 1.1288,
      "step": 7915
    },
    {
      "epoch": 2.0895,
      "grad_norm": 4.636866092681885,
      "learning_rate": 3.534810200514616e-05,
      "loss": 1.1512,
      "step": 7920
    },
    {
      "epoch": 2.08975,
      "grad_norm": 4.380329608917236,
      "learning_rate": 3.532928381544377e-05,
      "loss": 1.0485,
      "step": 7925
    },
    {
      "epoch": 2.09,
      "grad_norm": 4.648103713989258,
      "learning_rate": 3.531045856577328e-05,
      "loss": 1.2067,
      "step": 7930
    },
    {
      "epoch": 2.09025,
      "grad_norm": 3.9974913597106934,
      "learning_rate": 3.5291626269001574e-05,
      "loss": 1.1421,
      "step": 7935
    },
    {
      "epoch": 2.0905,
      "grad_norm": 4.56120491027832,
      "learning_rate": 3.5272786938000376e-05,
      "loss": 1.0532,
      "step": 7940
    },
    {
      "epoch": 2.09075,
      "grad_norm": 4.532558441162109,
      "learning_rate": 3.525394058564616e-05,
      "loss": 0.9672,
      "step": 7945
    },
    {
      "epoch": 2.091,
      "grad_norm": 3.6482601165771484,
      "learning_rate": 3.523508722482025e-05,
      "loss": 1.1194,
      "step": 7950
    },
    {
      "epoch": 2.09125,
      "grad_norm": 3.9155383110046387,
      "learning_rate": 3.521622686840873e-05,
      "loss": 1.1553,
      "step": 7955
    },
    {
      "epoch": 2.0915,
      "grad_norm": 4.53585958480835,
      "learning_rate": 3.519735952930248e-05,
      "loss": 1.2941,
      "step": 7960
    },
    {
      "epoch": 2.09175,
      "grad_norm": 3.4936838150024414,
      "learning_rate": 3.517848522039714e-05,
      "loss": 1.3587,
      "step": 7965
    },
    {
      "epoch": 2.092,
      "grad_norm": 3.466386079788208,
      "learning_rate": 3.515960395459314e-05,
      "loss": 1.1124,
      "step": 7970
    },
    {
      "epoch": 2.09225,
      "grad_norm": 3.738719940185547,
      "learning_rate": 3.514071574479563e-05,
      "loss": 1.2705,
      "step": 7975
    },
    {
      "epoch": 2.0925,
      "grad_norm": 3.97702693939209,
      "learning_rate": 3.512182060391453e-05,
      "loss": 1.141,
      "step": 7980
    },
    {
      "epoch": 2.09275,
      "grad_norm": 3.6597349643707275,
      "learning_rate": 3.510291854486448e-05,
      "loss": 1.2474,
      "step": 7985
    },
    {
      "epoch": 2.093,
      "grad_norm": 4.408993244171143,
      "learning_rate": 3.508400958056489e-05,
      "loss": 1.169,
      "step": 7990
    },
    {
      "epoch": 2.09325,
      "grad_norm": 3.44455623626709,
      "learning_rate": 3.506509372393983e-05,
      "loss": 1.2581,
      "step": 7995
    },
    {
      "epoch": 2.0935,
      "grad_norm": 4.503218173980713,
      "learning_rate": 3.5046170987918116e-05,
      "loss": 1.2906,
      "step": 8000
    },
    {
      "epoch": 2.0935,
      "eval_loss": 1.9998122453689575,
      "eval_runtime": 5.1912,
      "eval_samples_per_second": 197.257,
      "eval_steps_per_second": 24.657,
      "step": 8000
    },
    {
      "epoch": 2.09375,
      "grad_norm": 4.480685234069824,
      "learning_rate": 3.5027241385433264e-05,
      "loss": 1.4048,
      "step": 8005
    },
    {
      "epoch": 2.094,
      "grad_norm": 3.635209083557129,
      "learning_rate": 3.500830492942346e-05,
      "loss": 1.1626,
      "step": 8010
    },
    {
      "epoch": 2.09425,
      "grad_norm": 4.659136772155762,
      "learning_rate": 3.498936163283163e-05,
      "loss": 1.1407,
      "step": 8015
    },
    {
      "epoch": 2.0945,
      "grad_norm": 2.913647413253784,
      "learning_rate": 3.4970411508605314e-05,
      "loss": 1.092,
      "step": 8020
    },
    {
      "epoch": 2.09475,
      "grad_norm": 3.9385576248168945,
      "learning_rate": 3.495145456969675e-05,
      "loss": 1.2352,
      "step": 8025
    },
    {
      "epoch": 2.095,
      "grad_norm": 4.619152545928955,
      "learning_rate": 3.4932490829062815e-05,
      "loss": 1.2505,
      "step": 8030
    },
    {
      "epoch": 2.09525,
      "grad_norm": 4.486905097961426,
      "learning_rate": 3.4913520299665056e-05,
      "loss": 1.1998,
      "step": 8035
    },
    {
      "epoch": 2.0955,
      "grad_norm": 4.018485069274902,
      "learning_rate": 3.489454299446966e-05,
      "loss": 1.2816,
      "step": 8040
    },
    {
      "epoch": 2.09575,
      "grad_norm": 4.82197904586792,
      "learning_rate": 3.487555892644742e-05,
      "loss": 1.311,
      "step": 8045
    },
    {
      "epoch": 2.096,
      "grad_norm": 4.753468990325928,
      "learning_rate": 3.485656810857378e-05,
      "loss": 1.3229,
      "step": 8050
    },
    {
      "epoch": 2.09625,
      "grad_norm": 3.9656808376312256,
      "learning_rate": 3.483757055382878e-05,
      "loss": 1.2603,
      "step": 8055
    },
    {
      "epoch": 2.0965,
      "grad_norm": 3.3786027431488037,
      "learning_rate": 3.481856627519705e-05,
      "loss": 1.2074,
      "step": 8060
    },
    {
      "epoch": 2.09675,
      "grad_norm": 4.024455547332764,
      "learning_rate": 3.479955528566789e-05,
      "loss": 1.2325,
      "step": 8065
    },
    {
      "epoch": 2.097,
      "grad_norm": 4.544833660125732,
      "learning_rate": 3.478053759823507e-05,
      "loss": 1.1594,
      "step": 8070
    },
    {
      "epoch": 2.09725,
      "grad_norm": 4.046412467956543,
      "learning_rate": 3.476151322589704e-05,
      "loss": 1.1723,
      "step": 8075
    },
    {
      "epoch": 2.0975,
      "grad_norm": 8.654879570007324,
      "learning_rate": 3.474248218165675e-05,
      "loss": 1.303,
      "step": 8080
    },
    {
      "epoch": 2.09775,
      "grad_norm": 5.4185791015625,
      "learning_rate": 3.4723444478521764e-05,
      "loss": 1.2284,
      "step": 8085
    },
    {
      "epoch": 2.098,
      "grad_norm": 4.441618919372559,
      "learning_rate": 3.470440012950417e-05,
      "loss": 1.3199,
      "step": 8090
    },
    {
      "epoch": 2.09825,
      "grad_norm": 3.999368906021118,
      "learning_rate": 3.468534914762058e-05,
      "loss": 1.1259,
      "step": 8095
    },
    {
      "epoch": 2.0985,
      "grad_norm": 4.193452835083008,
      "learning_rate": 3.466629154589219e-05,
      "loss": 1.282,
      "step": 8100
    },
    {
      "epoch": 2.09875,
      "grad_norm": 4.042083263397217,
      "learning_rate": 3.4647227337344664e-05,
      "loss": 1.2728,
      "step": 8105
    },
    {
      "epoch": 2.099,
      "grad_norm": 3.423201322555542,
      "learning_rate": 3.462815653500823e-05,
      "loss": 1.1652,
      "step": 8110
    },
    {
      "epoch": 2.09925,
      "grad_norm": 5.728024005889893,
      "learning_rate": 3.460907915191759e-05,
      "loss": 1.337,
      "step": 8115
    },
    {
      "epoch": 2.0995,
      "grad_norm": 3.106931209564209,
      "learning_rate": 3.458999520111195e-05,
      "loss": 1.212,
      "step": 8120
    },
    {
      "epoch": 2.0997500000000002,
      "grad_norm": 3.36381196975708,
      "learning_rate": 3.4570904695635026e-05,
      "loss": 1.1137,
      "step": 8125
    },
    {
      "epoch": 2.1,
      "grad_norm": 3.600296974182129,
      "learning_rate": 3.455180764853499e-05,
      "loss": 1.301,
      "step": 8130
    },
    {
      "epoch": 2.10025,
      "grad_norm": 4.810622215270996,
      "learning_rate": 3.45327040728645e-05,
      "loss": 1.3323,
      "step": 8135
    },
    {
      "epoch": 2.1005,
      "grad_norm": 3.844233512878418,
      "learning_rate": 3.4513593981680654e-05,
      "loss": 1.1571,
      "step": 8140
    },
    {
      "epoch": 2.10075,
      "grad_norm": 4.281815052032471,
      "learning_rate": 3.4494477388045035e-05,
      "loss": 1.1629,
      "step": 8145
    },
    {
      "epoch": 2.101,
      "grad_norm": 5.810364723205566,
      "learning_rate": 3.4475354305023645e-05,
      "loss": 1.2863,
      "step": 8150
    },
    {
      "epoch": 2.10125,
      "grad_norm": 4.9955878257751465,
      "learning_rate": 3.4456224745686935e-05,
      "loss": 1.3803,
      "step": 8155
    },
    {
      "epoch": 2.1015,
      "grad_norm": 3.215630292892456,
      "learning_rate": 3.443708872310978e-05,
      "loss": 1.1331,
      "step": 8160
    },
    {
      "epoch": 2.10175,
      "grad_norm": 3.3629250526428223,
      "learning_rate": 3.441794625037147e-05,
      "loss": 1.1553,
      "step": 8165
    },
    {
      "epoch": 2.102,
      "grad_norm": 4.451306343078613,
      "learning_rate": 3.43987973405557e-05,
      "loss": 1.1328,
      "step": 8170
    },
    {
      "epoch": 2.10225,
      "grad_norm": 3.841831922531128,
      "learning_rate": 3.437964200675058e-05,
      "loss": 1.1985,
      "step": 8175
    },
    {
      "epoch": 2.1025,
      "grad_norm": 3.3633339405059814,
      "learning_rate": 3.436048026204859e-05,
      "loss": 1.1409,
      "step": 8180
    },
    {
      "epoch": 2.10275,
      "grad_norm": 4.224677085876465,
      "learning_rate": 3.43413121195466e-05,
      "loss": 1.1144,
      "step": 8185
    },
    {
      "epoch": 2.103,
      "grad_norm": 3.937563419342041,
      "learning_rate": 3.432213759234587e-05,
      "loss": 1.2678,
      "step": 8190
    },
    {
      "epoch": 2.10325,
      "grad_norm": 4.23951530456543,
      "learning_rate": 3.4302956693551986e-05,
      "loss": 1.0086,
      "step": 8195
    },
    {
      "epoch": 2.1035,
      "grad_norm": 4.816152095794678,
      "learning_rate": 3.428376943627493e-05,
      "loss": 1.2159,
      "step": 8200
    },
    {
      "epoch": 2.10375,
      "grad_norm": 3.047142505645752,
      "learning_rate": 3.426457583362901e-05,
      "loss": 0.9877,
      "step": 8205
    },
    {
      "epoch": 2.104,
      "grad_norm": 3.4881651401519775,
      "learning_rate": 3.424537589873286e-05,
      "loss": 1.2942,
      "step": 8210
    },
    {
      "epoch": 2.10425,
      "grad_norm": 4.878032207489014,
      "learning_rate": 3.422616964470946e-05,
      "loss": 1.2636,
      "step": 8215
    },
    {
      "epoch": 2.1045,
      "grad_norm": 3.584198474884033,
      "learning_rate": 3.4206957084686105e-05,
      "loss": 1.1705,
      "step": 8220
    },
    {
      "epoch": 2.10475,
      "grad_norm": 4.421065330505371,
      "learning_rate": 3.418773823179441e-05,
      "loss": 1.096,
      "step": 8225
    },
    {
      "epoch": 2.105,
      "grad_norm": 4.126249313354492,
      "learning_rate": 3.4168513099170264e-05,
      "loss": 1.2002,
      "step": 8230
    },
    {
      "epoch": 2.10525,
      "grad_norm": 5.103384494781494,
      "learning_rate": 3.414928169995386e-05,
      "loss": 1.1702,
      "step": 8235
    },
    {
      "epoch": 2.1055,
      "grad_norm": 4.930705547332764,
      "learning_rate": 3.41300440472897e-05,
      "loss": 1.1156,
      "step": 8240
    },
    {
      "epoch": 2.10575,
      "grad_norm": 5.55739688873291,
      "learning_rate": 3.4110800154326515e-05,
      "loss": 1.1198,
      "step": 8245
    },
    {
      "epoch": 2.106,
      "grad_norm": 4.346844673156738,
      "learning_rate": 3.409155003421734e-05,
      "loss": 1.0732,
      "step": 8250
    },
    {
      "epoch": 2.10625,
      "grad_norm": 3.4462385177612305,
      "learning_rate": 3.407229370011944e-05,
      "loss": 1.1184,
      "step": 8255
    },
    {
      "epoch": 2.1065,
      "grad_norm": 5.342063903808594,
      "learning_rate": 3.405303116519434e-05,
      "loss": 1.183,
      "step": 8260
    },
    {
      "epoch": 2.10675,
      "grad_norm": 4.292031288146973,
      "learning_rate": 3.403376244260781e-05,
      "loss": 1.1821,
      "step": 8265
    },
    {
      "epoch": 2.107,
      "grad_norm": 4.4320807456970215,
      "learning_rate": 3.401448754552983e-05,
      "loss": 1.1995,
      "step": 8270
    },
    {
      "epoch": 2.10725,
      "grad_norm": 4.459656715393066,
      "learning_rate": 3.3995206487134624e-05,
      "loss": 1.02,
      "step": 8275
    },
    {
      "epoch": 2.1075,
      "grad_norm": 3.746232032775879,
      "learning_rate": 3.39759192806006e-05,
      "loss": 1.2164,
      "step": 8280
    },
    {
      "epoch": 2.10775,
      "grad_norm": 4.2169928550720215,
      "learning_rate": 3.3956625939110376e-05,
      "loss": 1.0957,
      "step": 8285
    },
    {
      "epoch": 2.108,
      "grad_norm": 4.211275100708008,
      "learning_rate": 3.3937326475850786e-05,
      "loss": 1.1405,
      "step": 8290
    },
    {
      "epoch": 2.10825,
      "grad_norm": 4.3355021476745605,
      "learning_rate": 3.391802090401282e-05,
      "loss": 1.1533,
      "step": 8295
    },
    {
      "epoch": 2.1085,
      "grad_norm": 3.8829193115234375,
      "learning_rate": 3.389870923679166e-05,
      "loss": 1.0748,
      "step": 8300
    },
    {
      "epoch": 2.10875,
      "grad_norm": 4.436346054077148,
      "learning_rate": 3.387939148738665e-05,
      "loss": 1.1151,
      "step": 8305
    },
    {
      "epoch": 2.109,
      "grad_norm": 3.3311543464660645,
      "learning_rate": 3.386006766900128e-05,
      "loss": 1.1981,
      "step": 8310
    },
    {
      "epoch": 2.10925,
      "grad_norm": 3.5583362579345703,
      "learning_rate": 3.3840737794843206e-05,
      "loss": 1.1112,
      "step": 8315
    },
    {
      "epoch": 2.1095,
      "grad_norm": 3.9925732612609863,
      "learning_rate": 3.382140187812422e-05,
      "loss": 1.0116,
      "step": 8320
    },
    {
      "epoch": 2.10975,
      "grad_norm": 2.819265127182007,
      "learning_rate": 3.3802059932060216e-05,
      "loss": 0.9989,
      "step": 8325
    },
    {
      "epoch": 2.11,
      "grad_norm": 3.8898611068725586,
      "learning_rate": 3.378271196987126e-05,
      "loss": 1.1202,
      "step": 8330
    },
    {
      "epoch": 2.11025,
      "grad_norm": 5.212835788726807,
      "learning_rate": 3.3763358004781475e-05,
      "loss": 1.0923,
      "step": 8335
    },
    {
      "epoch": 2.1105,
      "grad_norm": 3.9404425621032715,
      "learning_rate": 3.374399805001914e-05,
      "loss": 1.1131,
      "step": 8340
    },
    {
      "epoch": 2.11075,
      "grad_norm": 4.44821834564209,
      "learning_rate": 3.3724632118816584e-05,
      "loss": 1.3793,
      "step": 8345
    },
    {
      "epoch": 2.111,
      "grad_norm": 4.143760681152344,
      "learning_rate": 3.370526022441024e-05,
      "loss": 1.1889,
      "step": 8350
    },
    {
      "epoch": 2.11125,
      "grad_norm": 4.769933223724365,
      "learning_rate": 3.368588238004062e-05,
      "loss": 1.3748,
      "step": 8355
    },
    {
      "epoch": 2.1115,
      "grad_norm": 4.818173408508301,
      "learning_rate": 3.36664985989523e-05,
      "loss": 1.2472,
      "step": 8360
    },
    {
      "epoch": 2.11175,
      "grad_norm": 4.44379997253418,
      "learning_rate": 3.364710889439391e-05,
      "loss": 1.2248,
      "step": 8365
    },
    {
      "epoch": 2.112,
      "grad_norm": 3.852381467819214,
      "learning_rate": 3.3627713279618124e-05,
      "loss": 1.2768,
      "step": 8370
    },
    {
      "epoch": 2.11225,
      "grad_norm": 3.7280752658843994,
      "learning_rate": 3.360831176788166e-05,
      "loss": 1.1861,
      "step": 8375
    },
    {
      "epoch": 2.1125,
      "grad_norm": 3.5358474254608154,
      "learning_rate": 3.358890437244528e-05,
      "loss": 1.4599,
      "step": 8380
    },
    {
      "epoch": 2.11275,
      "grad_norm": 3.721143960952759,
      "learning_rate": 3.3569491106573746e-05,
      "loss": 1.2064,
      "step": 8385
    },
    {
      "epoch": 2.113,
      "grad_norm": 4.000818252563477,
      "learning_rate": 3.355007198353586e-05,
      "loss": 1.2313,
      "step": 8390
    },
    {
      "epoch": 2.11325,
      "grad_norm": 3.531768321990967,
      "learning_rate": 3.353064701660438e-05,
      "loss": 1.247,
      "step": 8395
    },
    {
      "epoch": 2.1135,
      "grad_norm": 3.6750946044921875,
      "learning_rate": 3.3511216219056106e-05,
      "loss": 1.1817,
      "step": 8400
    },
    {
      "epoch": 2.11375,
      "grad_norm": 4.154402732849121,
      "learning_rate": 3.3491779604171816e-05,
      "loss": 1.1824,
      "step": 8405
    },
    {
      "epoch": 2.114,
      "grad_norm": 3.4331655502319336,
      "learning_rate": 3.347233718523622e-05,
      "loss": 1.1733,
      "step": 8410
    },
    {
      "epoch": 2.11425,
      "grad_norm": 5.533315181732178,
      "learning_rate": 3.345288897553806e-05,
      "loss": 1.2099,
      "step": 8415
    },
    {
      "epoch": 2.1145,
      "grad_norm": 3.918092966079712,
      "learning_rate": 3.343343498836999e-05,
      "loss": 1.3594,
      "step": 8420
    },
    {
      "epoch": 2.11475,
      "grad_norm": 4.523391246795654,
      "learning_rate": 3.341397523702862e-05,
      "loss": 1.1842,
      "step": 8425
    },
    {
      "epoch": 2.115,
      "grad_norm": 5.2833733558654785,
      "learning_rate": 3.339450973481452e-05,
      "loss": 1.3488,
      "step": 8430
    },
    {
      "epoch": 2.11525,
      "grad_norm": 5.687223434448242,
      "learning_rate": 3.3375038495032165e-05,
      "loss": 1.3134,
      "step": 8435
    },
    {
      "epoch": 2.1155,
      "grad_norm": 3.6729443073272705,
      "learning_rate": 3.335556153098998e-05,
      "loss": 0.9934,
      "step": 8440
    },
    {
      "epoch": 2.11575,
      "grad_norm": 4.306761741638184,
      "learning_rate": 3.3336078856000276e-05,
      "loss": 1.1503,
      "step": 8445
    },
    {
      "epoch": 2.116,
      "grad_norm": 4.148560047149658,
      "learning_rate": 3.331659048337927e-05,
      "loss": 1.2494,
      "step": 8450
    },
    {
      "epoch": 2.11625,
      "grad_norm": 4.865103721618652,
      "learning_rate": 3.32970964264471e-05,
      "loss": 1.1849,
      "step": 8455
    },
    {
      "epoch": 2.1165,
      "grad_norm": 4.224082946777344,
      "learning_rate": 3.327759669852776e-05,
      "loss": 1.1786,
      "step": 8460
    },
    {
      "epoch": 2.11675,
      "grad_norm": 3.7270827293395996,
      "learning_rate": 3.325809131294912e-05,
      "loss": 1.1533,
      "step": 8465
    },
    {
      "epoch": 2.117,
      "grad_norm": 3.560508966445923,
      "learning_rate": 3.323858028304294e-05,
      "loss": 1.1542,
      "step": 8470
    },
    {
      "epoch": 2.11725,
      "grad_norm": 3.4617204666137695,
      "learning_rate": 3.321906362214482e-05,
      "loss": 1.2282,
      "step": 8475
    },
    {
      "epoch": 2.1175,
      "grad_norm": 3.4883553981781006,
      "learning_rate": 3.319954134359422e-05,
      "loss": 1.1313,
      "step": 8480
    },
    {
      "epoch": 2.11775,
      "grad_norm": 4.3276777267456055,
      "learning_rate": 3.318001346073443e-05,
      "loss": 1.0948,
      "step": 8485
    },
    {
      "epoch": 2.118,
      "grad_norm": 5.0262932777404785,
      "learning_rate": 3.316047998691257e-05,
      "loss": 1.1292,
      "step": 8490
    },
    {
      "epoch": 2.11825,
      "grad_norm": 4.289428234100342,
      "learning_rate": 3.314094093547959e-05,
      "loss": 1.1463,
      "step": 8495
    },
    {
      "epoch": 2.1185,
      "grad_norm": 3.643082618713379,
      "learning_rate": 3.3121396319790235e-05,
      "loss": 1.0123,
      "step": 8500
    },
    {
      "epoch": 2.1185,
      "eval_loss": 2.0195155143737793,
      "eval_runtime": 5.2861,
      "eval_samples_per_second": 193.714,
      "eval_steps_per_second": 24.214,
      "step": 8500
    },
    {
      "epoch": 2.11875,
      "grad_norm": 4.087868690490723,
      "learning_rate": 3.310184615320309e-05,
      "loss": 1.1858,
      "step": 8505
    },
    {
      "epoch": 2.1189999999999998,
      "grad_norm": 4.400173187255859,
      "learning_rate": 3.308229044908048e-05,
      "loss": 1.1981,
      "step": 8510
    },
    {
      "epoch": 2.11925,
      "grad_norm": 3.7945237159729004,
      "learning_rate": 3.306272922078857e-05,
      "loss": 1.1408,
      "step": 8515
    },
    {
      "epoch": 2.1195,
      "grad_norm": 5.500916004180908,
      "learning_rate": 3.304316248169727e-05,
      "loss": 1.1241,
      "step": 8520
    },
    {
      "epoch": 2.11975,
      "grad_norm": 3.915214776992798,
      "learning_rate": 3.302359024518024e-05,
      "loss": 1.2005,
      "step": 8525
    },
    {
      "epoch": 2.12,
      "grad_norm": 3.2352495193481445,
      "learning_rate": 3.300401252461494e-05,
      "loss": 1.1496,
      "step": 8530
    },
    {
      "epoch": 2.12025,
      "grad_norm": 4.242740631103516,
      "learning_rate": 3.298442933338256e-05,
      "loss": 1.0074,
      "step": 8535
    },
    {
      "epoch": 2.1205,
      "grad_norm": 5.106780529022217,
      "learning_rate": 3.296484068486802e-05,
      "loss": 1.1512,
      "step": 8540
    },
    {
      "epoch": 2.12075,
      "grad_norm": 6.071771144866943,
      "learning_rate": 3.294524659245996e-05,
      "loss": 1.253,
      "step": 8545
    },
    {
      "epoch": 2.121,
      "grad_norm": 3.8794896602630615,
      "learning_rate": 3.2925647069550794e-05,
      "loss": 1.2036,
      "step": 8550
    },
    {
      "epoch": 2.12125,
      "grad_norm": 3.3821887969970703,
      "learning_rate": 3.290604212953658e-05,
      "loss": 1.0613,
      "step": 8555
    },
    {
      "epoch": 2.1215,
      "grad_norm": 4.052857875823975,
      "learning_rate": 3.288643178581712e-05,
      "loss": 1.2282,
      "step": 8560
    },
    {
      "epoch": 2.12175,
      "grad_norm": 4.552335739135742,
      "learning_rate": 3.2866816051795894e-05,
      "loss": 1.1221,
      "step": 8565
    },
    {
      "epoch": 2.122,
      "grad_norm": 3.869302272796631,
      "learning_rate": 3.284719494088009e-05,
      "loss": 1.2203,
      "step": 8570
    },
    {
      "epoch": 2.12225,
      "grad_norm": 4.188571453094482,
      "learning_rate": 3.282756846648052e-05,
      "loss": 1.1696,
      "step": 8575
    },
    {
      "epoch": 2.1225,
      "grad_norm": 3.6853747367858887,
      "learning_rate": 3.280793664201173e-05,
      "loss": 1.2051,
      "step": 8580
    },
    {
      "epoch": 2.12275,
      "grad_norm": 4.427053928375244,
      "learning_rate": 3.2788299480891866e-05,
      "loss": 1.1764,
      "step": 8585
    },
    {
      "epoch": 2.123,
      "grad_norm": 4.284086227416992,
      "learning_rate": 3.276865699654274e-05,
      "loss": 1.0958,
      "step": 8590
    },
    {
      "epoch": 2.12325,
      "grad_norm": 4.749359607696533,
      "learning_rate": 3.2749009202389814e-05,
      "loss": 1.0554,
      "step": 8595
    },
    {
      "epoch": 2.1235,
      "grad_norm": 3.5869710445404053,
      "learning_rate": 3.272935611186218e-05,
      "loss": 1.1451,
      "step": 8600
    },
    {
      "epoch": 2.12375,
      "grad_norm": 4.1584978103637695,
      "learning_rate": 3.27096977383925e-05,
      "loss": 1.1125,
      "step": 8605
    },
    {
      "epoch": 2.124,
      "grad_norm": 4.1740007400512695,
      "learning_rate": 3.269003409541713e-05,
      "loss": 1.1063,
      "step": 8610
    },
    {
      "epoch": 2.12425,
      "grad_norm": 4.300711631774902,
      "learning_rate": 3.2670365196375965e-05,
      "loss": 1.1006,
      "step": 8615
    },
    {
      "epoch": 2.1245,
      "grad_norm": 3.5820868015289307,
      "learning_rate": 3.2650691054712526e-05,
      "loss": 1.1486,
      "step": 8620
    },
    {
      "epoch": 2.12475,
      "grad_norm": 3.9251890182495117,
      "learning_rate": 3.263101168387388e-05,
      "loss": 1.0202,
      "step": 8625
    },
    {
      "epoch": 2.125,
      "grad_norm": 6.024598598480225,
      "learning_rate": 3.261132709731071e-05,
      "loss": 1.2133,
      "step": 8630
    },
    {
      "epoch": 2.12525,
      "grad_norm": 3.6956779956817627,
      "learning_rate": 3.259163730847723e-05,
      "loss": 1.1329,
      "step": 8635
    },
    {
      "epoch": 2.1255,
      "grad_norm": 3.9965691566467285,
      "learning_rate": 3.257194233083124e-05,
      "loss": 1.19,
      "step": 8640
    },
    {
      "epoch": 2.12575,
      "grad_norm": 4.374082565307617,
      "learning_rate": 3.255224217783408e-05,
      "loss": 1.0184,
      "step": 8645
    },
    {
      "epoch": 2.126,
      "grad_norm": 4.381711483001709,
      "learning_rate": 3.253253686295059e-05,
      "loss": 1.0981,
      "step": 8650
    },
    {
      "epoch": 2.12625,
      "grad_norm": 3.9536643028259277,
      "learning_rate": 3.2512826399649175e-05,
      "loss": 1.1165,
      "step": 8655
    },
    {
      "epoch": 2.1265,
      "grad_norm": 6.899511337280273,
      "learning_rate": 3.2493110801401774e-05,
      "loss": 1.1186,
      "step": 8660
    },
    {
      "epoch": 2.12675,
      "grad_norm": 4.4089789390563965,
      "learning_rate": 3.2473390081683784e-05,
      "loss": 1.1144,
      "step": 8665
    },
    {
      "epoch": 2.127,
      "grad_norm": 3.9418671131134033,
      "learning_rate": 3.245366425397416e-05,
      "loss": 1.0825,
      "step": 8670
    },
    {
      "epoch": 2.12725,
      "grad_norm": 5.6633782386779785,
      "learning_rate": 3.2433933331755285e-05,
      "loss": 1.0227,
      "step": 8675
    },
    {
      "epoch": 2.1275,
      "grad_norm": 3.6680355072021484,
      "learning_rate": 3.241419732851308e-05,
      "loss": 1.0529,
      "step": 8680
    },
    {
      "epoch": 2.12775,
      "grad_norm": 3.8757245540618896,
      "learning_rate": 3.2394456257736916e-05,
      "loss": 1.1801,
      "step": 8685
    },
    {
      "epoch": 2.128,
      "grad_norm": 4.0938005447387695,
      "learning_rate": 3.2374710132919616e-05,
      "loss": 1.0843,
      "step": 8690
    },
    {
      "epoch": 2.12825,
      "grad_norm": 6.066534042358398,
      "learning_rate": 3.23549589675575e-05,
      "loss": 1.2011,
      "step": 8695
    },
    {
      "epoch": 2.1285,
      "grad_norm": 3.9814765453338623,
      "learning_rate": 3.233520277515026e-05,
      "loss": 1.1242,
      "step": 8700
    },
    {
      "epoch": 2.12875,
      "grad_norm": 3.7603554725646973,
      "learning_rate": 3.23154415692011e-05,
      "loss": 1.3641,
      "step": 8705
    },
    {
      "epoch": 2.129,
      "grad_norm": 3.465862989425659,
      "learning_rate": 3.2295675363216606e-05,
      "loss": 1.0269,
      "step": 8710
    },
    {
      "epoch": 2.12925,
      "grad_norm": 3.965405225753784,
      "learning_rate": 3.2275904170706797e-05,
      "loss": 1.0547,
      "step": 8715
    },
    {
      "epoch": 2.1295,
      "grad_norm": 3.191563367843628,
      "learning_rate": 3.2256128005185096e-05,
      "loss": 1.0977,
      "step": 8720
    },
    {
      "epoch": 2.12975,
      "grad_norm": 4.727142333984375,
      "learning_rate": 3.223634688016832e-05,
      "loss": 0.9968,
      "step": 8725
    },
    {
      "epoch": 2.13,
      "grad_norm": 4.260378360748291,
      "learning_rate": 3.221656080917669e-05,
      "loss": 1.1033,
      "step": 8730
    },
    {
      "epoch": 2.13025,
      "grad_norm": 4.347822189331055,
      "learning_rate": 3.2196769805733794e-05,
      "loss": 1.066,
      "step": 8735
    },
    {
      "epoch": 2.1305,
      "grad_norm": 4.35614538192749,
      "learning_rate": 3.21769738833666e-05,
      "loss": 1.1726,
      "step": 8740
    },
    {
      "epoch": 2.13075,
      "grad_norm": 3.869988441467285,
      "learning_rate": 3.215717305560543e-05,
      "loss": 1.1417,
      "step": 8745
    },
    {
      "epoch": 2.1310000000000002,
      "grad_norm": 4.959303379058838,
      "learning_rate": 3.2137367335983965e-05,
      "loss": 0.9996,
      "step": 8750
    },
    {
      "epoch": 2.13125,
      "grad_norm": 5.000332355499268,
      "learning_rate": 3.211755673803922e-05,
      "loss": 1.063,
      "step": 8755
    },
    {
      "epoch": 2.1315,
      "grad_norm": 3.7695138454437256,
      "learning_rate": 3.2097741275311574e-05,
      "loss": 0.9966,
      "step": 8760
    },
    {
      "epoch": 2.13175,
      "grad_norm": 4.947207450866699,
      "learning_rate": 3.207792096134468e-05,
      "loss": 1.0608,
      "step": 8765
    },
    {
      "epoch": 2.132,
      "grad_norm": 3.993548631668091,
      "learning_rate": 3.205809580968556e-05,
      "loss": 1.0394,
      "step": 8770
    },
    {
      "epoch": 2.13225,
      "grad_norm": 5.677649021148682,
      "learning_rate": 3.20382658338845e-05,
      "loss": 1.0396,
      "step": 8775
    },
    {
      "epoch": 2.1325,
      "grad_norm": 3.3042759895324707,
      "learning_rate": 3.201843104749511e-05,
      "loss": 0.9974,
      "step": 8780
    },
    {
      "epoch": 2.13275,
      "grad_norm": 4.571619987487793,
      "learning_rate": 3.199859146407429e-05,
      "loss": 1.0238,
      "step": 8785
    },
    {
      "epoch": 2.133,
      "grad_norm": 2.8210065364837646,
      "learning_rate": 3.1978747097182185e-05,
      "loss": 0.8304,
      "step": 8790
    },
    {
      "epoch": 2.13325,
      "grad_norm": 7.247515678405762,
      "learning_rate": 3.195889796038225e-05,
      "loss": 1.1436,
      "step": 8795
    },
    {
      "epoch": 2.1335,
      "grad_norm": 3.322685718536377,
      "learning_rate": 3.193904406724117e-05,
      "loss": 0.8884,
      "step": 8800
    },
    {
      "epoch": 2.13375,
      "grad_norm": 3.025338649749756,
      "learning_rate": 3.19191854313289e-05,
      "loss": 0.8064,
      "step": 8805
    },
    {
      "epoch": 2.134,
      "grad_norm": 3.749882221221924,
      "learning_rate": 3.189932206621865e-05,
      "loss": 1.0017,
      "step": 8810
    },
    {
      "epoch": 2.13425,
      "grad_norm": 3.783776044845581,
      "learning_rate": 3.187945398548681e-05,
      "loss": 1.0133,
      "step": 8815
    },
    {
      "epoch": 2.1345,
      "grad_norm": 3.202890634536743,
      "learning_rate": 3.185958120271305e-05,
      "loss": 1.0278,
      "step": 8820
    },
    {
      "epoch": 2.13475,
      "grad_norm": 4.8442463874816895,
      "learning_rate": 3.1839703731480205e-05,
      "loss": 1.0896,
      "step": 8825
    },
    {
      "epoch": 2.135,
      "grad_norm": 4.163353443145752,
      "learning_rate": 3.181982158537436e-05,
      "loss": 1.1577,
      "step": 8830
    },
    {
      "epoch": 2.13525,
      "grad_norm": 6.422883987426758,
      "learning_rate": 3.179993477798477e-05,
      "loss": 1.188,
      "step": 8835
    },
    {
      "epoch": 2.1355,
      "grad_norm": 4.31515645980835,
      "learning_rate": 3.178004332290388e-05,
      "loss": 1.1354,
      "step": 8840
    },
    {
      "epoch": 2.13575,
      "grad_norm": 5.426665782928467,
      "learning_rate": 3.176014723372731e-05,
      "loss": 1.2016,
      "step": 8845
    },
    {
      "epoch": 2.136,
      "grad_norm": 4.75919771194458,
      "learning_rate": 3.1740246524053854e-05,
      "loss": 0.9789,
      "step": 8850
    },
    {
      "epoch": 2.13625,
      "grad_norm": 4.56886100769043,
      "learning_rate": 3.172034120748546e-05,
      "loss": 1.0507,
      "step": 8855
    },
    {
      "epoch": 2.1365,
      "grad_norm": 4.201623439788818,
      "learning_rate": 3.170043129762721e-05,
      "loss": 1.2085,
      "step": 8860
    },
    {
      "epoch": 2.13675,
      "grad_norm": 4.277887344360352,
      "learning_rate": 3.168051680808735e-05,
      "loss": 1.1332,
      "step": 8865
    },
    {
      "epoch": 2.137,
      "grad_norm": 4.663999080657959,
      "learning_rate": 3.166059775247724e-05,
      "loss": 1.2206,
      "step": 8870
    },
    {
      "epoch": 2.13725,
      "grad_norm": 4.055154800415039,
      "learning_rate": 3.164067414441138e-05,
      "loss": 1.2238,
      "step": 8875
    },
    {
      "epoch": 2.1375,
      "grad_norm": 4.998751163482666,
      "learning_rate": 3.1620745997507364e-05,
      "loss": 0.965,
      "step": 8880
    },
    {
      "epoch": 2.13775,
      "grad_norm": 5.478657245635986,
      "learning_rate": 3.1600813325385883e-05,
      "loss": 1.1009,
      "step": 8885
    },
    {
      "epoch": 2.138,
      "grad_norm": 3.649345874786377,
      "learning_rate": 3.158087614167073e-05,
      "loss": 1.1606,
      "step": 8890
    },
    {
      "epoch": 2.13825,
      "grad_norm": 4.739358901977539,
      "learning_rate": 3.1560934459988795e-05,
      "loss": 1.0905,
      "step": 8895
    },
    {
      "epoch": 2.1385,
      "grad_norm": 5.6891865730285645,
      "learning_rate": 3.154098829397002e-05,
      "loss": 1.2492,
      "step": 8900
    },
    {
      "epoch": 2.13875,
      "grad_norm": 4.406590938568115,
      "learning_rate": 3.152103765724743e-05,
      "loss": 1.2376,
      "step": 8905
    },
    {
      "epoch": 2.1390000000000002,
      "grad_norm": 3.9781556129455566,
      "learning_rate": 3.1501082563457084e-05,
      "loss": 1.0938,
      "step": 8910
    },
    {
      "epoch": 2.13925,
      "grad_norm": 3.993267774581909,
      "learning_rate": 3.148112302623811e-05,
      "loss": 1.1342,
      "step": 8915
    },
    {
      "epoch": 2.1395,
      "grad_norm": 3.8145132064819336,
      "learning_rate": 3.146115905923266e-05,
      "loss": 1.0307,
      "step": 8920
    },
    {
      "epoch": 2.13975,
      "grad_norm": 3.5352423191070557,
      "learning_rate": 3.1441190676085916e-05,
      "loss": 1.111,
      "step": 8925
    },
    {
      "epoch": 2.14,
      "grad_norm": 5.8878936767578125,
      "learning_rate": 3.1421217890446086e-05,
      "loss": 1.0039,
      "step": 8930
    },
    {
      "epoch": 2.14025,
      "grad_norm": 5.130059242248535,
      "learning_rate": 3.140124071596437e-05,
      "loss": 1.1654,
      "step": 8935
    },
    {
      "epoch": 2.1405,
      "grad_norm": 4.422653675079346,
      "learning_rate": 3.1381259166294995e-05,
      "loss": 0.9934,
      "step": 8940
    },
    {
      "epoch": 2.14075,
      "grad_norm": 5.075170516967773,
      "learning_rate": 3.1361273255095146e-05,
      "loss": 1.1289,
      "step": 8945
    },
    {
      "epoch": 2.141,
      "grad_norm": 3.9976658821105957,
      "learning_rate": 3.1341282996025014e-05,
      "loss": 1.0378,
      "step": 8950
    },
    {
      "epoch": 2.14125,
      "grad_norm": 4.515373706817627,
      "learning_rate": 3.132128840274774e-05,
      "loss": 1.1313,
      "step": 8955
    },
    {
      "epoch": 2.1415,
      "grad_norm": 4.023477077484131,
      "learning_rate": 3.130128948892946e-05,
      "loss": 0.9648,
      "step": 8960
    },
    {
      "epoch": 2.14175,
      "grad_norm": 3.994297742843628,
      "learning_rate": 3.128128626823922e-05,
      "loss": 0.9551,
      "step": 8965
    },
    {
      "epoch": 2.142,
      "grad_norm": 4.910055160522461,
      "learning_rate": 3.1261278754349055e-05,
      "loss": 1.1447,
      "step": 8970
    },
    {
      "epoch": 2.1422499999999998,
      "grad_norm": 6.5330328941345215,
      "learning_rate": 3.1241266960933904e-05,
      "loss": 1.1157,
      "step": 8975
    },
    {
      "epoch": 2.1425,
      "grad_norm": 3.3815810680389404,
      "learning_rate": 3.122125090167162e-05,
      "loss": 0.9863,
      "step": 8980
    },
    {
      "epoch": 2.14275,
      "grad_norm": 3.0370821952819824,
      "learning_rate": 3.120123059024303e-05,
      "loss": 1.0371,
      "step": 8985
    },
    {
      "epoch": 2.143,
      "grad_norm": 3.656813859939575,
      "learning_rate": 3.118120604033178e-05,
      "loss": 0.8929,
      "step": 8990
    },
    {
      "epoch": 2.14325,
      "grad_norm": 4.259893894195557,
      "learning_rate": 3.116117726562451e-05,
      "loss": 0.9371,
      "step": 8995
    },
    {
      "epoch": 2.1435,
      "grad_norm": 4.897599697113037,
      "learning_rate": 3.1141144279810666e-05,
      "loss": 1.1403,
      "step": 9000
    },
    {
      "epoch": 2.1435,
      "eval_loss": 2.072112560272217,
      "eval_runtime": 5.411,
      "eval_samples_per_second": 189.245,
      "eval_steps_per_second": 23.656,
      "step": 9000
    },
    {
      "epoch": 2.14375,
      "grad_norm": 5.485087871551514,
      "learning_rate": 3.112110709658261e-05,
      "loss": 1.0418,
      "step": 9005
    },
    {
      "epoch": 2.144,
      "grad_norm": 4.3355607986450195,
      "learning_rate": 3.1101065729635574e-05,
      "loss": 0.9776,
      "step": 9010
    },
    {
      "epoch": 2.14425,
      "grad_norm": 2.997403860092163,
      "learning_rate": 3.108102019266764e-05,
      "loss": 1.0815,
      "step": 9015
    },
    {
      "epoch": 2.1445,
      "grad_norm": 3.939298152923584,
      "learning_rate": 3.106097049937976e-05,
      "loss": 0.8703,
      "step": 9020
    },
    {
      "epoch": 2.14475,
      "grad_norm": 3.8220083713531494,
      "learning_rate": 3.1040916663475676e-05,
      "loss": 0.877,
      "step": 9025
    },
    {
      "epoch": 2.145,
      "grad_norm": 4.491975784301758,
      "learning_rate": 3.1020858698662026e-05,
      "loss": 0.9973,
      "step": 9030
    },
    {
      "epoch": 2.14525,
      "grad_norm": 3.1537322998046875,
      "learning_rate": 3.1000796618648225e-05,
      "loss": 0.8011,
      "step": 9035
    },
    {
      "epoch": 2.1455,
      "grad_norm": 4.922860145568848,
      "learning_rate": 3.0980730437146516e-05,
      "loss": 0.9765,
      "step": 9040
    },
    {
      "epoch": 2.14575,
      "grad_norm": 3.3490281105041504,
      "learning_rate": 3.096066016787195e-05,
      "loss": 1.0641,
      "step": 9045
    },
    {
      "epoch": 2.146,
      "grad_norm": 3.9529988765716553,
      "learning_rate": 3.094058582454237e-05,
      "loss": 0.8943,
      "step": 9050
    },
    {
      "epoch": 2.14625,
      "grad_norm": 3.9109809398651123,
      "learning_rate": 3.092050742087839e-05,
      "loss": 0.9788,
      "step": 9055
    },
    {
      "epoch": 2.1465,
      "grad_norm": 8.098536491394043,
      "learning_rate": 3.090042497060342e-05,
      "loss": 1.8877,
      "step": 9060
    },
    {
      "epoch": 2.14675,
      "grad_norm": 7.1124348640441895,
      "learning_rate": 3.088033848744362e-05,
      "loss": 2.1083,
      "step": 9065
    },
    {
      "epoch": 2.147,
      "grad_norm": 6.472527503967285,
      "learning_rate": 3.08602479851279e-05,
      "loss": 1.9418,
      "step": 9070
    },
    {
      "epoch": 2.14725,
      "grad_norm": 7.707386016845703,
      "learning_rate": 3.0840153477387953e-05,
      "loss": 1.7967,
      "step": 9075
    },
    {
      "epoch": 2.1475,
      "grad_norm": 5.816948413848877,
      "learning_rate": 3.082005497795817e-05,
      "loss": 1.7053,
      "step": 9080
    },
    {
      "epoch": 2.14775,
      "grad_norm": 6.726969242095947,
      "learning_rate": 3.0799952500575694e-05,
      "loss": 1.6629,
      "step": 9085
    },
    {
      "epoch": 2.148,
      "grad_norm": 6.114763259887695,
      "learning_rate": 3.077984605898038e-05,
      "loss": 1.6416,
      "step": 9090
    },
    {
      "epoch": 2.14825,
      "grad_norm": 6.3625969886779785,
      "learning_rate": 3.075973566691477e-05,
      "loss": 1.6101,
      "step": 9095
    },
    {
      "epoch": 2.1485,
      "grad_norm": 4.960183143615723,
      "learning_rate": 3.0739621338124145e-05,
      "loss": 1.697,
      "step": 9100
    },
    {
      "epoch": 2.14875,
      "grad_norm": 10.007241249084473,
      "learning_rate": 3.0719503086356464e-05,
      "loss": 1.7456,
      "step": 9105
    },
    {
      "epoch": 2.149,
      "grad_norm": 5.9353437423706055,
      "learning_rate": 3.069938092536235e-05,
      "loss": 1.6975,
      "step": 9110
    },
    {
      "epoch": 2.14925,
      "grad_norm": 4.393304347991943,
      "learning_rate": 3.067925486889512e-05,
      "loss": 1.6063,
      "step": 9115
    },
    {
      "epoch": 2.1495,
      "grad_norm": 5.224595546722412,
      "learning_rate": 3.0659124930710734e-05,
      "loss": 1.6654,
      "step": 9120
    },
    {
      "epoch": 2.14975,
      "grad_norm": 4.581831932067871,
      "learning_rate": 3.063899112456782e-05,
      "loss": 1.5751,
      "step": 9125
    },
    {
      "epoch": 2.15,
      "grad_norm": 5.760463714599609,
      "learning_rate": 3.0618853464227646e-05,
      "loss": 1.5741,
      "step": 9130
    },
    {
      "epoch": 2.1502499999999998,
      "grad_norm": 4.8696722984313965,
      "learning_rate": 3.0598711963454115e-05,
      "loss": 1.564,
      "step": 9135
    },
    {
      "epoch": 2.1505,
      "grad_norm": 6.988763332366943,
      "learning_rate": 3.057856663601375e-05,
      "loss": 1.7082,
      "step": 9140
    },
    {
      "epoch": 2.15075,
      "grad_norm": 7.313651084899902,
      "learning_rate": 3.055841749567569e-05,
      "loss": 1.6581,
      "step": 9145
    },
    {
      "epoch": 2.151,
      "grad_norm": 5.395272254943848,
      "learning_rate": 3.053826455621168e-05,
      "loss": 1.6469,
      "step": 9150
    },
    {
      "epoch": 2.15125,
      "grad_norm": 4.985645771026611,
      "learning_rate": 3.051810783139607e-05,
      "loss": 1.7665,
      "step": 9155
    },
    {
      "epoch": 2.1515,
      "grad_norm": 5.0618205070495605,
      "learning_rate": 3.0497947335005793e-05,
      "loss": 1.695,
      "step": 9160
    },
    {
      "epoch": 2.15175,
      "grad_norm": 6.57080602645874,
      "learning_rate": 3.0477783080820362e-05,
      "loss": 1.629,
      "step": 9165
    },
    {
      "epoch": 2.152,
      "grad_norm": 9.046772956848145,
      "learning_rate": 3.045761508262184e-05,
      "loss": 1.9316,
      "step": 9170
    },
    {
      "epoch": 2.15225,
      "grad_norm": 11.091529846191406,
      "learning_rate": 3.0437443354194872e-05,
      "loss": 1.8021,
      "step": 9175
    },
    {
      "epoch": 2.1525,
      "grad_norm": 9.780327796936035,
      "learning_rate": 3.041726790932664e-05,
      "loss": 1.6676,
      "step": 9180
    },
    {
      "epoch": 2.15275,
      "grad_norm": 8.770477294921875,
      "learning_rate": 3.039708876180688e-05,
      "loss": 1.7043,
      "step": 9185
    },
    {
      "epoch": 2.153,
      "grad_norm": 7.1704912185668945,
      "learning_rate": 3.037690592542784e-05,
      "loss": 1.9582,
      "step": 9190
    },
    {
      "epoch": 2.15325,
      "grad_norm": 4.587029457092285,
      "learning_rate": 3.0356719413984296e-05,
      "loss": 1.3806,
      "step": 9195
    },
    {
      "epoch": 3.00025,
      "grad_norm": 5.4067301750183105,
      "learning_rate": 3.0336529241273546e-05,
      "loss": 1.1064,
      "step": 9200
    },
    {
      "epoch": 3.0005,
      "grad_norm": 5.144780158996582,
      "learning_rate": 3.0316335421095388e-05,
      "loss": 1.0367,
      "step": 9205
    },
    {
      "epoch": 3.00075,
      "grad_norm": 4.557008743286133,
      "learning_rate": 3.029613796725209e-05,
      "loss": 1.071,
      "step": 9210
    },
    {
      "epoch": 3.001,
      "grad_norm": 6.599572658538818,
      "learning_rate": 3.027593689354844e-05,
      "loss": 0.9661,
      "step": 9215
    },
    {
      "epoch": 3.00125,
      "grad_norm": 5.805813312530518,
      "learning_rate": 3.0255732213791665e-05,
      "loss": 1.116,
      "step": 9220
    },
    {
      "epoch": 3.0015,
      "grad_norm": 5.549100399017334,
      "learning_rate": 3.0235523941791484e-05,
      "loss": 1.1312,
      "step": 9225
    },
    {
      "epoch": 3.00175,
      "grad_norm": 4.460596084594727,
      "learning_rate": 3.021531209136006e-05,
      "loss": 0.8955,
      "step": 9230
    },
    {
      "epoch": 3.002,
      "grad_norm": 4.822891712188721,
      "learning_rate": 3.019509667631199e-05,
      "loss": 0.9972,
      "step": 9235
    },
    {
      "epoch": 3.00225,
      "grad_norm": 4.7125372886657715,
      "learning_rate": 3.0174877710464333e-05,
      "loss": 0.9906,
      "step": 9240
    },
    {
      "epoch": 3.0025,
      "grad_norm": 6.400511741638184,
      "learning_rate": 3.0154655207636556e-05,
      "loss": 1.2192,
      "step": 9245
    },
    {
      "epoch": 3.00275,
      "grad_norm": 7.309605598449707,
      "learning_rate": 3.0134429181650552e-05,
      "loss": 1.2626,
      "step": 9250
    },
    {
      "epoch": 3.003,
      "grad_norm": 6.470257759094238,
      "learning_rate": 3.011419964633062e-05,
      "loss": 1.2518,
      "step": 9255
    },
    {
      "epoch": 3.00325,
      "grad_norm": 5.4546098709106445,
      "learning_rate": 3.0093966615503444e-05,
      "loss": 1.2584,
      "step": 9260
    },
    {
      "epoch": 3.0035,
      "grad_norm": 4.675332546234131,
      "learning_rate": 3.007373010299813e-05,
      "loss": 1.1122,
      "step": 9265
    },
    {
      "epoch": 3.00375,
      "grad_norm": 4.352956295013428,
      "learning_rate": 3.0053490122646128e-05,
      "loss": 1.0044,
      "step": 9270
    },
    {
      "epoch": 3.004,
      "grad_norm": 8.945550918579102,
      "learning_rate": 3.003324668828128e-05,
      "loss": 1.1447,
      "step": 9275
    },
    {
      "epoch": 3.00425,
      "grad_norm": 9.211960792541504,
      "learning_rate": 3.0012999813739785e-05,
      "loss": 1.2722,
      "step": 9280
    },
    {
      "epoch": 3.0045,
      "grad_norm": 6.532122611999512,
      "learning_rate": 2.9992749512860173e-05,
      "loss": 1.1927,
      "step": 9285
    },
    {
      "epoch": 3.00475,
      "grad_norm": 7.786715984344482,
      "learning_rate": 2.9972495799483365e-05,
      "loss": 1.3037,
      "step": 9290
    },
    {
      "epoch": 3.005,
      "grad_norm": 5.903716564178467,
      "learning_rate": 2.9952238687452555e-05,
      "loss": 1.3312,
      "step": 9295
    },
    {
      "epoch": 3.00525,
      "grad_norm": 5.8744425773620605,
      "learning_rate": 2.9931978190613303e-05,
      "loss": 1.2597,
      "step": 9300
    },
    {
      "epoch": 3.0055,
      "grad_norm": 6.473535060882568,
      "learning_rate": 2.9911714322813467e-05,
      "loss": 1.4058,
      "step": 9305
    },
    {
      "epoch": 3.00575,
      "grad_norm": 6.093421936035156,
      "learning_rate": 2.9891447097903197e-05,
      "loss": 1.2542,
      "step": 9310
    },
    {
      "epoch": 3.006,
      "grad_norm": 6.486798286437988,
      "learning_rate": 2.9871176529734968e-05,
      "loss": 1.3521,
      "step": 9315
    },
    {
      "epoch": 3.00625,
      "grad_norm": 5.99893856048584,
      "learning_rate": 2.9850902632163502e-05,
      "loss": 1.2716,
      "step": 9320
    },
    {
      "epoch": 3.0065,
      "grad_norm": 6.456104755401611,
      "learning_rate": 2.9830625419045837e-05,
      "loss": 1.3692,
      "step": 9325
    },
    {
      "epoch": 3.00675,
      "grad_norm": 5.056381702423096,
      "learning_rate": 2.981034490424125e-05,
      "loss": 1.1839,
      "step": 9330
    },
    {
      "epoch": 3.007,
      "grad_norm": 6.475823879241943,
      "learning_rate": 2.9790061101611276e-05,
      "loss": 1.1444,
      "step": 9335
    },
    {
      "epoch": 3.00725,
      "grad_norm": 5.695411205291748,
      "learning_rate": 2.976977402501971e-05,
      "loss": 1.2434,
      "step": 9340
    },
    {
      "epoch": 3.0075,
      "grad_norm": 5.5996551513671875,
      "learning_rate": 2.9749483688332574e-05,
      "loss": 1.3112,
      "step": 9345
    },
    {
      "epoch": 3.00775,
      "grad_norm": 6.560408115386963,
      "learning_rate": 2.9729190105418115e-05,
      "loss": 1.2243,
      "step": 9350
    },
    {
      "epoch": 3.008,
      "grad_norm": 5.523835182189941,
      "learning_rate": 2.9708893290146823e-05,
      "loss": 1.215,
      "step": 9355
    },
    {
      "epoch": 3.00825,
      "grad_norm": 6.058961868286133,
      "learning_rate": 2.9688593256391357e-05,
      "loss": 1.2413,
      "step": 9360
    },
    {
      "epoch": 3.0085,
      "grad_norm": 7.617275714874268,
      "learning_rate": 2.9668290018026617e-05,
      "loss": 1.1436,
      "step": 9365
    },
    {
      "epoch": 3.00875,
      "grad_norm": 6.304651260375977,
      "learning_rate": 2.9647983588929674e-05,
      "loss": 1.1673,
      "step": 9370
    },
    {
      "epoch": 3.009,
      "grad_norm": 7.073794841766357,
      "learning_rate": 2.9627673982979764e-05,
      "loss": 1.2645,
      "step": 9375
    },
    {
      "epoch": 3.00925,
      "grad_norm": 5.542210102081299,
      "learning_rate": 2.960736121405834e-05,
      "loss": 1.1421,
      "step": 9380
    },
    {
      "epoch": 3.0095,
      "grad_norm": 7.508455753326416,
      "learning_rate": 2.9587045296048955e-05,
      "loss": 1.0796,
      "step": 9385
    },
    {
      "epoch": 3.00975,
      "grad_norm": 8.708338737487793,
      "learning_rate": 2.9566726242837374e-05,
      "loss": 1.2972,
      "step": 9390
    },
    {
      "epoch": 3.01,
      "grad_norm": 5.342316150665283,
      "learning_rate": 2.9546404068311462e-05,
      "loss": 1.1565,
      "step": 9395
    },
    {
      "epoch": 3.01025,
      "grad_norm": 6.468432426452637,
      "learning_rate": 2.952607878636124e-05,
      "loss": 1.2096,
      "step": 9400
    },
    {
      "epoch": 3.0105,
      "grad_norm": 5.116028308868408,
      "learning_rate": 2.9505750410878845e-05,
      "loss": 1.2489,
      "step": 9405
    },
    {
      "epoch": 3.01075,
      "grad_norm": 8.154041290283203,
      "learning_rate": 2.948541895575853e-05,
      "loss": 1.1085,
      "step": 9410
    },
    {
      "epoch": 3.011,
      "grad_norm": 6.195204734802246,
      "learning_rate": 2.9465084434896663e-05,
      "loss": 1.1075,
      "step": 9415
    },
    {
      "epoch": 3.01125,
      "grad_norm": 5.341363906860352,
      "learning_rate": 2.944474686219168e-05,
      "loss": 1.1343,
      "step": 9420
    },
    {
      "epoch": 3.0115,
      "grad_norm": 5.717336177825928,
      "learning_rate": 2.9424406251544122e-05,
      "loss": 1.1299,
      "step": 9425
    },
    {
      "epoch": 3.01175,
      "grad_norm": 6.340883731842041,
      "learning_rate": 2.9404062616856627e-05,
      "loss": 1.2219,
      "step": 9430
    },
    {
      "epoch": 3.012,
      "grad_norm": 5.897980690002441,
      "learning_rate": 2.9383715972033844e-05,
      "loss": 1.2038,
      "step": 9435
    },
    {
      "epoch": 3.01225,
      "grad_norm": 5.2830634117126465,
      "learning_rate": 2.9363366330982543e-05,
      "loss": 1.3516,
      "step": 9440
    },
    {
      "epoch": 3.0125,
      "grad_norm": 5.6786322593688965,
      "learning_rate": 2.9343013707611493e-05,
      "loss": 1.16,
      "step": 9445
    },
    {
      "epoch": 3.01275,
      "grad_norm": 5.498394966125488,
      "learning_rate": 2.9322658115831527e-05,
      "loss": 1.0426,
      "step": 9450
    },
    {
      "epoch": 3.013,
      "grad_norm": 5.831837177276611,
      "learning_rate": 2.9302299569555504e-05,
      "loss": 1.1332,
      "step": 9455
    },
    {
      "epoch": 3.01325,
      "grad_norm": 5.77179479598999,
      "learning_rate": 2.928193808269828e-05,
      "loss": 1.167,
      "step": 9460
    },
    {
      "epoch": 3.0135,
      "grad_norm": 5.968153953552246,
      "learning_rate": 2.926157366917677e-05,
      "loss": 1.0227,
      "step": 9465
    },
    {
      "epoch": 3.01375,
      "grad_norm": 6.326714038848877,
      "learning_rate": 2.9241206342909834e-05,
      "loss": 1.031,
      "step": 9470
    },
    {
      "epoch": 3.014,
      "grad_norm": 7.1772966384887695,
      "learning_rate": 2.9220836117818344e-05,
      "loss": 1.0632,
      "step": 9475
    },
    {
      "epoch": 3.01425,
      "grad_norm": 4.7726616859436035,
      "learning_rate": 2.9200463007825187e-05,
      "loss": 0.9301,
      "step": 9480
    },
    {
      "epoch": 3.0145,
      "grad_norm": 4.143409252166748,
      "learning_rate": 2.918008702685516e-05,
      "loss": 0.7755,
      "step": 9485
    },
    {
      "epoch": 3.01475,
      "grad_norm": 5.381266117095947,
      "learning_rate": 2.9159708188835072e-05,
      "loss": 0.7904,
      "step": 9490
    },
    {
      "epoch": 3.015,
      "grad_norm": 4.080560207366943,
      "learning_rate": 2.9139326507693654e-05,
      "loss": 0.7157,
      "step": 9495
    },
    {
      "epoch": 3.01525,
      "grad_norm": 4.625173568725586,
      "learning_rate": 2.91189419973616e-05,
      "loss": 0.624,
      "step": 9500
    },
    {
      "epoch": 3.01525,
      "eval_loss": 2.1259706020355225,
      "eval_runtime": 5.0428,
      "eval_samples_per_second": 203.06,
      "eval_steps_per_second": 25.383,
      "step": 9500
    },
    {
      "epoch": 3.0155,
      "grad_norm": 4.347296237945557,
      "learning_rate": 2.9098554671771534e-05,
      "loss": 0.5653,
      "step": 9505
    },
    {
      "epoch": 3.01575,
      "grad_norm": 2.4452250003814697,
      "learning_rate": 2.9078164544857996e-05,
      "loss": 0.46,
      "step": 9510
    },
    {
      "epoch": 3.016,
      "grad_norm": 2.307112455368042,
      "learning_rate": 2.9057771630557444e-05,
      "loss": 0.458,
      "step": 9515
    },
    {
      "epoch": 3.01625,
      "grad_norm": 3.591906785964966,
      "learning_rate": 2.903737594280825e-05,
      "loss": 0.5046,
      "step": 9520
    },
    {
      "epoch": 3.0165,
      "grad_norm": 11.049358367919922,
      "learning_rate": 2.9016977495550657e-05,
      "loss": 0.6716,
      "step": 9525
    },
    {
      "epoch": 3.01675,
      "grad_norm": 9.499156951904297,
      "learning_rate": 2.8996576302726835e-05,
      "loss": 1.3206,
      "step": 9530
    },
    {
      "epoch": 3.017,
      "grad_norm": 9.580826759338379,
      "learning_rate": 2.8976172378280797e-05,
      "loss": 1.3576,
      "step": 9535
    },
    {
      "epoch": 3.01725,
      "grad_norm": 6.921323299407959,
      "learning_rate": 2.895576573615843e-05,
      "loss": 1.3835,
      "step": 9540
    },
    {
      "epoch": 3.0175,
      "grad_norm": 7.3420209884643555,
      "learning_rate": 2.8935356390307477e-05,
      "loss": 1.2706,
      "step": 9545
    },
    {
      "epoch": 3.01775,
      "grad_norm": 6.382271766662598,
      "learning_rate": 2.8914944354677544e-05,
      "loss": 1.1904,
      "step": 9550
    },
    {
      "epoch": 3.018,
      "grad_norm": 5.624538421630859,
      "learning_rate": 2.8894529643220064e-05,
      "loss": 1.2164,
      "step": 9555
    },
    {
      "epoch": 3.01825,
      "grad_norm": 6.900250434875488,
      "learning_rate": 2.8874112269888287e-05,
      "loss": 1.3105,
      "step": 9560
    },
    {
      "epoch": 3.0185,
      "grad_norm": 5.3919267654418945,
      "learning_rate": 2.8853692248637315e-05,
      "loss": 1.291,
      "step": 9565
    },
    {
      "epoch": 3.01875,
      "grad_norm": 6.29521369934082,
      "learning_rate": 2.8833269593424018e-05,
      "loss": 1.151,
      "step": 9570
    },
    {
      "epoch": 3.019,
      "grad_norm": 5.960546493530273,
      "learning_rate": 2.88128443182071e-05,
      "loss": 1.2206,
      "step": 9575
    },
    {
      "epoch": 3.01925,
      "grad_norm": 5.217897415161133,
      "learning_rate": 2.8792416436947055e-05,
      "loss": 1.1921,
      "step": 9580
    },
    {
      "epoch": 3.0195,
      "grad_norm": 6.279829502105713,
      "learning_rate": 2.8771985963606128e-05,
      "loss": 1.2294,
      "step": 9585
    },
    {
      "epoch": 3.01975,
      "grad_norm": 6.380166530609131,
      "learning_rate": 2.8751552912148368e-05,
      "loss": 1.0377,
      "step": 9590
    },
    {
      "epoch": 3.02,
      "grad_norm": 6.100897789001465,
      "learning_rate": 2.8731117296539557e-05,
      "loss": 1.1668,
      "step": 9595
    },
    {
      "epoch": 3.02025,
      "grad_norm": 7.806006908416748,
      "learning_rate": 2.8710679130747266e-05,
      "loss": 1.37,
      "step": 9600
    },
    {
      "epoch": 3.0205,
      "grad_norm": 5.487634181976318,
      "learning_rate": 2.8690238428740772e-05,
      "loss": 1.2788,
      "step": 9605
    },
    {
      "epoch": 3.02075,
      "grad_norm": 7.220844745635986,
      "learning_rate": 2.8669795204491112e-05,
      "loss": 1.1113,
      "step": 9610
    },
    {
      "epoch": 3.021,
      "grad_norm": 7.90500020980835,
      "learning_rate": 2.8649349471971025e-05,
      "loss": 1.292,
      "step": 9615
    },
    {
      "epoch": 3.02125,
      "grad_norm": 5.2886505126953125,
      "learning_rate": 2.8628901245155e-05,
      "loss": 1.1503,
      "step": 9620
    },
    {
      "epoch": 3.0215,
      "grad_norm": 6.589010238647461,
      "learning_rate": 2.860845053801919e-05,
      "loss": 1.1421,
      "step": 9625
    },
    {
      "epoch": 3.02175,
      "grad_norm": 8.839849472045898,
      "learning_rate": 2.858799736454146e-05,
      "loss": 1.2106,
      "step": 9630
    },
    {
      "epoch": 3.022,
      "grad_norm": 6.56943941116333,
      "learning_rate": 2.856754173870136e-05,
      "loss": 1.221,
      "step": 9635
    },
    {
      "epoch": 3.02225,
      "grad_norm": 8.538193702697754,
      "learning_rate": 2.8547083674480118e-05,
      "loss": 1.3624,
      "step": 9640
    },
    {
      "epoch": 3.0225,
      "grad_norm": 5.659195423126221,
      "learning_rate": 2.852662318586064e-05,
      "loss": 1.3719,
      "step": 9645
    },
    {
      "epoch": 3.02275,
      "grad_norm": 6.176497936248779,
      "learning_rate": 2.8506160286827472e-05,
      "loss": 1.225,
      "step": 9650
    },
    {
      "epoch": 3.023,
      "grad_norm": 6.527254104614258,
      "learning_rate": 2.8485694991366807e-05,
      "loss": 1.2302,
      "step": 9655
    },
    {
      "epoch": 3.02325,
      "grad_norm": 5.538982391357422,
      "learning_rate": 2.8465227313466474e-05,
      "loss": 1.0615,
      "step": 9660
    },
    {
      "epoch": 3.0235,
      "grad_norm": 6.873575687408447,
      "learning_rate": 2.844475726711595e-05,
      "loss": 1.2294,
      "step": 9665
    },
    {
      "epoch": 3.02375,
      "grad_norm": 5.177328109741211,
      "learning_rate": 2.842428486630633e-05,
      "loss": 1.0799,
      "step": 9670
    },
    {
      "epoch": 3.024,
      "grad_norm": 6.598661422729492,
      "learning_rate": 2.8403810125030277e-05,
      "loss": 1.1997,
      "step": 9675
    },
    {
      "epoch": 3.02425,
      "grad_norm": 5.956587791442871,
      "learning_rate": 2.8383333057282107e-05,
      "loss": 1.0552,
      "step": 9680
    },
    {
      "epoch": 3.0245,
      "grad_norm": 6.9054036140441895,
      "learning_rate": 2.8362853677057688e-05,
      "loss": 1.3705,
      "step": 9685
    },
    {
      "epoch": 3.02475,
      "grad_norm": 6.267490386962891,
      "learning_rate": 2.8342371998354494e-05,
      "loss": 1.1432,
      "step": 9690
    },
    {
      "epoch": 3.025,
      "grad_norm": 8.078243255615234,
      "learning_rate": 2.8321888035171567e-05,
      "loss": 1.2243,
      "step": 9695
    },
    {
      "epoch": 3.02525,
      "grad_norm": 4.083207130432129,
      "learning_rate": 2.830140180150948e-05,
      "loss": 1.1814,
      "step": 9700
    },
    {
      "epoch": 3.0255,
      "grad_norm": 7.32029914855957,
      "learning_rate": 2.8280913311370393e-05,
      "loss": 1.3181,
      "step": 9705
    },
    {
      "epoch": 3.02575,
      "grad_norm": 6.603347301483154,
      "learning_rate": 2.8260422578757994e-05,
      "loss": 1.2538,
      "step": 9710
    },
    {
      "epoch": 3.026,
      "grad_norm": 6.527811050415039,
      "learning_rate": 2.8239929617677506e-05,
      "loss": 1.116,
      "step": 9715
    },
    {
      "epoch": 3.02625,
      "grad_norm": 7.368244647979736,
      "learning_rate": 2.8219434442135683e-05,
      "loss": 1.3624,
      "step": 9720
    },
    {
      "epoch": 3.0265,
      "grad_norm": 5.863884449005127,
      "learning_rate": 2.8198937066140767e-05,
      "loss": 1.0784,
      "step": 9725
    },
    {
      "epoch": 3.02675,
      "grad_norm": 6.901076793670654,
      "learning_rate": 2.8178437503702537e-05,
      "loss": 1.2243,
      "step": 9730
    },
    {
      "epoch": 3.027,
      "grad_norm": 7.373124599456787,
      "learning_rate": 2.815793576883224e-05,
      "loss": 1.1507,
      "step": 9735
    },
    {
      "epoch": 3.02725,
      "grad_norm": 6.318828582763672,
      "learning_rate": 2.8137431875542624e-05,
      "loss": 1.1697,
      "step": 9740
    },
    {
      "epoch": 3.0275,
      "grad_norm": 6.183470249176025,
      "learning_rate": 2.811692583784791e-05,
      "loss": 1.0693,
      "step": 9745
    },
    {
      "epoch": 3.02775,
      "grad_norm": 5.925792694091797,
      "learning_rate": 2.809641766976377e-05,
      "loss": 1.2084,
      "step": 9750
    },
    {
      "epoch": 3.028,
      "grad_norm": 8.52265739440918,
      "learning_rate": 2.8075907385307355e-05,
      "loss": 1.1988,
      "step": 9755
    },
    {
      "epoch": 3.02825,
      "grad_norm": 7.056344032287598,
      "learning_rate": 2.8055394998497237e-05,
      "loss": 1.2733,
      "step": 9760
    },
    {
      "epoch": 3.0285,
      "grad_norm": 6.213364124298096,
      "learning_rate": 2.803488052335346e-05,
      "loss": 1.1195,
      "step": 9765
    },
    {
      "epoch": 3.02875,
      "grad_norm": 8.536331176757812,
      "learning_rate": 2.8014363973897455e-05,
      "loss": 1.2421,
      "step": 9770
    },
    {
      "epoch": 3.029,
      "grad_norm": 6.444204330444336,
      "learning_rate": 2.7993845364152098e-05,
      "loss": 1.2061,
      "step": 9775
    },
    {
      "epoch": 3.02925,
      "grad_norm": 5.668451309204102,
      "learning_rate": 2.797332470814167e-05,
      "loss": 1.1766,
      "step": 9780
    },
    {
      "epoch": 3.0295,
      "grad_norm": 3.7784571647644043,
      "learning_rate": 2.7952802019891833e-05,
      "loss": 1.0403,
      "step": 9785
    },
    {
      "epoch": 3.02975,
      "grad_norm": 7.457467079162598,
      "learning_rate": 2.7932277313429662e-05,
      "loss": 1.1621,
      "step": 9790
    },
    {
      "epoch": 3.03,
      "grad_norm": 4.98514986038208,
      "learning_rate": 2.7911750602783603e-05,
      "loss": 1.1055,
      "step": 9795
    },
    {
      "epoch": 3.03025,
      "grad_norm": 8.039066314697266,
      "learning_rate": 2.7891221901983457e-05,
      "loss": 1.179,
      "step": 9800
    },
    {
      "epoch": 3.0305,
      "grad_norm": 5.472352504730225,
      "learning_rate": 2.7870691225060404e-05,
      "loss": 1.1899,
      "step": 9805
    },
    {
      "epoch": 3.03075,
      "grad_norm": 6.506392002105713,
      "learning_rate": 2.785015858604697e-05,
      "loss": 1.1987,
      "step": 9810
    },
    {
      "epoch": 3.031,
      "grad_norm": 4.551123142242432,
      "learning_rate": 2.7829623998977012e-05,
      "loss": 1.1061,
      "step": 9815
    },
    {
      "epoch": 3.03125,
      "grad_norm": 5.922723770141602,
      "learning_rate": 2.7809087477885735e-05,
      "loss": 1.2604,
      "step": 9820
    },
    {
      "epoch": 3.0315,
      "grad_norm": 6.691093921661377,
      "learning_rate": 2.7788549036809657e-05,
      "loss": 1.1425,
      "step": 9825
    },
    {
      "epoch": 3.03175,
      "grad_norm": 4.933826923370361,
      "learning_rate": 2.7768008689786608e-05,
      "loss": 1.1519,
      "step": 9830
    },
    {
      "epoch": 3.032,
      "grad_norm": 6.301338195800781,
      "learning_rate": 2.7747466450855718e-05,
      "loss": 1.0421,
      "step": 9835
    },
    {
      "epoch": 3.03225,
      "grad_norm": 7.433212757110596,
      "learning_rate": 2.7726922334057415e-05,
      "loss": 1.0646,
      "step": 9840
    },
    {
      "epoch": 3.0325,
      "grad_norm": 6.594202518463135,
      "learning_rate": 2.7706376353433415e-05,
      "loss": 1.0543,
      "step": 9845
    },
    {
      "epoch": 3.03275,
      "grad_norm": 6.191073894500732,
      "learning_rate": 2.7685828523026692e-05,
      "loss": 0.9952,
      "step": 9850
    },
    {
      "epoch": 3.033,
      "grad_norm": 6.668955326080322,
      "learning_rate": 2.76652788568815e-05,
      "loss": 1.1482,
      "step": 9855
    },
    {
      "epoch": 3.03325,
      "grad_norm": 5.688051700592041,
      "learning_rate": 2.7644727369043345e-05,
      "loss": 1.1558,
      "step": 9860
    },
    {
      "epoch": 3.0335,
      "grad_norm": 5.834364414215088,
      "learning_rate": 2.7624174073558968e-05,
      "loss": 1.148,
      "step": 9865
    },
    {
      "epoch": 3.03375,
      "grad_norm": 7.147920608520508,
      "learning_rate": 2.760361898447636e-05,
      "loss": 1.152,
      "step": 9870
    },
    {
      "epoch": 3.034,
      "grad_norm": 5.89534330368042,
      "learning_rate": 2.7583062115844715e-05,
      "loss": 1.334,
      "step": 9875
    },
    {
      "epoch": 3.03425,
      "grad_norm": 6.17475700378418,
      "learning_rate": 2.7562503481714485e-05,
      "loss": 1.0505,
      "step": 9880
    },
    {
      "epoch": 3.0345,
      "grad_norm": 6.121830463409424,
      "learning_rate": 2.7541943096137286e-05,
      "loss": 1.1938,
      "step": 9885
    },
    {
      "epoch": 3.03475,
      "grad_norm": 4.767579078674316,
      "learning_rate": 2.752138097316594e-05,
      "loss": 1.2303,
      "step": 9890
    },
    {
      "epoch": 3.035,
      "grad_norm": 5.508645534515381,
      "learning_rate": 2.7500817126854482e-05,
      "loss": 1.3564,
      "step": 9895
    },
    {
      "epoch": 3.03525,
      "grad_norm": 5.200982093811035,
      "learning_rate": 2.74802515712581e-05,
      "loss": 1.2742,
      "step": 9900
    },
    {
      "epoch": 3.0355,
      "grad_norm": 5.108534336090088,
      "learning_rate": 2.7459684320433154e-05,
      "loss": 1.2013,
      "step": 9905
    },
    {
      "epoch": 3.03575,
      "grad_norm": 5.307029724121094,
      "learning_rate": 2.743911538843717e-05,
      "loss": 1.1934,
      "step": 9910
    },
    {
      "epoch": 3.036,
      "grad_norm": 7.057661533355713,
      "learning_rate": 2.7418544789328816e-05,
      "loss": 1.4229,
      "step": 9915
    },
    {
      "epoch": 3.03625,
      "grad_norm": 5.793703556060791,
      "learning_rate": 2.7397972537167903e-05,
      "loss": 1.1858,
      "step": 9920
    },
    {
      "epoch": 3.0365,
      "grad_norm": 5.723255157470703,
      "learning_rate": 2.7377398646015367e-05,
      "loss": 1.2741,
      "step": 9925
    },
    {
      "epoch": 3.03675,
      "grad_norm": 5.543675899505615,
      "learning_rate": 2.7356823129933284e-05,
      "loss": 1.2502,
      "step": 9930
    },
    {
      "epoch": 3.037,
      "grad_norm": 4.663748741149902,
      "learning_rate": 2.7336246002984815e-05,
      "loss": 1.3072,
      "step": 9935
    },
    {
      "epoch": 3.03725,
      "grad_norm": 5.477057456970215,
      "learning_rate": 2.7315667279234226e-05,
      "loss": 1.2785,
      "step": 9940
    },
    {
      "epoch": 3.0375,
      "grad_norm": 6.061802864074707,
      "learning_rate": 2.729508697274689e-05,
      "loss": 1.4375,
      "step": 9945
    },
    {
      "epoch": 3.03775,
      "grad_norm": 4.912423133850098,
      "learning_rate": 2.7274505097589253e-05,
      "loss": 1.2338,
      "step": 9950
    },
    {
      "epoch": 3.038,
      "grad_norm": 4.81335973739624,
      "learning_rate": 2.7253921667828823e-05,
      "loss": 1.2501,
      "step": 9955
    },
    {
      "epoch": 3.03825,
      "grad_norm": 7.019362926483154,
      "learning_rate": 2.723333669753419e-05,
      "loss": 1.2665,
      "step": 9960
    },
    {
      "epoch": 3.0385,
      "grad_norm": 5.622282028198242,
      "learning_rate": 2.721275020077498e-05,
      "loss": 1.2639,
      "step": 9965
    },
    {
      "epoch": 3.03875,
      "grad_norm": 6.509200572967529,
      "learning_rate": 2.7192162191621878e-05,
      "loss": 1.3683,
      "step": 9970
    },
    {
      "epoch": 3.039,
      "grad_norm": 5.036050319671631,
      "learning_rate": 2.7171572684146583e-05,
      "loss": 1.2346,
      "step": 9975
    },
    {
      "epoch": 3.03925,
      "grad_norm": 5.4685773849487305,
      "learning_rate": 2.715098169242184e-05,
      "loss": 1.2337,
      "step": 9980
    },
    {
      "epoch": 3.0395,
      "grad_norm": 6.400442600250244,
      "learning_rate": 2.7130389230521392e-05,
      "loss": 1.2397,
      "step": 9985
    },
    {
      "epoch": 3.03975,
      "grad_norm": 4.7060160636901855,
      "learning_rate": 2.7109795312519988e-05,
      "loss": 1.1394,
      "step": 9990
    },
    {
      "epoch": 3.04,
      "grad_norm": 6.118445873260498,
      "learning_rate": 2.708919995249339e-05,
      "loss": 1.1907,
      "step": 9995
    },
    {
      "epoch": 3.04025,
      "grad_norm": 4.85801362991333,
      "learning_rate": 2.706860316451832e-05,
      "loss": 1.2389,
      "step": 10000
    },
    {
      "epoch": 3.04025,
      "eval_loss": 1.8895844221115112,
      "eval_runtime": 5.243,
      "eval_samples_per_second": 195.306,
      "eval_steps_per_second": 24.413,
      "step": 10000
    },
    {
      "epoch": 3.0405,
      "grad_norm": 6.00018310546875,
      "learning_rate": 2.704800496267249e-05,
      "loss": 1.1439,
      "step": 10005
    },
    {
      "epoch": 3.04075,
      "grad_norm": 4.422332286834717,
      "learning_rate": 2.7027405361034585e-05,
      "loss": 1.1115,
      "step": 10010
    },
    {
      "epoch": 3.041,
      "grad_norm": 6.107067108154297,
      "learning_rate": 2.700680437368423e-05,
      "loss": 1.3398,
      "step": 10015
    },
    {
      "epoch": 3.04125,
      "grad_norm": 5.841360092163086,
      "learning_rate": 2.698620201470201e-05,
      "loss": 1.1948,
      "step": 10020
    },
    {
      "epoch": 3.0415,
      "grad_norm": 5.537746429443359,
      "learning_rate": 2.696559829816944e-05,
      "loss": 1.2704,
      "step": 10025
    },
    {
      "epoch": 3.04175,
      "grad_norm": 5.0027570724487305,
      "learning_rate": 2.694499323816897e-05,
      "loss": 1.3129,
      "step": 10030
    },
    {
      "epoch": 3.042,
      "grad_norm": 7.080565452575684,
      "learning_rate": 2.6924386848783962e-05,
      "loss": 1.256,
      "step": 10035
    },
    {
      "epoch": 3.04225,
      "grad_norm": 6.736631870269775,
      "learning_rate": 2.6903779144098686e-05,
      "loss": 1.2333,
      "step": 10040
    },
    {
      "epoch": 3.0425,
      "grad_norm": 5.700403690338135,
      "learning_rate": 2.6883170138198323e-05,
      "loss": 1.2544,
      "step": 10045
    },
    {
      "epoch": 3.04275,
      "grad_norm": 6.112886428833008,
      "learning_rate": 2.6862559845168923e-05,
      "loss": 1.2577,
      "step": 10050
    },
    {
      "epoch": 3.043,
      "grad_norm": 7.830439567565918,
      "learning_rate": 2.684194827909743e-05,
      "loss": 1.2322,
      "step": 10055
    },
    {
      "epoch": 3.04325,
      "grad_norm": 5.4724040031433105,
      "learning_rate": 2.6821335454071662e-05,
      "loss": 1.3812,
      "step": 10060
    },
    {
      "epoch": 3.0435,
      "grad_norm": 7.612553596496582,
      "learning_rate": 2.6800721384180277e-05,
      "loss": 1.393,
      "step": 10065
    },
    {
      "epoch": 3.04375,
      "grad_norm": 5.332843780517578,
      "learning_rate": 2.678010608351281e-05,
      "loss": 1.1343,
      "step": 10070
    },
    {
      "epoch": 3.044,
      "grad_norm": 5.567480087280273,
      "learning_rate": 2.6759489566159624e-05,
      "loss": 1.194,
      "step": 10075
    },
    {
      "epoch": 3.04425,
      "grad_norm": 4.286108016967773,
      "learning_rate": 2.6738871846211904e-05,
      "loss": 1.1568,
      "step": 10080
    },
    {
      "epoch": 3.0445,
      "grad_norm": 5.124247074127197,
      "learning_rate": 2.671825293776168e-05,
      "loss": 1.1501,
      "step": 10085
    },
    {
      "epoch": 3.04475,
      "grad_norm": 6.057650089263916,
      "learning_rate": 2.6697632854901772e-05,
      "loss": 1.255,
      "step": 10090
    },
    {
      "epoch": 3.045,
      "grad_norm": 5.502997875213623,
      "learning_rate": 2.667701161172581e-05,
      "loss": 1.1909,
      "step": 10095
    },
    {
      "epoch": 3.04525,
      "grad_norm": 5.659416675567627,
      "learning_rate": 2.6656389222328237e-05,
      "loss": 1.2192,
      "step": 10100
    },
    {
      "epoch": 3.0455,
      "grad_norm": 6.351268768310547,
      "learning_rate": 2.663576570080424e-05,
      "loss": 1.1901,
      "step": 10105
    },
    {
      "epoch": 3.04575,
      "grad_norm": 4.940362453460693,
      "learning_rate": 2.6615141061249814e-05,
      "loss": 1.1941,
      "step": 10110
    },
    {
      "epoch": 3.046,
      "grad_norm": 5.317033290863037,
      "learning_rate": 2.6594515317761705e-05,
      "loss": 1.1797,
      "step": 10115
    },
    {
      "epoch": 3.04625,
      "grad_norm": 5.979988098144531,
      "learning_rate": 2.6573888484437404e-05,
      "loss": 1.3502,
      "step": 10120
    },
    {
      "epoch": 3.0465,
      "grad_norm": 5.158536434173584,
      "learning_rate": 2.6553260575375172e-05,
      "loss": 1.1428,
      "step": 10125
    },
    {
      "epoch": 3.04675,
      "grad_norm": 5.019683837890625,
      "learning_rate": 2.653263160467398e-05,
      "loss": 1.1481,
      "step": 10130
    },
    {
      "epoch": 3.047,
      "grad_norm": 5.859641075134277,
      "learning_rate": 2.651200158643354e-05,
      "loss": 1.185,
      "step": 10135
    },
    {
      "epoch": 3.04725,
      "grad_norm": 5.923574447631836,
      "learning_rate": 2.649137053475427e-05,
      "loss": 1.2382,
      "step": 10140
    },
    {
      "epoch": 3.0475,
      "grad_norm": 5.653359413146973,
      "learning_rate": 2.6470738463737306e-05,
      "loss": 1.1133,
      "step": 10145
    },
    {
      "epoch": 3.04775,
      "grad_norm": 6.488893985748291,
      "learning_rate": 2.6450105387484464e-05,
      "loss": 1.2912,
      "step": 10150
    },
    {
      "epoch": 3.048,
      "grad_norm": 6.033206462860107,
      "learning_rate": 2.6429471320098266e-05,
      "loss": 1.3379,
      "step": 10155
    },
    {
      "epoch": 3.04825,
      "grad_norm": 6.430360794067383,
      "learning_rate": 2.6408836275681908e-05,
      "loss": 1.3111,
      "step": 10160
    },
    {
      "epoch": 3.0485,
      "grad_norm": 4.734593868255615,
      "learning_rate": 2.638820026833923e-05,
      "loss": 1.2612,
      "step": 10165
    },
    {
      "epoch": 3.04875,
      "grad_norm": 4.846219062805176,
      "learning_rate": 2.636756331217476e-05,
      "loss": 1.3008,
      "step": 10170
    },
    {
      "epoch": 3.049,
      "grad_norm": 5.91774845123291,
      "learning_rate": 2.6346925421293668e-05,
      "loss": 1.2864,
      "step": 10175
    },
    {
      "epoch": 3.04925,
      "grad_norm": 5.299856662750244,
      "learning_rate": 2.6326286609801744e-05,
      "loss": 1.2815,
      "step": 10180
    },
    {
      "epoch": 3.0495,
      "grad_norm": 8.734354019165039,
      "learning_rate": 2.6305646891805435e-05,
      "loss": 1.2528,
      "step": 10185
    },
    {
      "epoch": 3.04975,
      "grad_norm": 4.45852518081665,
      "learning_rate": 2.6285006281411785e-05,
      "loss": 1.1488,
      "step": 10190
    },
    {
      "epoch": 3.05,
      "grad_norm": 5.604281425476074,
      "learning_rate": 2.6264364792728446e-05,
      "loss": 1.1629,
      "step": 10195
    },
    {
      "epoch": 3.05025,
      "grad_norm": 5.6410040855407715,
      "learning_rate": 2.624372243986371e-05,
      "loss": 1.2297,
      "step": 10200
    },
    {
      "epoch": 3.0505,
      "grad_norm": 5.790082931518555,
      "learning_rate": 2.6223079236926407e-05,
      "loss": 1.2443,
      "step": 10205
    },
    {
      "epoch": 3.05075,
      "grad_norm": 5.927577495574951,
      "learning_rate": 2.620243519802598e-05,
      "loss": 1.3126,
      "step": 10210
    },
    {
      "epoch": 3.051,
      "grad_norm": 4.254844665527344,
      "learning_rate": 2.6181790337272433e-05,
      "loss": 1.1434,
      "step": 10215
    },
    {
      "epoch": 3.05125,
      "grad_norm": 5.455382823944092,
      "learning_rate": 2.6161144668776334e-05,
      "loss": 1.2528,
      "step": 10220
    },
    {
      "epoch": 3.0515,
      "grad_norm": 4.63544225692749,
      "learning_rate": 2.61404982066488e-05,
      "loss": 1.0667,
      "step": 10225
    },
    {
      "epoch": 3.05175,
      "grad_norm": 5.789538383483887,
      "learning_rate": 2.6119850965001503e-05,
      "loss": 1.2394,
      "step": 10230
    },
    {
      "epoch": 3.052,
      "grad_norm": 4.624111652374268,
      "learning_rate": 2.6099202957946624e-05,
      "loss": 1.1092,
      "step": 10235
    },
    {
      "epoch": 3.05225,
      "grad_norm": 5.402312755584717,
      "learning_rate": 2.6078554199596896e-05,
      "loss": 1.1234,
      "step": 10240
    },
    {
      "epoch": 3.0525,
      "grad_norm": 5.732051372528076,
      "learning_rate": 2.6057904704065533e-05,
      "loss": 1.2011,
      "step": 10245
    },
    {
      "epoch": 3.05275,
      "grad_norm": 6.311025619506836,
      "learning_rate": 2.6037254485466285e-05,
      "loss": 1.1536,
      "step": 10250
    },
    {
      "epoch": 3.053,
      "grad_norm": 5.903662204742432,
      "learning_rate": 2.6016603557913378e-05,
      "loss": 1.1253,
      "step": 10255
    },
    {
      "epoch": 3.05325,
      "grad_norm": 5.297785758972168,
      "learning_rate": 2.5995951935521523e-05,
      "loss": 1.1542,
      "step": 10260
    },
    {
      "epoch": 3.0535,
      "grad_norm": 6.166801452636719,
      "learning_rate": 2.5975299632405914e-05,
      "loss": 1.0501,
      "step": 10265
    },
    {
      "epoch": 3.05375,
      "grad_norm": 4.928371429443359,
      "learning_rate": 2.5954646662682196e-05,
      "loss": 1.0589,
      "step": 10270
    },
    {
      "epoch": 3.054,
      "grad_norm": 5.857297897338867,
      "learning_rate": 2.593399304046649e-05,
      "loss": 1.0469,
      "step": 10275
    },
    {
      "epoch": 3.05425,
      "grad_norm": 6.205385208129883,
      "learning_rate": 2.5913338779875356e-05,
      "loss": 1.0763,
      "step": 10280
    },
    {
      "epoch": 3.0545,
      "grad_norm": 5.9689154624938965,
      "learning_rate": 2.5892683895025767e-05,
      "loss": 1.2766,
      "step": 10285
    },
    {
      "epoch": 3.05475,
      "grad_norm": 4.65309476852417,
      "learning_rate": 2.5872028400035157e-05,
      "loss": 1.249,
      "step": 10290
    },
    {
      "epoch": 3.055,
      "grad_norm": 6.227772235870361,
      "learning_rate": 2.5851372309021354e-05,
      "loss": 1.1357,
      "step": 10295
    },
    {
      "epoch": 3.05525,
      "grad_norm": 4.729949951171875,
      "learning_rate": 2.583071563610262e-05,
      "loss": 1.1595,
      "step": 10300
    },
    {
      "epoch": 3.0555,
      "grad_norm": 5.821236610412598,
      "learning_rate": 2.581005839539757e-05,
      "loss": 1.0957,
      "step": 10305
    },
    {
      "epoch": 3.05575,
      "grad_norm": 5.903336524963379,
      "learning_rate": 2.5789400601025242e-05,
      "loss": 1.1932,
      "step": 10310
    },
    {
      "epoch": 3.056,
      "grad_norm": 5.486414909362793,
      "learning_rate": 2.5768742267105055e-05,
      "loss": 1.2015,
      "step": 10315
    },
    {
      "epoch": 3.05625,
      "grad_norm": 7.356430530548096,
      "learning_rate": 2.5748083407756774e-05,
      "loss": 1.226,
      "step": 10320
    },
    {
      "epoch": 3.0565,
      "grad_norm": 4.337281227111816,
      "learning_rate": 2.5727424037100544e-05,
      "loss": 1.0398,
      "step": 10325
    },
    {
      "epoch": 3.05675,
      "grad_norm": 7.066836357116699,
      "learning_rate": 2.5706764169256835e-05,
      "loss": 1.1802,
      "step": 10330
    },
    {
      "epoch": 3.057,
      "grad_norm": 4.625661373138428,
      "learning_rate": 2.568610381834648e-05,
      "loss": 1.0269,
      "step": 10335
    },
    {
      "epoch": 3.05725,
      "grad_norm": 4.04449462890625,
      "learning_rate": 2.5665442998490642e-05,
      "loss": 1.1423,
      "step": 10340
    },
    {
      "epoch": 3.0575,
      "grad_norm": 6.133667945861816,
      "learning_rate": 2.5644781723810785e-05,
      "loss": 1.1893,
      "step": 10345
    },
    {
      "epoch": 3.05775,
      "grad_norm": 5.514564514160156,
      "learning_rate": 2.5624120008428705e-05,
      "loss": 1.0192,
      "step": 10350
    },
    {
      "epoch": 3.058,
      "grad_norm": 4.902724266052246,
      "learning_rate": 2.5603457866466474e-05,
      "loss": 0.999,
      "step": 10355
    },
    {
      "epoch": 3.05825,
      "grad_norm": 5.3253560066223145,
      "learning_rate": 2.558279531204649e-05,
      "loss": 1.0331,
      "step": 10360
    },
    {
      "epoch": 3.0585,
      "grad_norm": 4.873372554779053,
      "learning_rate": 2.55621323592914e-05,
      "loss": 1.1357,
      "step": 10365
    },
    {
      "epoch": 3.05875,
      "grad_norm": 6.469996452331543,
      "learning_rate": 2.554146902232415e-05,
      "loss": 1.1044,
      "step": 10370
    },
    {
      "epoch": 3.059,
      "grad_norm": 7.162414073944092,
      "learning_rate": 2.5520805315267925e-05,
      "loss": 1.2993,
      "step": 10375
    },
    {
      "epoch": 3.05925,
      "grad_norm": 3.958965301513672,
      "learning_rate": 2.5500141252246173e-05,
      "loss": 1.105,
      "step": 10380
    },
    {
      "epoch": 3.0595,
      "grad_norm": 5.858436107635498,
      "learning_rate": 2.5479476847382596e-05,
      "loss": 1.2433,
      "step": 10385
    },
    {
      "epoch": 3.05975,
      "grad_norm": 6.219203948974609,
      "learning_rate": 2.5458812114801117e-05,
      "loss": 1.1995,
      "step": 10390
    },
    {
      "epoch": 3.06,
      "grad_norm": 6.0213141441345215,
      "learning_rate": 2.5438147068625882e-05,
      "loss": 1.3014,
      "step": 10395
    },
    {
      "epoch": 3.06025,
      "grad_norm": 5.45381498336792,
      "learning_rate": 2.5417481722981262e-05,
      "loss": 1.1614,
      "step": 10400
    },
    {
      "epoch": 3.0605,
      "grad_norm": 6.8955302238464355,
      "learning_rate": 2.539681609199181e-05,
      "loss": 1.2959,
      "step": 10405
    },
    {
      "epoch": 3.06075,
      "grad_norm": 5.668941020965576,
      "learning_rate": 2.5376150189782305e-05,
      "loss": 1.1121,
      "step": 10410
    },
    {
      "epoch": 3.061,
      "grad_norm": 5.147541046142578,
      "learning_rate": 2.53554840304777e-05,
      "loss": 1.1317,
      "step": 10415
    },
    {
      "epoch": 3.06125,
      "grad_norm": 6.637824535369873,
      "learning_rate": 2.5334817628203107e-05,
      "loss": 1.0947,
      "step": 10420
    },
    {
      "epoch": 3.0615,
      "grad_norm": 5.121960163116455,
      "learning_rate": 2.531415099708382e-05,
      "loss": 1.1759,
      "step": 10425
    },
    {
      "epoch": 3.06175,
      "grad_norm": 5.771144866943359,
      "learning_rate": 2.529348415124529e-05,
      "loss": 1.0989,
      "step": 10430
    },
    {
      "epoch": 3.062,
      "grad_norm": 4.836717128753662,
      "learning_rate": 2.5272817104813107e-05,
      "loss": 1.1595,
      "step": 10435
    },
    {
      "epoch": 3.06225,
      "grad_norm": 7.189809322357178,
      "learning_rate": 2.525214987191301e-05,
      "loss": 1.2011,
      "step": 10440
    },
    {
      "epoch": 3.0625,
      "grad_norm": 4.614213466644287,
      "learning_rate": 2.5231482466670852e-05,
      "loss": 1.0196,
      "step": 10445
    },
    {
      "epoch": 3.06275,
      "grad_norm": 6.3956098556518555,
      "learning_rate": 2.5210814903212615e-05,
      "loss": 1.0272,
      "step": 10450
    },
    {
      "epoch": 3.063,
      "grad_norm": 6.221757888793945,
      "learning_rate": 2.519014719566437e-05,
      "loss": 1.1792,
      "step": 10455
    },
    {
      "epoch": 3.06325,
      "grad_norm": 5.847418308258057,
      "learning_rate": 2.516947935815231e-05,
      "loss": 1.2776,
      "step": 10460
    },
    {
      "epoch": 3.0635,
      "grad_norm": 4.62680721282959,
      "learning_rate": 2.514881140480272e-05,
      "loss": 1.0021,
      "step": 10465
    },
    {
      "epoch": 3.06375,
      "grad_norm": 5.341935634613037,
      "learning_rate": 2.512814334974193e-05,
      "loss": 1.1252,
      "step": 10470
    },
    {
      "epoch": 3.064,
      "grad_norm": 6.914158821105957,
      "learning_rate": 2.5107475207096365e-05,
      "loss": 1.0719,
      "step": 10475
    },
    {
      "epoch": 3.06425,
      "grad_norm": 7.503772735595703,
      "learning_rate": 2.508680699099251e-05,
      "loss": 1.1725,
      "step": 10480
    },
    {
      "epoch": 3.0645,
      "grad_norm": 4.483884811401367,
      "learning_rate": 2.5066138715556904e-05,
      "loss": 1.0709,
      "step": 10485
    },
    {
      "epoch": 3.06475,
      "grad_norm": 6.2267560958862305,
      "learning_rate": 2.5045470394916105e-05,
      "loss": 1.1855,
      "step": 10490
    },
    {
      "epoch": 3.065,
      "grad_norm": 6.765442371368408,
      "learning_rate": 2.5024802043196715e-05,
      "loss": 1.2487,
      "step": 10495
    },
    {
      "epoch": 3.06525,
      "grad_norm": 7.179675102233887,
      "learning_rate": 2.5004133674525364e-05,
      "loss": 1.2909,
      "step": 10500
    },
    {
      "epoch": 3.06525,
      "eval_loss": 1.9255540370941162,
      "eval_runtime": 5.8361,
      "eval_samples_per_second": 175.458,
      "eval_steps_per_second": 21.932,
      "step": 10500
    },
    {
      "epoch": 3.0655,
      "grad_norm": 5.945554256439209,
      "learning_rate": 2.4983465303028693e-05,
      "loss": 1.1319,
      "step": 10505
    },
    {
      "epoch": 3.06575,
      "grad_norm": 6.144309043884277,
      "learning_rate": 2.4962796942833322e-05,
      "loss": 1.0836,
      "step": 10510
    },
    {
      "epoch": 3.066,
      "grad_norm": 5.8739848136901855,
      "learning_rate": 2.4942128608065894e-05,
      "loss": 1.1737,
      "step": 10515
    },
    {
      "epoch": 3.06625,
      "grad_norm": 7.642446994781494,
      "learning_rate": 2.492146031285301e-05,
      "loss": 1.3598,
      "step": 10520
    },
    {
      "epoch": 3.0665,
      "grad_norm": 6.544162273406982,
      "learning_rate": 2.490079207132127e-05,
      "loss": 1.1057,
      "step": 10525
    },
    {
      "epoch": 3.06675,
      "grad_norm": 6.542543888092041,
      "learning_rate": 2.488012389759721e-05,
      "loss": 1.1093,
      "step": 10530
    },
    {
      "epoch": 3.067,
      "grad_norm": 4.874777793884277,
      "learning_rate": 2.4859455805807337e-05,
      "loss": 1.0706,
      "step": 10535
    },
    {
      "epoch": 3.06725,
      "grad_norm": 4.734777450561523,
      "learning_rate": 2.4838787810078097e-05,
      "loss": 0.9816,
      "step": 10540
    },
    {
      "epoch": 3.0675,
      "grad_norm": 5.200559139251709,
      "learning_rate": 2.4818119924535873e-05,
      "loss": 0.9814,
      "step": 10545
    },
    {
      "epoch": 3.06775,
      "grad_norm": 5.349554061889648,
      "learning_rate": 2.4797452163306956e-05,
      "loss": 0.9594,
      "step": 10550
    },
    {
      "epoch": 3.068,
      "grad_norm": 4.623469352722168,
      "learning_rate": 2.477678454051758e-05,
      "loss": 1.2297,
      "step": 10555
    },
    {
      "epoch": 3.06825,
      "grad_norm": 4.657070636749268,
      "learning_rate": 2.4756117070293863e-05,
      "loss": 1.103,
      "step": 10560
    },
    {
      "epoch": 3.0685000000000002,
      "grad_norm": 4.626328945159912,
      "learning_rate": 2.4735449766761835e-05,
      "loss": 1.087,
      "step": 10565
    },
    {
      "epoch": 3.06875,
      "grad_norm": 3.981457233428955,
      "learning_rate": 2.4714782644047387e-05,
      "loss": 1.026,
      "step": 10570
    },
    {
      "epoch": 3.069,
      "grad_norm": 3.5647637844085693,
      "learning_rate": 2.469411571627632e-05,
      "loss": 1.0058,
      "step": 10575
    },
    {
      "epoch": 3.06925,
      "grad_norm": 5.287620544433594,
      "learning_rate": 2.467344899757427e-05,
      "loss": 1.0021,
      "step": 10580
    },
    {
      "epoch": 3.0695,
      "grad_norm": 3.7356176376342773,
      "learning_rate": 2.4652782502066755e-05,
      "loss": 0.9845,
      "step": 10585
    },
    {
      "epoch": 3.06975,
      "grad_norm": 3.6626994609832764,
      "learning_rate": 2.4632116243879134e-05,
      "loss": 1.0369,
      "step": 10590
    },
    {
      "epoch": 3.07,
      "grad_norm": 5.0465569496154785,
      "learning_rate": 2.4611450237136584e-05,
      "loss": 1.1052,
      "step": 10595
    },
    {
      "epoch": 3.07025,
      "grad_norm": 4.031811237335205,
      "learning_rate": 2.459078449596413e-05,
      "loss": 0.9577,
      "step": 10600
    },
    {
      "epoch": 3.0705,
      "grad_norm": 3.5179874897003174,
      "learning_rate": 2.457011903448662e-05,
      "loss": 1.0477,
      "step": 10605
    },
    {
      "epoch": 3.07075,
      "grad_norm": 4.4852519035339355,
      "learning_rate": 2.454945386682869e-05,
      "loss": 0.9764,
      "step": 10610
    },
    {
      "epoch": 3.071,
      "grad_norm": 4.4765849113464355,
      "learning_rate": 2.45287890071148e-05,
      "loss": 0.9322,
      "step": 10615
    },
    {
      "epoch": 3.07125,
      "grad_norm": 3.8258097171783447,
      "learning_rate": 2.4508124469469172e-05,
      "loss": 0.9289,
      "step": 10620
    },
    {
      "epoch": 3.0715,
      "grad_norm": 4.726226329803467,
      "learning_rate": 2.448746026801584e-05,
      "loss": 0.9812,
      "step": 10625
    },
    {
      "epoch": 3.07175,
      "grad_norm": 4.902612209320068,
      "learning_rate": 2.4466796416878578e-05,
      "loss": 1.2129,
      "step": 10630
    },
    {
      "epoch": 3.072,
      "grad_norm": 4.493462562561035,
      "learning_rate": 2.444613293018093e-05,
      "loss": 1.0555,
      "step": 10635
    },
    {
      "epoch": 3.07225,
      "grad_norm": 4.730551242828369,
      "learning_rate": 2.4425469822046218e-05,
      "loss": 0.9873,
      "step": 10640
    },
    {
      "epoch": 3.0725,
      "grad_norm": 4.466738224029541,
      "learning_rate": 2.440480710659746e-05,
      "loss": 0.8782,
      "step": 10645
    },
    {
      "epoch": 3.07275,
      "grad_norm": 4.708541393280029,
      "learning_rate": 2.4384144797957427e-05,
      "loss": 1.0385,
      "step": 10650
    },
    {
      "epoch": 3.073,
      "grad_norm": 4.097324371337891,
      "learning_rate": 2.436348291024862e-05,
      "loss": 0.9994,
      "step": 10655
    },
    {
      "epoch": 3.07325,
      "grad_norm": 5.303429126739502,
      "learning_rate": 2.4342821457593237e-05,
      "loss": 0.9133,
      "step": 10660
    },
    {
      "epoch": 3.0735,
      "grad_norm": 4.222739219665527,
      "learning_rate": 2.4322160454113197e-05,
      "loss": 0.8714,
      "step": 10665
    },
    {
      "epoch": 3.07375,
      "grad_norm": 3.9797186851501465,
      "learning_rate": 2.430149991393009e-05,
      "loss": 1.0589,
      "step": 10670
    },
    {
      "epoch": 3.074,
      "grad_norm": 4.72617769241333,
      "learning_rate": 2.428083985116521e-05,
      "loss": 0.948,
      "step": 10675
    },
    {
      "epoch": 3.07425,
      "grad_norm": 4.507969856262207,
      "learning_rate": 2.4260180279939504e-05,
      "loss": 0.9479,
      "step": 10680
    },
    {
      "epoch": 3.0745,
      "grad_norm": 5.372661113739014,
      "learning_rate": 2.4239521214373615e-05,
      "loss": 1.0702,
      "step": 10685
    },
    {
      "epoch": 3.07475,
      "grad_norm": 3.6371281147003174,
      "learning_rate": 2.421886266858779e-05,
      "loss": 0.8413,
      "step": 10690
    },
    {
      "epoch": 3.075,
      "grad_norm": 3.793177366256714,
      "learning_rate": 2.4198204656701973e-05,
      "loss": 0.8539,
      "step": 10695
    },
    {
      "epoch": 3.07525,
      "grad_norm": 5.093835830688477,
      "learning_rate": 2.417754719283571e-05,
      "loss": 0.9422,
      "step": 10700
    },
    {
      "epoch": 3.0755,
      "grad_norm": 4.663920879364014,
      "learning_rate": 2.4156890291108187e-05,
      "loss": 1.0438,
      "step": 10705
    },
    {
      "epoch": 3.07575,
      "grad_norm": 4.724298477172852,
      "learning_rate": 2.4136233965638195e-05,
      "loss": 0.9532,
      "step": 10710
    },
    {
      "epoch": 3.076,
      "grad_norm": 4.977344036102295,
      "learning_rate": 2.4115578230544147e-05,
      "loss": 1.0773,
      "step": 10715
    },
    {
      "epoch": 3.07625,
      "grad_norm": 4.281359672546387,
      "learning_rate": 2.4094923099944035e-05,
      "loss": 1.0179,
      "step": 10720
    },
    {
      "epoch": 3.0765,
      "grad_norm": 4.82738733291626,
      "learning_rate": 2.4074268587955455e-05,
      "loss": 0.9589,
      "step": 10725
    },
    {
      "epoch": 3.07675,
      "grad_norm": 4.375828266143799,
      "learning_rate": 2.4053614708695575e-05,
      "loss": 0.9675,
      "step": 10730
    },
    {
      "epoch": 3.077,
      "grad_norm": 4.5061211585998535,
      "learning_rate": 2.4032961476281102e-05,
      "loss": 0.9015,
      "step": 10735
    },
    {
      "epoch": 3.07725,
      "grad_norm": 4.220491409301758,
      "learning_rate": 2.4012308904828342e-05,
      "loss": 0.9697,
      "step": 10740
    },
    {
      "epoch": 3.0775,
      "grad_norm": 5.171847343444824,
      "learning_rate": 2.399165700845314e-05,
      "loss": 1.0357,
      "step": 10745
    },
    {
      "epoch": 3.07775,
      "grad_norm": 3.845505714416504,
      "learning_rate": 2.3971005801270856e-05,
      "loss": 0.9482,
      "step": 10750
    },
    {
      "epoch": 3.078,
      "grad_norm": 4.233018398284912,
      "learning_rate": 2.395035529739641e-05,
      "loss": 1.0115,
      "step": 10755
    },
    {
      "epoch": 3.07825,
      "grad_norm": 3.912193775177002,
      "learning_rate": 2.392970551094422e-05,
      "loss": 0.9438,
      "step": 10760
    },
    {
      "epoch": 3.0785,
      "grad_norm": 5.069922924041748,
      "learning_rate": 2.390905645602822e-05,
      "loss": 0.869,
      "step": 10765
    },
    {
      "epoch": 3.07875,
      "grad_norm": 4.4896368980407715,
      "learning_rate": 2.3888408146761843e-05,
      "loss": 0.9047,
      "step": 10770
    },
    {
      "epoch": 3.079,
      "grad_norm": 5.950148582458496,
      "learning_rate": 2.3867760597258014e-05,
      "loss": 0.9118,
      "step": 10775
    },
    {
      "epoch": 3.07925,
      "grad_norm": 5.364752292633057,
      "learning_rate": 2.384711382162916e-05,
      "loss": 1.0266,
      "step": 10780
    },
    {
      "epoch": 3.0795,
      "grad_norm": 4.708291530609131,
      "learning_rate": 2.382646783398712e-05,
      "loss": 0.9082,
      "step": 10785
    },
    {
      "epoch": 3.07975,
      "grad_norm": 6.07080602645874,
      "learning_rate": 2.380582264844325e-05,
      "loss": 0.9744,
      "step": 10790
    },
    {
      "epoch": 3.08,
      "grad_norm": 3.4936447143554688,
      "learning_rate": 2.3785178279108344e-05,
      "loss": 0.9733,
      "step": 10795
    },
    {
      "epoch": 3.08025,
      "grad_norm": 4.694728851318359,
      "learning_rate": 2.3764534740092624e-05,
      "loss": 1.0168,
      "step": 10800
    },
    {
      "epoch": 3.0805,
      "grad_norm": 4.954636573791504,
      "learning_rate": 2.3743892045505764e-05,
      "loss": 1.0078,
      "step": 10805
    },
    {
      "epoch": 3.08075,
      "grad_norm": 5.639698505401611,
      "learning_rate": 2.3723250209456842e-05,
      "loss": 1.0009,
      "step": 10810
    },
    {
      "epoch": 3.081,
      "grad_norm": 3.9849894046783447,
      "learning_rate": 2.370260924605437e-05,
      "loss": 0.9936,
      "step": 10815
    },
    {
      "epoch": 3.08125,
      "grad_norm": 3.673048973083496,
      "learning_rate": 2.3681969169406233e-05,
      "loss": 1.0247,
      "step": 10820
    },
    {
      "epoch": 3.0815,
      "grad_norm": 3.663918972015381,
      "learning_rate": 2.3661329993619757e-05,
      "loss": 0.9724,
      "step": 10825
    },
    {
      "epoch": 3.08175,
      "grad_norm": 3.565776824951172,
      "learning_rate": 2.3640691732801597e-05,
      "loss": 0.9963,
      "step": 10830
    },
    {
      "epoch": 3.082,
      "grad_norm": 4.064138412475586,
      "learning_rate": 2.3620054401057824e-05,
      "loss": 1.0397,
      "step": 10835
    },
    {
      "epoch": 3.08225,
      "grad_norm": 4.382983207702637,
      "learning_rate": 2.359941801249385e-05,
      "loss": 0.9443,
      "step": 10840
    },
    {
      "epoch": 3.0825,
      "grad_norm": 4.525401592254639,
      "learning_rate": 2.3578782581214465e-05,
      "loss": 0.9997,
      "step": 10845
    },
    {
      "epoch": 3.08275,
      "grad_norm": 4.199427604675293,
      "learning_rate": 2.3558148121323777e-05,
      "loss": 0.9621,
      "step": 10850
    },
    {
      "epoch": 3.083,
      "grad_norm": 4.298840522766113,
      "learning_rate": 2.3537514646925257e-05,
      "loss": 0.9251,
      "step": 10855
    },
    {
      "epoch": 3.08325,
      "grad_norm": 4.789324760437012,
      "learning_rate": 2.351688217212168e-05,
      "loss": 0.8969,
      "step": 10860
    },
    {
      "epoch": 3.0835,
      "grad_norm": 5.929366111755371,
      "learning_rate": 2.349625071101516e-05,
      "loss": 1.074,
      "step": 10865
    },
    {
      "epoch": 3.08375,
      "grad_norm": 3.868479013442993,
      "learning_rate": 2.3475620277707105e-05,
      "loss": 0.8683,
      "step": 10870
    },
    {
      "epoch": 3.084,
      "grad_norm": 5.770437717437744,
      "learning_rate": 2.3454990886298207e-05,
      "loss": 0.8288,
      "step": 10875
    },
    {
      "epoch": 3.08425,
      "grad_norm": 3.376136541366577,
      "learning_rate": 2.3434362550888467e-05,
      "loss": 0.7616,
      "step": 10880
    },
    {
      "epoch": 3.0845,
      "grad_norm": 6.022212028503418,
      "learning_rate": 2.3413735285577167e-05,
      "loss": 0.9875,
      "step": 10885
    },
    {
      "epoch": 3.08475,
      "grad_norm": 3.5034890174865723,
      "learning_rate": 2.3393109104462835e-05,
      "loss": 0.807,
      "step": 10890
    },
    {
      "epoch": 3.085,
      "grad_norm": 4.467817783355713,
      "learning_rate": 2.3372484021643284e-05,
      "loss": 0.8118,
      "step": 10895
    },
    {
      "epoch": 3.08525,
      "grad_norm": 4.254854679107666,
      "learning_rate": 2.3351860051215556e-05,
      "loss": 0.8223,
      "step": 10900
    },
    {
      "epoch": 3.0855,
      "grad_norm": 3.2972302436828613,
      "learning_rate": 2.333123720727594e-05,
      "loss": 0.7444,
      "step": 10905
    },
    {
      "epoch": 3.08575,
      "grad_norm": 3.9498131275177,
      "learning_rate": 2.331061550391996e-05,
      "loss": 0.9056,
      "step": 10910
    },
    {
      "epoch": 3.086,
      "grad_norm": 5.128267765045166,
      "learning_rate": 2.328999495524235e-05,
      "loss": 0.9205,
      "step": 10915
    },
    {
      "epoch": 3.08625,
      "grad_norm": 4.588869094848633,
      "learning_rate": 2.3269375575337076e-05,
      "loss": 0.7945,
      "step": 10920
    },
    {
      "epoch": 3.0865,
      "grad_norm": 3.704223871231079,
      "learning_rate": 2.3248757378297263e-05,
      "loss": 0.8171,
      "step": 10925
    },
    {
      "epoch": 3.08675,
      "grad_norm": 4.028062343597412,
      "learning_rate": 2.3228140378215273e-05,
      "loss": 0.8584,
      "step": 10930
    },
    {
      "epoch": 3.087,
      "grad_norm": 4.251224994659424,
      "learning_rate": 2.3207524589182627e-05,
      "loss": 0.8972,
      "step": 10935
    },
    {
      "epoch": 3.08725,
      "grad_norm": 4.4667649269104,
      "learning_rate": 2.318691002529002e-05,
      "loss": 0.8482,
      "step": 10940
    },
    {
      "epoch": 3.0875,
      "grad_norm": 5.0932416915893555,
      "learning_rate": 2.3166296700627315e-05,
      "loss": 0.8446,
      "step": 10945
    },
    {
      "epoch": 3.0877499999999998,
      "grad_norm": 4.01995849609375,
      "learning_rate": 2.3145684629283525e-05,
      "loss": 0.8034,
      "step": 10950
    },
    {
      "epoch": 3.088,
      "grad_norm": 5.195940017700195,
      "learning_rate": 2.3125073825346806e-05,
      "loss": 0.8495,
      "step": 10955
    },
    {
      "epoch": 3.08825,
      "grad_norm": 4.14820671081543,
      "learning_rate": 2.3104464302904447e-05,
      "loss": 0.8217,
      "step": 10960
    },
    {
      "epoch": 3.0885,
      "grad_norm": 5.003696918487549,
      "learning_rate": 2.308385607604287e-05,
      "loss": 0.783,
      "step": 10965
    },
    {
      "epoch": 3.08875,
      "grad_norm": 4.453670024871826,
      "learning_rate": 2.3063249158847596e-05,
      "loss": 0.912,
      "step": 10970
    },
    {
      "epoch": 3.089,
      "grad_norm": 4.447891712188721,
      "learning_rate": 2.3042643565403254e-05,
      "loss": 0.8743,
      "step": 10975
    },
    {
      "epoch": 3.08925,
      "grad_norm": 4.376669883728027,
      "learning_rate": 2.3022039309793573e-05,
      "loss": 0.7996,
      "step": 10980
    },
    {
      "epoch": 3.0895,
      "grad_norm": 5.160890102386475,
      "learning_rate": 2.3001436406101383e-05,
      "loss": 0.8396,
      "step": 10985
    },
    {
      "epoch": 3.08975,
      "grad_norm": 4.135390758514404,
      "learning_rate": 2.2980834868408557e-05,
      "loss": 0.749,
      "step": 10990
    },
    {
      "epoch": 3.09,
      "grad_norm": 4.911041736602783,
      "learning_rate": 2.2960234710796063e-05,
      "loss": 0.8725,
      "step": 10995
    },
    {
      "epoch": 3.09025,
      "grad_norm": 4.769533157348633,
      "learning_rate": 2.2939635947343907e-05,
      "loss": 0.8328,
      "step": 11000
    },
    {
      "epoch": 3.09025,
      "eval_loss": 2.0912914276123047,
      "eval_runtime": 5.4858,
      "eval_samples_per_second": 186.662,
      "eval_steps_per_second": 23.333,
      "step": 11000
    },
    {
      "epoch": 3.0905,
      "grad_norm": 4.876537322998047,
      "learning_rate": 2.2919038592131158e-05,
      "loss": 0.7639,
      "step": 11005
    },
    {
      "epoch": 3.09075,
      "grad_norm": 4.963976860046387,
      "learning_rate": 2.2898442659235913e-05,
      "loss": 0.7209,
      "step": 11010
    },
    {
      "epoch": 3.091,
      "grad_norm": 4.399697303771973,
      "learning_rate": 2.287784816273528e-05,
      "loss": 0.8083,
      "step": 11015
    },
    {
      "epoch": 3.09125,
      "grad_norm": 4.2888712882995605,
      "learning_rate": 2.285725511670543e-05,
      "loss": 0.8424,
      "step": 11020
    },
    {
      "epoch": 3.0915,
      "grad_norm": 5.099954128265381,
      "learning_rate": 2.283666353522149e-05,
      "loss": 0.9392,
      "step": 11025
    },
    {
      "epoch": 3.09175,
      "grad_norm": 3.8004982471466064,
      "learning_rate": 2.2816073432357634e-05,
      "loss": 0.997,
      "step": 11030
    },
    {
      "epoch": 3.092,
      "grad_norm": 4.542891502380371,
      "learning_rate": 2.2795484822186993e-05,
      "loss": 0.8019,
      "step": 11035
    },
    {
      "epoch": 3.09225,
      "grad_norm": 4.741520881652832,
      "learning_rate": 2.2774897718781682e-05,
      "loss": 0.932,
      "step": 11040
    },
    {
      "epoch": 3.0925,
      "grad_norm": 4.786231994628906,
      "learning_rate": 2.2754312136212804e-05,
      "loss": 0.8577,
      "step": 11045
    },
    {
      "epoch": 3.09275,
      "grad_norm": 3.8261919021606445,
      "learning_rate": 2.2733728088550405e-05,
      "loss": 0.9109,
      "step": 11050
    },
    {
      "epoch": 3.093,
      "grad_norm": 4.550124645233154,
      "learning_rate": 2.271314558986349e-05,
      "loss": 0.8464,
      "step": 11055
    },
    {
      "epoch": 3.09325,
      "grad_norm": 4.509208679199219,
      "learning_rate": 2.2692564654220007e-05,
      "loss": 0.9014,
      "step": 11060
    },
    {
      "epoch": 3.0935,
      "grad_norm": 4.460159778594971,
      "learning_rate": 2.2671985295686813e-05,
      "loss": 0.9291,
      "step": 11065
    },
    {
      "epoch": 3.09375,
      "grad_norm": 5.380171298980713,
      "learning_rate": 2.265140752832972e-05,
      "loss": 1.0662,
      "step": 11070
    },
    {
      "epoch": 3.094,
      "grad_norm": 3.6659886837005615,
      "learning_rate": 2.2630831366213433e-05,
      "loss": 0.8499,
      "step": 11075
    },
    {
      "epoch": 3.09425,
      "grad_norm": 4.711562156677246,
      "learning_rate": 2.2610256823401567e-05,
      "loss": 0.7994,
      "step": 11080
    },
    {
      "epoch": 3.0945,
      "grad_norm": 3.3331949710845947,
      "learning_rate": 2.2589683913956636e-05,
      "loss": 0.7947,
      "step": 11085
    },
    {
      "epoch": 3.09475,
      "grad_norm": 4.743300437927246,
      "learning_rate": 2.2569112651940016e-05,
      "loss": 0.9015,
      "step": 11090
    },
    {
      "epoch": 3.095,
      "grad_norm": 5.085861682891846,
      "learning_rate": 2.254854305141198e-05,
      "loss": 0.9122,
      "step": 11095
    },
    {
      "epoch": 3.09525,
      "grad_norm": 4.362017631530762,
      "learning_rate": 2.2527975126431648e-05,
      "loss": 0.8546,
      "step": 11100
    },
    {
      "epoch": 3.0955,
      "grad_norm": 4.8154683113098145,
      "learning_rate": 2.250740889105702e-05,
      "loss": 0.9398,
      "step": 11105
    },
    {
      "epoch": 3.09575,
      "grad_norm": 6.139589786529541,
      "learning_rate": 2.2486844359344902e-05,
      "loss": 0.9944,
      "step": 11110
    },
    {
      "epoch": 3.096,
      "grad_norm": 5.280840873718262,
      "learning_rate": 2.2466281545350962e-05,
      "loss": 1.0216,
      "step": 11115
    },
    {
      "epoch": 3.09625,
      "grad_norm": 5.108925819396973,
      "learning_rate": 2.2445720463129703e-05,
      "loss": 0.9477,
      "step": 11120
    },
    {
      "epoch": 3.0965,
      "grad_norm": 4.287631511688232,
      "learning_rate": 2.242516112673441e-05,
      "loss": 0.9236,
      "step": 11125
    },
    {
      "epoch": 3.09675,
      "grad_norm": 4.749042987823486,
      "learning_rate": 2.2404603550217214e-05,
      "loss": 0.9353,
      "step": 11130
    },
    {
      "epoch": 3.097,
      "grad_norm": 5.067904472351074,
      "learning_rate": 2.2384047747629023e-05,
      "loss": 0.868,
      "step": 11135
    },
    {
      "epoch": 3.09725,
      "grad_norm": 3.9917523860931396,
      "learning_rate": 2.2363493733019522e-05,
      "loss": 0.8777,
      "step": 11140
    },
    {
      "epoch": 3.0975,
      "grad_norm": 7.671396732330322,
      "learning_rate": 2.23429415204372e-05,
      "loss": 0.9825,
      "step": 11145
    },
    {
      "epoch": 3.09775,
      "grad_norm": 5.327398777008057,
      "learning_rate": 2.23223911239293e-05,
      "loss": 0.9113,
      "step": 11150
    },
    {
      "epoch": 3.098,
      "grad_norm": 5.543046474456787,
      "learning_rate": 2.2301842557541808e-05,
      "loss": 0.9932,
      "step": 11155
    },
    {
      "epoch": 3.09825,
      "grad_norm": 4.446226119995117,
      "learning_rate": 2.2281295835319487e-05,
      "loss": 0.8371,
      "step": 11160
    },
    {
      "epoch": 3.0985,
      "grad_norm": 4.998643398284912,
      "learning_rate": 2.226075097130583e-05,
      "loss": 0.9982,
      "step": 11165
    },
    {
      "epoch": 3.09875,
      "grad_norm": 4.797536373138428,
      "learning_rate": 2.2240207979543053e-05,
      "loss": 0.9605,
      "step": 11170
    },
    {
      "epoch": 3.099,
      "grad_norm": 4.330894470214844,
      "learning_rate": 2.2219666874072093e-05,
      "loss": 0.889,
      "step": 11175
    },
    {
      "epoch": 3.09925,
      "grad_norm": 6.160038471221924,
      "learning_rate": 2.219912766893261e-05,
      "loss": 0.9929,
      "step": 11180
    },
    {
      "epoch": 3.0995,
      "grad_norm": 3.6625659465789795,
      "learning_rate": 2.217859037816296e-05,
      "loss": 0.9186,
      "step": 11185
    },
    {
      "epoch": 3.0997500000000002,
      "grad_norm": 3.763399839401245,
      "learning_rate": 2.2158055015800174e-05,
      "loss": 0.8038,
      "step": 11190
    },
    {
      "epoch": 3.1,
      "grad_norm": 4.1213698387146,
      "learning_rate": 2.213752159588e-05,
      "loss": 0.9709,
      "step": 11195
    },
    {
      "epoch": 3.10025,
      "grad_norm": 5.699910640716553,
      "learning_rate": 2.2116990132436818e-05,
      "loss": 0.9902,
      "step": 11200
    },
    {
      "epoch": 3.1005,
      "grad_norm": 4.515742301940918,
      "learning_rate": 2.209646063950369e-05,
      "loss": 0.8113,
      "step": 11205
    },
    {
      "epoch": 3.10075,
      "grad_norm": 4.583585739135742,
      "learning_rate": 2.2075933131112342e-05,
      "loss": 0.8272,
      "step": 11210
    },
    {
      "epoch": 3.101,
      "grad_norm": 5.680623531341553,
      "learning_rate": 2.2055407621293122e-05,
      "loss": 0.9163,
      "step": 11215
    },
    {
      "epoch": 3.10125,
      "grad_norm": 5.083887577056885,
      "learning_rate": 2.203488412407503e-05,
      "loss": 1.059,
      "step": 11220
    },
    {
      "epoch": 3.1015,
      "grad_norm": 3.5480973720550537,
      "learning_rate": 2.2014362653485675e-05,
      "loss": 0.8376,
      "step": 11225
    },
    {
      "epoch": 3.10175,
      "grad_norm": 3.469794273376465,
      "learning_rate": 2.1993843223551297e-05,
      "loss": 0.8227,
      "step": 11230
    },
    {
      "epoch": 3.102,
      "grad_norm": 4.875741004943848,
      "learning_rate": 2.197332584829673e-05,
      "loss": 0.8319,
      "step": 11235
    },
    {
      "epoch": 3.10225,
      "grad_norm": 4.58941650390625,
      "learning_rate": 2.1952810541745402e-05,
      "loss": 0.882,
      "step": 11240
    },
    {
      "epoch": 3.1025,
      "grad_norm": 4.237195014953613,
      "learning_rate": 2.1932297317919348e-05,
      "loss": 0.8208,
      "step": 11245
    },
    {
      "epoch": 3.10275,
      "grad_norm": 4.511893272399902,
      "learning_rate": 2.1911786190839143e-05,
      "loss": 0.8073,
      "step": 11250
    },
    {
      "epoch": 3.103,
      "grad_norm": 4.722198486328125,
      "learning_rate": 2.189127717452395e-05,
      "loss": 0.9085,
      "step": 11255
    },
    {
      "epoch": 3.10325,
      "grad_norm": 4.315439701080322,
      "learning_rate": 2.1870770282991504e-05,
      "loss": 0.7379,
      "step": 11260
    },
    {
      "epoch": 3.1035,
      "grad_norm": 4.986917972564697,
      "learning_rate": 2.1850265530258055e-05,
      "loss": 0.9026,
      "step": 11265
    },
    {
      "epoch": 3.10375,
      "grad_norm": 3.532893180847168,
      "learning_rate": 2.1829762930338422e-05,
      "loss": 0.7242,
      "step": 11270
    },
    {
      "epoch": 3.104,
      "grad_norm": 4.052214622497559,
      "learning_rate": 2.1809262497245925e-05,
      "loss": 0.959,
      "step": 11275
    },
    {
      "epoch": 3.10425,
      "grad_norm": 4.906370162963867,
      "learning_rate": 2.1788764244992427e-05,
      "loss": 0.9224,
      "step": 11280
    },
    {
      "epoch": 3.1045,
      "grad_norm": 4.0057373046875,
      "learning_rate": 2.1768268187588293e-05,
      "loss": 0.8384,
      "step": 11285
    },
    {
      "epoch": 3.10475,
      "grad_norm": 4.590488433837891,
      "learning_rate": 2.174777433904238e-05,
      "loss": 0.7893,
      "step": 11290
    },
    {
      "epoch": 3.105,
      "grad_norm": 4.70190954208374,
      "learning_rate": 2.1727282713362036e-05,
      "loss": 0.8961,
      "step": 11295
    },
    {
      "epoch": 3.10525,
      "grad_norm": 4.848373889923096,
      "learning_rate": 2.17067933245531e-05,
      "loss": 0.8333,
      "step": 11300
    },
    {
      "epoch": 3.1055,
      "grad_norm": 5.37358283996582,
      "learning_rate": 2.1686306186619863e-05,
      "loss": 0.7965,
      "step": 11305
    },
    {
      "epoch": 3.10575,
      "grad_norm": 5.651182174682617,
      "learning_rate": 2.166582131356511e-05,
      "loss": 0.7932,
      "step": 11310
    },
    {
      "epoch": 3.106,
      "grad_norm": 5.057504653930664,
      "learning_rate": 2.1645338719390047e-05,
      "loss": 0.7908,
      "step": 11315
    },
    {
      "epoch": 3.10625,
      "grad_norm": 3.918440103530884,
      "learning_rate": 2.162485841809434e-05,
      "loss": 0.7932,
      "step": 11320
    },
    {
      "epoch": 3.1065,
      "grad_norm": 5.905324935913086,
      "learning_rate": 2.1604380423676073e-05,
      "loss": 0.8587,
      "step": 11325
    },
    {
      "epoch": 3.10675,
      "grad_norm": 4.431978225708008,
      "learning_rate": 2.158390475013177e-05,
      "loss": 0.842,
      "step": 11330
    },
    {
      "epoch": 3.107,
      "grad_norm": 5.637931823730469,
      "learning_rate": 2.1563431411456367e-05,
      "loss": 0.8966,
      "step": 11335
    },
    {
      "epoch": 3.10725,
      "grad_norm": 4.72392463684082,
      "learning_rate": 2.1542960421643187e-05,
      "loss": 0.7211,
      "step": 11340
    },
    {
      "epoch": 3.1075,
      "grad_norm": 4.37111234664917,
      "learning_rate": 2.152249179468395e-05,
      "loss": 0.8855,
      "step": 11345
    },
    {
      "epoch": 3.10775,
      "grad_norm": 4.020848751068115,
      "learning_rate": 2.1502025544568788e-05,
      "loss": 0.822,
      "step": 11350
    },
    {
      "epoch": 3.108,
      "grad_norm": 4.806123733520508,
      "learning_rate": 2.148156168528618e-05,
      "loss": 0.8169,
      "step": 11355
    },
    {
      "epoch": 3.10825,
      "grad_norm": 4.141100883483887,
      "learning_rate": 2.1461100230822984e-05,
      "loss": 0.8051,
      "step": 11360
    },
    {
      "epoch": 3.1085,
      "grad_norm": 4.382250785827637,
      "learning_rate": 2.1440641195164405e-05,
      "loss": 0.7794,
      "step": 11365
    },
    {
      "epoch": 3.10875,
      "grad_norm": 4.819477081298828,
      "learning_rate": 2.1420184592294017e-05,
      "loss": 0.8,
      "step": 11370
    },
    {
      "epoch": 3.109,
      "grad_norm": 4.008170127868652,
      "learning_rate": 2.1399730436193697e-05,
      "loss": 0.8761,
      "step": 11375
    },
    {
      "epoch": 3.10925,
      "grad_norm": 4.4284234046936035,
      "learning_rate": 2.1379278740843673e-05,
      "loss": 0.8342,
      "step": 11380
    },
    {
      "epoch": 3.1095,
      "grad_norm": 5.049548149108887,
      "learning_rate": 2.1358829520222502e-05,
      "loss": 0.7381,
      "step": 11385
    },
    {
      "epoch": 3.10975,
      "grad_norm": 3.3173892498016357,
      "learning_rate": 2.1338382788307014e-05,
      "loss": 0.7435,
      "step": 11390
    },
    {
      "epoch": 3.11,
      "grad_norm": 4.113122940063477,
      "learning_rate": 2.1317938559072352e-05,
      "loss": 0.8019,
      "step": 11395
    },
    {
      "epoch": 3.11025,
      "grad_norm": 5.103049278259277,
      "learning_rate": 2.1297496846491977e-05,
      "loss": 0.7931,
      "step": 11400
    },
    {
      "epoch": 3.1105,
      "grad_norm": 4.268495559692383,
      "learning_rate": 2.127705766453759e-05,
      "loss": 0.8058,
      "step": 11405
    },
    {
      "epoch": 3.11075,
      "grad_norm": 5.110839366912842,
      "learning_rate": 2.1256621027179186e-05,
      "loss": 1.061,
      "step": 11410
    },
    {
      "epoch": 3.111,
      "grad_norm": 4.585909843444824,
      "learning_rate": 2.1236186948385007e-05,
      "loss": 0.8795,
      "step": 11415
    },
    {
      "epoch": 3.11125,
      "grad_norm": 5.821427822113037,
      "learning_rate": 2.121575544212156e-05,
      "loss": 1.02,
      "step": 11420
    },
    {
      "epoch": 3.1115,
      "grad_norm": 4.84199333190918,
      "learning_rate": 2.119532652235359e-05,
      "loss": 0.9282,
      "step": 11425
    },
    {
      "epoch": 3.11175,
      "grad_norm": 4.903069019317627,
      "learning_rate": 2.1174900203044066e-05,
      "loss": 0.9493,
      "step": 11430
    },
    {
      "epoch": 3.112,
      "grad_norm": 5.07769775390625,
      "learning_rate": 2.115447649815418e-05,
      "loss": 0.9909,
      "step": 11435
    },
    {
      "epoch": 3.11225,
      "grad_norm": 4.465160369873047,
      "learning_rate": 2.113405542164335e-05,
      "loss": 0.8783,
      "step": 11440
    },
    {
      "epoch": 3.1125,
      "grad_norm": 4.319166660308838,
      "learning_rate": 2.1113636987469183e-05,
      "loss": 1.1362,
      "step": 11445
    },
    {
      "epoch": 3.11275,
      "grad_norm": 3.965996265411377,
      "learning_rate": 2.1093221209587492e-05,
      "loss": 0.9107,
      "step": 11450
    },
    {
      "epoch": 3.113,
      "grad_norm": 4.284527778625488,
      "learning_rate": 2.107280810195226e-05,
      "loss": 0.9389,
      "step": 11455
    },
    {
      "epoch": 3.11325,
      "grad_norm": 4.171040058135986,
      "learning_rate": 2.1052397678515665e-05,
      "loss": 0.9669,
      "step": 11460
    },
    {
      "epoch": 3.1135,
      "grad_norm": 4.166055202484131,
      "learning_rate": 2.1031989953228027e-05,
      "loss": 0.8944,
      "step": 11465
    },
    {
      "epoch": 3.11375,
      "grad_norm": 4.232789516448975,
      "learning_rate": 2.1011584940037836e-05,
      "loss": 0.864,
      "step": 11470
    },
    {
      "epoch": 3.114,
      "grad_norm": 4.175373554229736,
      "learning_rate": 2.0991182652891746e-05,
      "loss": 0.9039,
      "step": 11475
    },
    {
      "epoch": 3.11425,
      "grad_norm": 5.836989879608154,
      "learning_rate": 2.0970783105734506e-05,
      "loss": 0.8999,
      "step": 11480
    },
    {
      "epoch": 3.1145,
      "grad_norm": 4.460977077484131,
      "learning_rate": 2.0950386312509015e-05,
      "loss": 1.023,
      "step": 11485
    },
    {
      "epoch": 3.11475,
      "grad_norm": 5.048220157623291,
      "learning_rate": 2.0929992287156298e-05,
      "loss": 0.8671,
      "step": 11490
    },
    {
      "epoch": 3.115,
      "grad_norm": 5.913463115692139,
      "learning_rate": 2.090960104361547e-05,
      "loss": 1.0474,
      "step": 11495
    },
    {
      "epoch": 3.11525,
      "grad_norm": 6.49947452545166,
      "learning_rate": 2.088921259582376e-05,
      "loss": 0.9886,
      "step": 11500
    },
    {
      "epoch": 3.11525,
      "eval_loss": 2.0972788333892822,
      "eval_runtime": 5.2196,
      "eval_samples_per_second": 196.182,
      "eval_steps_per_second": 24.523,
      "step": 11500
    },
    {
      "epoch": 3.1155,
      "grad_norm": 4.492157459259033,
      "learning_rate": 2.086882695771648e-05,
      "loss": 0.6965,
      "step": 11505
    },
    {
      "epoch": 3.11575,
      "grad_norm": 4.76690149307251,
      "learning_rate": 2.084844414322702e-05,
      "loss": 0.829,
      "step": 11510
    },
    {
      "epoch": 3.116,
      "grad_norm": 4.859016418457031,
      "learning_rate": 2.0828064166286838e-05,
      "loss": 0.9186,
      "step": 11515
    },
    {
      "epoch": 3.11625,
      "grad_norm": 5.530061721801758,
      "learning_rate": 2.080768704082546e-05,
      "loss": 0.8841,
      "step": 11520
    },
    {
      "epoch": 3.1165,
      "grad_norm": 5.238788604736328,
      "learning_rate": 2.0787312780770467e-05,
      "loss": 0.8344,
      "step": 11525
    },
    {
      "epoch": 3.11675,
      "grad_norm": 4.470865249633789,
      "learning_rate": 2.0766941400047462e-05,
      "loss": 0.8483,
      "step": 11530
    },
    {
      "epoch": 3.117,
      "grad_norm": 3.893965721130371,
      "learning_rate": 2.0746572912580086e-05,
      "loss": 0.8304,
      "step": 11535
    },
    {
      "epoch": 3.11725,
      "grad_norm": 3.869633197784424,
      "learning_rate": 2.0726207332290026e-05,
      "loss": 0.8891,
      "step": 11540
    },
    {
      "epoch": 3.1175,
      "grad_norm": 3.9134414196014404,
      "learning_rate": 2.0705844673096945e-05,
      "loss": 0.8003,
      "step": 11545
    },
    {
      "epoch": 3.11775,
      "grad_norm": 4.902710437774658,
      "learning_rate": 2.068548494891855e-05,
      "loss": 0.7881,
      "step": 11550
    },
    {
      "epoch": 3.118,
      "grad_norm": 5.6194634437561035,
      "learning_rate": 2.0665128173670502e-05,
      "loss": 0.8191,
      "step": 11555
    },
    {
      "epoch": 3.11825,
      "grad_norm": 4.667191505432129,
      "learning_rate": 2.0644774361266472e-05,
      "loss": 0.828,
      "step": 11560
    },
    {
      "epoch": 3.1185,
      "grad_norm": 3.5743489265441895,
      "learning_rate": 2.0624423525618098e-05,
      "loss": 0.7295,
      "step": 11565
    },
    {
      "epoch": 3.11875,
      "grad_norm": 4.671918869018555,
      "learning_rate": 2.0604075680634995e-05,
      "loss": 0.8624,
      "step": 11570
    },
    {
      "epoch": 3.1189999999999998,
      "grad_norm": 4.627743721008301,
      "learning_rate": 2.0583730840224703e-05,
      "loss": 0.9185,
      "step": 11575
    },
    {
      "epoch": 3.11925,
      "grad_norm": 4.63300085067749,
      "learning_rate": 2.056338901829274e-05,
      "loss": 0.8589,
      "step": 11580
    },
    {
      "epoch": 3.1195,
      "grad_norm": 5.820733070373535,
      "learning_rate": 2.0543050228742548e-05,
      "loss": 0.7898,
      "step": 11585
    },
    {
      "epoch": 3.11975,
      "grad_norm": 4.802334308624268,
      "learning_rate": 2.05227144854755e-05,
      "loss": 0.9035,
      "step": 11590
    },
    {
      "epoch": 3.12,
      "grad_norm": 4.23307991027832,
      "learning_rate": 2.050238180239088e-05,
      "loss": 0.8509,
      "step": 11595
    },
    {
      "epoch": 3.12025,
      "grad_norm": 4.606180191040039,
      "learning_rate": 2.0482052193385893e-05,
      "loss": 0.7276,
      "step": 11600
    },
    {
      "epoch": 3.1205,
      "grad_norm": 4.4818830490112305,
      "learning_rate": 2.0461725672355623e-05,
      "loss": 0.8149,
      "step": 11605
    },
    {
      "epoch": 3.12075,
      "grad_norm": 6.5735955238342285,
      "learning_rate": 2.0441402253193072e-05,
      "loss": 0.9325,
      "step": 11610
    },
    {
      "epoch": 3.121,
      "grad_norm": 4.692362308502197,
      "learning_rate": 2.0421081949789097e-05,
      "loss": 0.9168,
      "step": 11615
    },
    {
      "epoch": 3.12125,
      "grad_norm": 3.78690242767334,
      "learning_rate": 2.0400764776032424e-05,
      "loss": 0.7577,
      "step": 11620
    },
    {
      "epoch": 3.1215,
      "grad_norm": 4.578655242919922,
      "learning_rate": 2.038045074580966e-05,
      "loss": 0.9322,
      "step": 11625
    },
    {
      "epoch": 3.12175,
      "grad_norm": 5.127806186676025,
      "learning_rate": 2.0360139873005253e-05,
      "loss": 0.8074,
      "step": 11630
    },
    {
      "epoch": 3.122,
      "grad_norm": 4.5189619064331055,
      "learning_rate": 2.0339832171501487e-05,
      "loss": 0.8945,
      "step": 11635
    },
    {
      "epoch": 3.12225,
      "grad_norm": 4.688840866088867,
      "learning_rate": 2.0319527655178495e-05,
      "loss": 0.8373,
      "step": 11640
    },
    {
      "epoch": 3.1225,
      "grad_norm": 4.2990827560424805,
      "learning_rate": 2.029922633791421e-05,
      "loss": 0.8528,
      "step": 11645
    },
    {
      "epoch": 3.12275,
      "grad_norm": 4.387007236480713,
      "learning_rate": 2.027892823358441e-05,
      "loss": 0.8517,
      "step": 11650
    },
    {
      "epoch": 3.123,
      "grad_norm": 4.568553924560547,
      "learning_rate": 2.0258633356062632e-05,
      "loss": 0.767,
      "step": 11655
    },
    {
      "epoch": 3.12325,
      "grad_norm": 5.115042209625244,
      "learning_rate": 2.0238341719220254e-05,
      "loss": 0.7929,
      "step": 11660
    },
    {
      "epoch": 3.1235,
      "grad_norm": 4.09073543548584,
      "learning_rate": 2.0218053336926417e-05,
      "loss": 0.8481,
      "step": 11665
    },
    {
      "epoch": 3.12375,
      "grad_norm": 4.519857406616211,
      "learning_rate": 2.019776822304802e-05,
      "loss": 0.8072,
      "step": 11670
    },
    {
      "epoch": 3.124,
      "grad_norm": 4.716091632843018,
      "learning_rate": 2.0177486391449764e-05,
      "loss": 0.8041,
      "step": 11675
    },
    {
      "epoch": 3.12425,
      "grad_norm": 4.821226119995117,
      "learning_rate": 2.0157207855994088e-05,
      "loss": 0.7862,
      "step": 11680
    },
    {
      "epoch": 3.1245,
      "grad_norm": 4.154922962188721,
      "learning_rate": 2.0136932630541176e-05,
      "loss": 0.8398,
      "step": 11685
    },
    {
      "epoch": 3.12475,
      "grad_norm": 4.45950984954834,
      "learning_rate": 2.011666072894896e-05,
      "loss": 0.718,
      "step": 11690
    },
    {
      "epoch": 3.125,
      "grad_norm": 6.393787384033203,
      "learning_rate": 2.0096392165073082e-05,
      "loss": 0.8911,
      "step": 11695
    },
    {
      "epoch": 3.12525,
      "grad_norm": 3.598799705505371,
      "learning_rate": 2.0076126952766925e-05,
      "loss": 0.8169,
      "step": 11700
    },
    {
      "epoch": 3.1255,
      "grad_norm": 4.221689701080322,
      "learning_rate": 2.005586510588156e-05,
      "loss": 0.8641,
      "step": 11705
    },
    {
      "epoch": 3.12575,
      "grad_norm": 5.003805637359619,
      "learning_rate": 2.003560663826579e-05,
      "loss": 0.736,
      "step": 11710
    },
    {
      "epoch": 3.126,
      "grad_norm": 4.901957988739014,
      "learning_rate": 2.0015351563766062e-05,
      "loss": 0.7777,
      "step": 11715
    },
    {
      "epoch": 3.12625,
      "grad_norm": 3.7544424533843994,
      "learning_rate": 1.9995099896226532e-05,
      "loss": 0.7857,
      "step": 11720
    },
    {
      "epoch": 3.1265,
      "grad_norm": 7.058006763458252,
      "learning_rate": 1.997485164948903e-05,
      "loss": 0.8124,
      "step": 11725
    },
    {
      "epoch": 3.12675,
      "grad_norm": 4.553875923156738,
      "learning_rate": 1.9954606837393052e-05,
      "loss": 0.8081,
      "step": 11730
    },
    {
      "epoch": 3.127,
      "grad_norm": 3.924345016479492,
      "learning_rate": 1.9934365473775713e-05,
      "loss": 0.7897,
      "step": 11735
    },
    {
      "epoch": 3.12725,
      "grad_norm": 5.4212422370910645,
      "learning_rate": 1.9914127572471815e-05,
      "loss": 0.7169,
      "step": 11740
    },
    {
      "epoch": 3.1275,
      "grad_norm": 4.0348334312438965,
      "learning_rate": 1.989389314731376e-05,
      "loss": 0.7266,
      "step": 11745
    },
    {
      "epoch": 3.12775,
      "grad_norm": 4.7539286613464355,
      "learning_rate": 1.9873662212131594e-05,
      "loss": 0.8379,
      "step": 11750
    },
    {
      "epoch": 3.128,
      "grad_norm": 4.298587322235107,
      "learning_rate": 1.9853434780752973e-05,
      "loss": 0.7574,
      "step": 11755
    },
    {
      "epoch": 3.12825,
      "grad_norm": 6.218019485473633,
      "learning_rate": 1.9833210867003144e-05,
      "loss": 0.8133,
      "step": 11760
    },
    {
      "epoch": 3.1285,
      "grad_norm": 4.25946569442749,
      "learning_rate": 1.981299048470497e-05,
      "loss": 0.8041,
      "step": 11765
    },
    {
      "epoch": 3.12875,
      "grad_norm": 4.183948040008545,
      "learning_rate": 1.9792773647678882e-05,
      "loss": 0.9842,
      "step": 11770
    },
    {
      "epoch": 3.129,
      "grad_norm": 3.789961338043213,
      "learning_rate": 1.9772560369742904e-05,
      "loss": 0.7065,
      "step": 11775
    },
    {
      "epoch": 3.12925,
      "grad_norm": 3.9369215965270996,
      "learning_rate": 1.975235066471263e-05,
      "loss": 0.7333,
      "step": 11780
    },
    {
      "epoch": 3.1295,
      "grad_norm": 3.4967055320739746,
      "learning_rate": 1.973214454640119e-05,
      "loss": 0.7778,
      "step": 11785
    },
    {
      "epoch": 3.12975,
      "grad_norm": 4.599077224731445,
      "learning_rate": 1.971194202861928e-05,
      "loss": 0.6971,
      "step": 11790
    },
    {
      "epoch": 3.13,
      "grad_norm": 5.03056526184082,
      "learning_rate": 1.9691743125175126e-05,
      "loss": 0.7876,
      "step": 11795
    },
    {
      "epoch": 3.13025,
      "grad_norm": 4.960763454437256,
      "learning_rate": 1.96715478498745e-05,
      "loss": 0.7399,
      "step": 11800
    },
    {
      "epoch": 3.1305,
      "grad_norm": 4.31507682800293,
      "learning_rate": 1.965135621652068e-05,
      "loss": 0.8453,
      "step": 11805
    },
    {
      "epoch": 3.13075,
      "grad_norm": 4.732880115509033,
      "learning_rate": 1.9631168238914437e-05,
      "loss": 0.8247,
      "step": 11810
    },
    {
      "epoch": 3.1310000000000002,
      "grad_norm": 4.863857746124268,
      "learning_rate": 1.9610983930854087e-05,
      "loss": 0.7216,
      "step": 11815
    },
    {
      "epoch": 3.13125,
      "grad_norm": 5.005334377288818,
      "learning_rate": 1.9590803306135397e-05,
      "loss": 0.7483,
      "step": 11820
    },
    {
      "epoch": 3.1315,
      "grad_norm": 4.145999908447266,
      "learning_rate": 1.9570626378551645e-05,
      "loss": 0.6943,
      "step": 11825
    },
    {
      "epoch": 3.13175,
      "grad_norm": 4.994760513305664,
      "learning_rate": 1.9550453161893566e-05,
      "loss": 0.7778,
      "step": 11830
    },
    {
      "epoch": 3.132,
      "grad_norm": 4.260490417480469,
      "learning_rate": 1.953028366994936e-05,
      "loss": 0.7308,
      "step": 11835
    },
    {
      "epoch": 3.13225,
      "grad_norm": 6.113140106201172,
      "learning_rate": 1.95101179165047e-05,
      "loss": 0.772,
      "step": 11840
    },
    {
      "epoch": 3.1325,
      "grad_norm": 3.727163553237915,
      "learning_rate": 1.9489955915342673e-05,
      "loss": 0.7472,
      "step": 11845
    },
    {
      "epoch": 3.13275,
      "grad_norm": 4.076592922210693,
      "learning_rate": 1.9469797680243828e-05,
      "loss": 0.7565,
      "step": 11850
    },
    {
      "epoch": 3.133,
      "grad_norm": 3.261106014251709,
      "learning_rate": 1.9449643224986115e-05,
      "loss": 0.6152,
      "step": 11855
    },
    {
      "epoch": 3.13325,
      "grad_norm": 6.831793308258057,
      "learning_rate": 1.942949256334492e-05,
      "loss": 0.8245,
      "step": 11860
    },
    {
      "epoch": 3.1335,
      "grad_norm": 3.6160457134246826,
      "learning_rate": 1.9409345709093036e-05,
      "loss": 0.6377,
      "step": 11865
    },
    {
      "epoch": 3.13375,
      "grad_norm": 3.192051410675049,
      "learning_rate": 1.9389202676000633e-05,
      "loss": 0.5803,
      "step": 11870
    },
    {
      "epoch": 3.134,
      "grad_norm": 4.272932529449463,
      "learning_rate": 1.9369063477835297e-05,
      "loss": 0.7131,
      "step": 11875
    },
    {
      "epoch": 3.13425,
      "grad_norm": 4.298463344573975,
      "learning_rate": 1.9348928128361985e-05,
      "loss": 0.7255,
      "step": 11880
    },
    {
      "epoch": 3.1345,
      "grad_norm": 3.3434956073760986,
      "learning_rate": 1.9328796641343006e-05,
      "loss": 0.7494,
      "step": 11885
    },
    {
      "epoch": 3.13475,
      "grad_norm": 5.315514087677002,
      "learning_rate": 1.9308669030538047e-05,
      "loss": 0.7869,
      "step": 11890
    },
    {
      "epoch": 3.135,
      "grad_norm": 4.869283676147461,
      "learning_rate": 1.928854530970415e-05,
      "loss": 0.8051,
      "step": 11895
    },
    {
      "epoch": 3.13525,
      "grad_norm": 7.3212761878967285,
      "learning_rate": 1.926842549259566e-05,
      "loss": 0.8322,
      "step": 11900
    },
    {
      "epoch": 3.1355,
      "grad_norm": 4.444051265716553,
      "learning_rate": 1.9248309592964315e-05,
      "loss": 0.8083,
      "step": 11905
    },
    {
      "epoch": 3.13575,
      "grad_norm": 5.392923355102539,
      "learning_rate": 1.9228197624559115e-05,
      "loss": 0.8468,
      "step": 11910
    },
    {
      "epoch": 3.136,
      "grad_norm": 5.41290283203125,
      "learning_rate": 1.9208089601126425e-05,
      "loss": 0.7194,
      "step": 11915
    },
    {
      "epoch": 3.13625,
      "grad_norm": 4.288748741149902,
      "learning_rate": 1.918798553640987e-05,
      "loss": 0.7623,
      "step": 11920
    },
    {
      "epoch": 3.1365,
      "grad_norm": 4.626491069793701,
      "learning_rate": 1.9167885444150395e-05,
      "loss": 0.8795,
      "step": 11925
    },
    {
      "epoch": 3.13675,
      "grad_norm": 4.4327073097229,
      "learning_rate": 1.9147789338086235e-05,
      "loss": 0.8017,
      "step": 11930
    },
    {
      "epoch": 3.137,
      "grad_norm": 6.016772747039795,
      "learning_rate": 1.912769723195287e-05,
      "loss": 0.8881,
      "step": 11935
    },
    {
      "epoch": 3.13725,
      "grad_norm": 4.571079254150391,
      "learning_rate": 1.910760913948309e-05,
      "loss": 0.8878,
      "step": 11940
    },
    {
      "epoch": 3.1375,
      "grad_norm": 4.438422679901123,
      "learning_rate": 1.908752507440689e-05,
      "loss": 0.6724,
      "step": 11945
    },
    {
      "epoch": 3.13775,
      "grad_norm": 6.104885101318359,
      "learning_rate": 1.906744505045155e-05,
      "loss": 0.7826,
      "step": 11950
    },
    {
      "epoch": 3.138,
      "grad_norm": 3.951505661010742,
      "learning_rate": 1.904736908134158e-05,
      "loss": 0.8332,
      "step": 11955
    },
    {
      "epoch": 3.13825,
      "grad_norm": 5.691518306732178,
      "learning_rate": 1.9027297180798707e-05,
      "loss": 0.7616,
      "step": 11960
    },
    {
      "epoch": 3.1385,
      "grad_norm": 5.973130226135254,
      "learning_rate": 1.9007229362541898e-05,
      "loss": 0.9177,
      "step": 11965
    },
    {
      "epoch": 3.13875,
      "grad_norm": 4.9425201416015625,
      "learning_rate": 1.8987165640287303e-05,
      "loss": 0.9062,
      "step": 11970
    },
    {
      "epoch": 3.1390000000000002,
      "grad_norm": 4.351063251495361,
      "learning_rate": 1.8967106027748298e-05,
      "loss": 0.7615,
      "step": 11975
    },
    {
      "epoch": 3.13925,
      "grad_norm": 4.643908977508545,
      "learning_rate": 1.8947050538635444e-05,
      "loss": 0.8265,
      "step": 11980
    },
    {
      "epoch": 3.1395,
      "grad_norm": 4.037263870239258,
      "learning_rate": 1.8926999186656464e-05,
      "loss": 0.7254,
      "step": 11985
    },
    {
      "epoch": 3.13975,
      "grad_norm": 4.231268882751465,
      "learning_rate": 1.8906951985516285e-05,
      "loss": 0.807,
      "step": 11990
    },
    {
      "epoch": 3.14,
      "grad_norm": 5.818610191345215,
      "learning_rate": 1.8886908948916966e-05,
      "loss": 0.7079,
      "step": 11995
    },
    {
      "epoch": 3.14025,
      "grad_norm": 5.556854724884033,
      "learning_rate": 1.8866870090557727e-05,
      "loss": 0.8272,
      "step": 12000
    },
    {
      "epoch": 3.14025,
      "eval_loss": 2.134965181350708,
      "eval_runtime": 5.9014,
      "eval_samples_per_second": 173.518,
      "eval_steps_per_second": 21.69,
      "step": 12000
    },
    {
      "epoch": 3.1405,
      "grad_norm": 4.289228916168213,
      "learning_rate": 1.8846835424134963e-05,
      "loss": 0.7172,
      "step": 12005
    },
    {
      "epoch": 3.14075,
      "grad_norm": 5.129300594329834,
      "learning_rate": 1.8826804963342153e-05,
      "loss": 0.7736,
      "step": 12010
    },
    {
      "epoch": 3.141,
      "grad_norm": 4.362031936645508,
      "learning_rate": 1.8806778721869945e-05,
      "loss": 0.7252,
      "step": 12015
    },
    {
      "epoch": 3.14125,
      "grad_norm": 4.989051818847656,
      "learning_rate": 1.8786756713406084e-05,
      "loss": 0.7802,
      "step": 12020
    },
    {
      "epoch": 3.1415,
      "grad_norm": 4.202798366546631,
      "learning_rate": 1.876673895163542e-05,
      "loss": 0.6887,
      "step": 12025
    },
    {
      "epoch": 3.14175,
      "grad_norm": 4.204812049865723,
      "learning_rate": 1.874672545023991e-05,
      "loss": 0.6725,
      "step": 12030
    },
    {
      "epoch": 3.142,
      "grad_norm": 4.701841354370117,
      "learning_rate": 1.8726716222898592e-05,
      "loss": 0.7989,
      "step": 12035
    },
    {
      "epoch": 3.1422499999999998,
      "grad_norm": 6.774209499359131,
      "learning_rate": 1.8706711283287576e-05,
      "loss": 0.779,
      "step": 12040
    },
    {
      "epoch": 3.1425,
      "grad_norm": 3.9091079235076904,
      "learning_rate": 1.868671064508006e-05,
      "loss": 0.6832,
      "step": 12045
    },
    {
      "epoch": 3.14275,
      "grad_norm": 3.0339009761810303,
      "learning_rate": 1.8666714321946283e-05,
      "loss": 0.746,
      "step": 12050
    },
    {
      "epoch": 3.143,
      "grad_norm": 4.069052219390869,
      "learning_rate": 1.8646722327553552e-05,
      "loss": 0.6554,
      "step": 12055
    },
    {
      "epoch": 3.14325,
      "grad_norm": 4.033178806304932,
      "learning_rate": 1.8626734675566196e-05,
      "loss": 0.7041,
      "step": 12060
    },
    {
      "epoch": 3.1435,
      "grad_norm": 5.602537155151367,
      "learning_rate": 1.8606751379645593e-05,
      "loss": 0.8909,
      "step": 12065
    },
    {
      "epoch": 3.14375,
      "grad_norm": 5.4228620529174805,
      "learning_rate": 1.8586772453450143e-05,
      "loss": 0.7285,
      "step": 12070
    },
    {
      "epoch": 3.144,
      "grad_norm": 4.493231773376465,
      "learning_rate": 1.8566797910635243e-05,
      "loss": 0.6917,
      "step": 12075
    },
    {
      "epoch": 3.14425,
      "grad_norm": 3.2694685459136963,
      "learning_rate": 1.854682776485332e-05,
      "loss": 0.7678,
      "step": 12080
    },
    {
      "epoch": 3.1445,
      "grad_norm": 4.456009387969971,
      "learning_rate": 1.8526862029753767e-05,
      "loss": 0.6236,
      "step": 12085
    },
    {
      "epoch": 3.14475,
      "grad_norm": 3.726771593093872,
      "learning_rate": 1.8506900718982972e-05,
      "loss": 0.6462,
      "step": 12090
    },
    {
      "epoch": 3.145,
      "grad_norm": 5.0792155265808105,
      "learning_rate": 1.8486943846184314e-05,
      "loss": 0.7313,
      "step": 12095
    },
    {
      "epoch": 3.14525,
      "grad_norm": 3.349839687347412,
      "learning_rate": 1.8466991424998114e-05,
      "loss": 0.5752,
      "step": 12100
    },
    {
      "epoch": 3.1455,
      "grad_norm": 5.818941116333008,
      "learning_rate": 1.844704346906168e-05,
      "loss": 0.686,
      "step": 12105
    },
    {
      "epoch": 3.14575,
      "grad_norm": 3.891427993774414,
      "learning_rate": 1.8427099992009242e-05,
      "loss": 0.7543,
      "step": 12110
    },
    {
      "epoch": 3.146,
      "grad_norm": 4.066272735595703,
      "learning_rate": 1.8407161007471978e-05,
      "loss": 0.6405,
      "step": 12115
    },
    {
      "epoch": 3.14625,
      "grad_norm": 4.0686235427856445,
      "learning_rate": 1.8387226529078006e-05,
      "loss": 0.6563,
      "step": 12120
    },
    {
      "epoch": 3.1465,
      "grad_norm": 9.465550422668457,
      "learning_rate": 1.8367296570452338e-05,
      "loss": 1.4914,
      "step": 12125
    },
    {
      "epoch": 3.14675,
      "grad_norm": 9.243069648742676,
      "learning_rate": 1.8347371145216935e-05,
      "loss": 1.7167,
      "step": 12130
    },
    {
      "epoch": 3.147,
      "grad_norm": 8.770210266113281,
      "learning_rate": 1.8327450266990616e-05,
      "loss": 1.518,
      "step": 12135
    },
    {
      "epoch": 3.14725,
      "grad_norm": 8.46898365020752,
      "learning_rate": 1.8307533949389117e-05,
      "loss": 1.3989,
      "step": 12140
    },
    {
      "epoch": 3.1475,
      "grad_norm": 7.826614856719971,
      "learning_rate": 1.828762220602506e-05,
      "loss": 1.3176,
      "step": 12145
    },
    {
      "epoch": 3.14775,
      "grad_norm": 9.539278030395508,
      "learning_rate": 1.8267715050507933e-05,
      "loss": 1.2751,
      "step": 12150
    },
    {
      "epoch": 3.148,
      "grad_norm": 7.794161796569824,
      "learning_rate": 1.824781249644409e-05,
      "loss": 1.2665,
      "step": 12155
    },
    {
      "epoch": 3.14825,
      "grad_norm": 7.675650596618652,
      "learning_rate": 1.8227914557436727e-05,
      "loss": 1.2024,
      "step": 12160
    },
    {
      "epoch": 3.1485,
      "grad_norm": 6.272205829620361,
      "learning_rate": 1.820802124708591e-05,
      "loss": 1.2666,
      "step": 12165
    },
    {
      "epoch": 3.14875,
      "grad_norm": 12.050131797790527,
      "learning_rate": 1.818813257898853e-05,
      "loss": 1.2857,
      "step": 12170
    },
    {
      "epoch": 3.149,
      "grad_norm": 6.3474931716918945,
      "learning_rate": 1.8168248566738308e-05,
      "loss": 1.2453,
      "step": 12175
    },
    {
      "epoch": 3.14925,
      "grad_norm": 5.458146095275879,
      "learning_rate": 1.814836922392576e-05,
      "loss": 1.1631,
      "step": 12180
    },
    {
      "epoch": 3.1495,
      "grad_norm": 6.655935764312744,
      "learning_rate": 1.8128494564138242e-05,
      "loss": 1.2198,
      "step": 12185
    },
    {
      "epoch": 3.14975,
      "grad_norm": 5.767645835876465,
      "learning_rate": 1.8108624600959895e-05,
      "loss": 1.1402,
      "step": 12190
    },
    {
      "epoch": 3.15,
      "grad_norm": 6.711587429046631,
      "learning_rate": 1.808875934797165e-05,
      "loss": 1.1331,
      "step": 12195
    },
    {
      "epoch": 3.1502499999999998,
      "grad_norm": 5.583449840545654,
      "learning_rate": 1.8068898818751222e-05,
      "loss": 1.1272,
      "step": 12200
    },
    {
      "epoch": 3.1505,
      "grad_norm": 7.497832775115967,
      "learning_rate": 1.8049043026873096e-05,
      "loss": 1.2588,
      "step": 12205
    },
    {
      "epoch": 3.15075,
      "grad_norm": 8.839014053344727,
      "learning_rate": 1.8029191985908518e-05,
      "loss": 1.1858,
      "step": 12210
    },
    {
      "epoch": 3.151,
      "grad_norm": 6.721787452697754,
      "learning_rate": 1.8009345709425486e-05,
      "loss": 1.2123,
      "step": 12215
    },
    {
      "epoch": 3.15125,
      "grad_norm": 6.2058634757995605,
      "learning_rate": 1.798950421098875e-05,
      "loss": 1.3694,
      "step": 12220
    },
    {
      "epoch": 3.1515,
      "grad_norm": 5.545412540435791,
      "learning_rate": 1.7969667504159775e-05,
      "loss": 1.2393,
      "step": 12225
    },
    {
      "epoch": 3.15175,
      "grad_norm": 8.078967094421387,
      "learning_rate": 1.7949835602496766e-05,
      "loss": 1.2131,
      "step": 12230
    },
    {
      "epoch": 3.152,
      "grad_norm": 9.751370429992676,
      "learning_rate": 1.793000851955465e-05,
      "loss": 1.3137,
      "step": 12235
    },
    {
      "epoch": 3.15225,
      "grad_norm": 10.986952781677246,
      "learning_rate": 1.7910186268885037e-05,
      "loss": 1.2143,
      "step": 12240
    },
    {
      "epoch": 3.1525,
      "grad_norm": 12.323747634887695,
      "learning_rate": 1.7890368864036266e-05,
      "loss": 1.1847,
      "step": 12245
    },
    {
      "epoch": 3.15275,
      "grad_norm": 8.969053268432617,
      "learning_rate": 1.7870556318553327e-05,
      "loss": 1.3457,
      "step": 12250
    },
    {
      "epoch": 3.153,
      "grad_norm": 7.7210822105407715,
      "learning_rate": 1.7850748645977923e-05,
      "loss": 1.6289,
      "step": 12255
    },
    {
      "epoch": 3.15325,
      "grad_norm": 6.37969446182251,
      "learning_rate": 1.7830945859848402e-05,
      "loss": 1.007,
      "step": 12260
    },
    {
      "epoch": 4.00025,
      "grad_norm": 5.859982967376709,
      "learning_rate": 1.7811147973699787e-05,
      "loss": 0.8357,
      "step": 12265
    },
    {
      "epoch": 4.0005,
      "grad_norm": 5.5415940284729,
      "learning_rate": 1.7791355001063752e-05,
      "loss": 0.7997,
      "step": 12270
    },
    {
      "epoch": 4.00075,
      "grad_norm": 4.843841552734375,
      "learning_rate": 1.77715669554686e-05,
      "loss": 0.8121,
      "step": 12275
    },
    {
      "epoch": 4.001,
      "grad_norm": 7.305495262145996,
      "learning_rate": 1.7751783850439265e-05,
      "loss": 0.7492,
      "step": 12280
    },
    {
      "epoch": 4.00125,
      "grad_norm": 5.8747239112854,
      "learning_rate": 1.7732005699497327e-05,
      "loss": 0.869,
      "step": 12285
    },
    {
      "epoch": 4.0015,
      "grad_norm": 5.767449378967285,
      "learning_rate": 1.7712232516160957e-05,
      "loss": 0.8676,
      "step": 12290
    },
    {
      "epoch": 4.00175,
      "grad_norm": 4.918191432952881,
      "learning_rate": 1.7692464313944944e-05,
      "loss": 0.712,
      "step": 12295
    },
    {
      "epoch": 4.002,
      "grad_norm": 5.146254539489746,
      "learning_rate": 1.7672701106360667e-05,
      "loss": 0.762,
      "step": 12300
    },
    {
      "epoch": 4.00225,
      "grad_norm": 5.3681321144104,
      "learning_rate": 1.7652942906916093e-05,
      "loss": 0.7599,
      "step": 12305
    },
    {
      "epoch": 4.0025,
      "grad_norm": 8.094789505004883,
      "learning_rate": 1.763318972911576e-05,
      "loss": 0.9186,
      "step": 12310
    },
    {
      "epoch": 4.00275,
      "grad_norm": 8.084534645080566,
      "learning_rate": 1.7613441586460793e-05,
      "loss": 0.9332,
      "step": 12315
    },
    {
      "epoch": 4.003,
      "grad_norm": 7.824738502502441,
      "learning_rate": 1.7593698492448837e-05,
      "loss": 0.9403,
      "step": 12320
    },
    {
      "epoch": 4.00325,
      "grad_norm": 6.2032952308654785,
      "learning_rate": 1.7573960460574133e-05,
      "loss": 0.9413,
      "step": 12325
    },
    {
      "epoch": 4.0035,
      "grad_norm": 4.928696155548096,
      "learning_rate": 1.7554227504327424e-05,
      "loss": 0.8186,
      "step": 12330
    },
    {
      "epoch": 4.00375,
      "grad_norm": 4.43587064743042,
      "learning_rate": 1.7534499637196012e-05,
      "loss": 0.7416,
      "step": 12335
    },
    {
      "epoch": 4.004,
      "grad_norm": 6.987716197967529,
      "learning_rate": 1.7514776872663695e-05,
      "loss": 0.8402,
      "step": 12340
    },
    {
      "epoch": 4.00425,
      "grad_norm": 10.742378234863281,
      "learning_rate": 1.7495059224210813e-05,
      "loss": 0.9229,
      "step": 12345
    },
    {
      "epoch": 4.0045,
      "grad_norm": 7.336915493011475,
      "learning_rate": 1.747534670531418e-05,
      "loss": 0.8595,
      "step": 12350
    },
    {
      "epoch": 4.00475,
      "grad_norm": 8.822477340698242,
      "learning_rate": 1.7455639329447118e-05,
      "loss": 0.9657,
      "step": 12355
    },
    {
      "epoch": 4.005,
      "grad_norm": 7.050124168395996,
      "learning_rate": 1.7435937110079448e-05,
      "loss": 0.9665,
      "step": 12360
    },
    {
      "epoch": 4.00525,
      "grad_norm": 6.685603618621826,
      "learning_rate": 1.7416240060677426e-05,
      "loss": 0.9486,
      "step": 12365
    },
    {
      "epoch": 4.0055,
      "grad_norm": 6.5103373527526855,
      "learning_rate": 1.739654819470381e-05,
      "loss": 1.0098,
      "step": 12370
    },
    {
      "epoch": 4.00575,
      "grad_norm": 7.331483840942383,
      "learning_rate": 1.737686152561781e-05,
      "loss": 0.9129,
      "step": 12375
    },
    {
      "epoch": 4.006,
      "grad_norm": 7.061288833618164,
      "learning_rate": 1.7357180066875073e-05,
      "loss": 0.9656,
      "step": 12380
    },
    {
      "epoch": 4.00625,
      "grad_norm": 6.801433563232422,
      "learning_rate": 1.73375038319277e-05,
      "loss": 0.9227,
      "step": 12385
    },
    {
      "epoch": 4.0065,
      "grad_norm": 6.816232204437256,
      "learning_rate": 1.7317832834224196e-05,
      "loss": 0.9836,
      "step": 12390
    },
    {
      "epoch": 4.00675,
      "grad_norm": 6.022160530090332,
      "learning_rate": 1.7298167087209522e-05,
      "loss": 0.8573,
      "step": 12395
    },
    {
      "epoch": 4.007,
      "grad_norm": 6.570262908935547,
      "learning_rate": 1.7278506604325008e-05,
      "loss": 0.8271,
      "step": 12400
    },
    {
      "epoch": 4.00725,
      "grad_norm": 6.67108154296875,
      "learning_rate": 1.725885139900843e-05,
      "loss": 0.9015,
      "step": 12405
    },
    {
      "epoch": 4.0075,
      "grad_norm": 5.965832710266113,
      "learning_rate": 1.7239201484693928e-05,
      "loss": 0.9728,
      "step": 12410
    },
    {
      "epoch": 4.00775,
      "grad_norm": 7.6633219718933105,
      "learning_rate": 1.7219556874812026e-05,
      "loss": 0.9234,
      "step": 12415
    },
    {
      "epoch": 4.008,
      "grad_norm": 5.986259460449219,
      "learning_rate": 1.7199917582789633e-05,
      "loss": 0.8916,
      "step": 12420
    },
    {
      "epoch": 4.00825,
      "grad_norm": 6.691920280456543,
      "learning_rate": 1.7180283622050025e-05,
      "loss": 0.9119,
      "step": 12425
    },
    {
      "epoch": 4.0085,
      "grad_norm": 8.814393043518066,
      "learning_rate": 1.7160655006012823e-05,
      "loss": 0.8463,
      "step": 12430
    },
    {
      "epoch": 4.00875,
      "grad_norm": 6.066495895385742,
      "learning_rate": 1.7141031748094006e-05,
      "loss": 0.8336,
      "step": 12435
    },
    {
      "epoch": 4.009,
      "grad_norm": 7.814515113830566,
      "learning_rate": 1.712141386170588e-05,
      "loss": 0.9111,
      "step": 12440
    },
    {
      "epoch": 4.00925,
      "grad_norm": 5.543437957763672,
      "learning_rate": 1.710180136025709e-05,
      "loss": 0.8284,
      "step": 12445
    },
    {
      "epoch": 4.0095,
      "grad_norm": 6.930166721343994,
      "learning_rate": 1.7082194257152593e-05,
      "loss": 0.8149,
      "step": 12450
    },
    {
      "epoch": 4.00975,
      "grad_norm": 7.602517127990723,
      "learning_rate": 1.706259256579367e-05,
      "loss": 0.9551,
      "step": 12455
    },
    {
      "epoch": 4.01,
      "grad_norm": 5.643614292144775,
      "learning_rate": 1.7042996299577886e-05,
      "loss": 0.8387,
      "step": 12460
    },
    {
      "epoch": 4.01025,
      "grad_norm": 6.5672760009765625,
      "learning_rate": 1.7023405471899088e-05,
      "loss": 0.8712,
      "step": 12465
    },
    {
      "epoch": 4.0105,
      "grad_norm": 5.210174560546875,
      "learning_rate": 1.7003820096147434e-05,
      "loss": 0.8962,
      "step": 12470
    },
    {
      "epoch": 4.01075,
      "grad_norm": 6.7683329582214355,
      "learning_rate": 1.6984240185709353e-05,
      "loss": 0.7884,
      "step": 12475
    },
    {
      "epoch": 4.011,
      "grad_norm": 5.767886161804199,
      "learning_rate": 1.6964665753967518e-05,
      "loss": 0.8104,
      "step": 12480
    },
    {
      "epoch": 4.01125,
      "grad_norm": 5.162835597991943,
      "learning_rate": 1.6945096814300874e-05,
      "loss": 0.7928,
      "step": 12485
    },
    {
      "epoch": 4.0115,
      "grad_norm": 5.6613593101501465,
      "learning_rate": 1.6925533380084598e-05,
      "loss": 0.8223,
      "step": 12490
    },
    {
      "epoch": 4.01175,
      "grad_norm": 6.569499969482422,
      "learning_rate": 1.6905975464690127e-05,
      "loss": 0.9003,
      "step": 12495
    },
    {
      "epoch": 4.012,
      "grad_norm": 5.642648696899414,
      "learning_rate": 1.6886423081485106e-05,
      "loss": 0.8954,
      "step": 12500
    },
    {
      "epoch": 4.012,
      "eval_loss": 2.0355124473571777,
      "eval_runtime": 5.3671,
      "eval_samples_per_second": 190.794,
      "eval_steps_per_second": 23.849,
      "step": 12500
    },
    {
      "epoch": 4.01225,
      "grad_norm": 6.0009026527404785,
      "learning_rate": 1.6866876243833393e-05,
      "loss": 1.0034,
      "step": 12505
    },
    {
      "epoch": 4.0125,
      "grad_norm": 5.838094234466553,
      "learning_rate": 1.6847334965095083e-05,
      "loss": 0.8945,
      "step": 12510
    },
    {
      "epoch": 4.01275,
      "grad_norm": 5.951459884643555,
      "learning_rate": 1.6827799258626442e-05,
      "loss": 0.8248,
      "step": 12515
    },
    {
      "epoch": 4.013,
      "grad_norm": 6.409419536590576,
      "learning_rate": 1.680826913777995e-05,
      "loss": 0.8684,
      "step": 12520
    },
    {
      "epoch": 4.01325,
      "grad_norm": 5.978057384490967,
      "learning_rate": 1.678874461590426e-05,
      "loss": 0.8675,
      "step": 12525
    },
    {
      "epoch": 4.0135,
      "grad_norm": 5.545065879821777,
      "learning_rate": 1.67692257063442e-05,
      "loss": 0.7648,
      "step": 12530
    },
    {
      "epoch": 4.01375,
      "grad_norm": 5.289519309997559,
      "learning_rate": 1.674971242244076e-05,
      "loss": 0.8093,
      "step": 12535
    },
    {
      "epoch": 4.014,
      "grad_norm": 7.090638637542725,
      "learning_rate": 1.6730204777531077e-05,
      "loss": 0.7119,
      "step": 12540
    },
    {
      "epoch": 4.01425,
      "grad_norm": 4.602142333984375,
      "learning_rate": 1.6710702784948456e-05,
      "loss": 0.6602,
      "step": 12545
    },
    {
      "epoch": 4.0145,
      "grad_norm": 3.575718641281128,
      "learning_rate": 1.6691206458022323e-05,
      "loss": 0.5833,
      "step": 12550
    },
    {
      "epoch": 4.01475,
      "grad_norm": 4.203227996826172,
      "learning_rate": 1.6671715810078217e-05,
      "loss": 0.5838,
      "step": 12555
    },
    {
      "epoch": 4.015,
      "grad_norm": 2.9082274436950684,
      "learning_rate": 1.665223085443783e-05,
      "loss": 0.4953,
      "step": 12560
    },
    {
      "epoch": 4.01525,
      "grad_norm": 3.3459537029266357,
      "learning_rate": 1.6632751604418934e-05,
      "loss": 0.5398,
      "step": 12565
    },
    {
      "epoch": 4.0155,
      "grad_norm": 4.022373199462891,
      "learning_rate": 1.6613278073335417e-05,
      "loss": 0.4498,
      "step": 12570
    },
    {
      "epoch": 4.01575,
      "grad_norm": 2.6193740367889404,
      "learning_rate": 1.6593810274497263e-05,
      "loss": 0.3796,
      "step": 12575
    },
    {
      "epoch": 4.016,
      "grad_norm": 1.9958451986312866,
      "learning_rate": 1.657434822121051e-05,
      "loss": 0.4104,
      "step": 12580
    },
    {
      "epoch": 4.01625,
      "grad_norm": 2.186871290206909,
      "learning_rate": 1.6554891926777306e-05,
      "loss": 0.432,
      "step": 12585
    },
    {
      "epoch": 4.0165,
      "grad_norm": 9.054025650024414,
      "learning_rate": 1.6535441404495837e-05,
      "loss": 0.5117,
      "step": 12590
    },
    {
      "epoch": 4.01675,
      "grad_norm": 7.563310146331787,
      "learning_rate": 1.651599666766036e-05,
      "loss": 0.911,
      "step": 12595
    },
    {
      "epoch": 4.017,
      "grad_norm": 9.536293029785156,
      "learning_rate": 1.6496557729561153e-05,
      "loss": 0.9201,
      "step": 12600
    },
    {
      "epoch": 4.01725,
      "grad_norm": 8.753608703613281,
      "learning_rate": 1.6477124603484552e-05,
      "loss": 1.0053,
      "step": 12605
    },
    {
      "epoch": 4.0175,
      "grad_norm": 8.329896926879883,
      "learning_rate": 1.6457697302712918e-05,
      "loss": 0.8963,
      "step": 12610
    },
    {
      "epoch": 4.01775,
      "grad_norm": 7.768951892852783,
      "learning_rate": 1.6438275840524634e-05,
      "loss": 0.8774,
      "step": 12615
    },
    {
      "epoch": 4.018,
      "grad_norm": 6.56020975112915,
      "learning_rate": 1.641886023019408e-05,
      "loss": 0.8972,
      "step": 12620
    },
    {
      "epoch": 4.01825,
      "grad_norm": 7.442455768585205,
      "learning_rate": 1.6399450484991642e-05,
      "loss": 0.9508,
      "step": 12625
    },
    {
      "epoch": 4.0185,
      "grad_norm": 5.414704322814941,
      "learning_rate": 1.6380046618183695e-05,
      "loss": 0.9149,
      "step": 12630
    },
    {
      "epoch": 4.01875,
      "grad_norm": 5.740363597869873,
      "learning_rate": 1.6360648643032613e-05,
      "loss": 0.8326,
      "step": 12635
    },
    {
      "epoch": 4.019,
      "grad_norm": 6.303509712219238,
      "learning_rate": 1.634125657279672e-05,
      "loss": 0.8742,
      "step": 12640
    },
    {
      "epoch": 4.01925,
      "grad_norm": 5.083320140838623,
      "learning_rate": 1.63218704207303e-05,
      "loss": 0.8662,
      "step": 12645
    },
    {
      "epoch": 4.0195,
      "grad_norm": 6.457040309906006,
      "learning_rate": 1.6302490200083627e-05,
      "loss": 0.8957,
      "step": 12650
    },
    {
      "epoch": 4.01975,
      "grad_norm": 6.122378826141357,
      "learning_rate": 1.628311592410288e-05,
      "loss": 0.7858,
      "step": 12655
    },
    {
      "epoch": 4.02,
      "grad_norm": 6.541699409484863,
      "learning_rate": 1.626374760603021e-05,
      "loss": 0.8849,
      "step": 12660
    },
    {
      "epoch": 4.02025,
      "grad_norm": 8.128742218017578,
      "learning_rate": 1.6244385259103674e-05,
      "loss": 0.9789,
      "step": 12665
    },
    {
      "epoch": 4.0205,
      "grad_norm": 5.511203765869141,
      "learning_rate": 1.6225028896557247e-05,
      "loss": 0.9271,
      "step": 12670
    },
    {
      "epoch": 4.02075,
      "grad_norm": 7.267895698547363,
      "learning_rate": 1.620567853162084e-05,
      "loss": 0.8436,
      "step": 12675
    },
    {
      "epoch": 4.021,
      "grad_norm": 9.625333786010742,
      "learning_rate": 1.6186334177520217e-05,
      "loss": 0.9554,
      "step": 12680
    },
    {
      "epoch": 4.02125,
      "grad_norm": 5.008791446685791,
      "learning_rate": 1.6166995847477083e-05,
      "loss": 0.8393,
      "step": 12685
    },
    {
      "epoch": 4.0215,
      "grad_norm": 6.886652946472168,
      "learning_rate": 1.6147663554709002e-05,
      "loss": 0.8731,
      "step": 12690
    },
    {
      "epoch": 4.02175,
      "grad_norm": 8.000205039978027,
      "learning_rate": 1.6128337312429397e-05,
      "loss": 0.8573,
      "step": 12695
    },
    {
      "epoch": 4.022,
      "grad_norm": 6.67234992980957,
      "learning_rate": 1.610901713384759e-05,
      "loss": 0.848,
      "step": 12700
    },
    {
      "epoch": 4.02225,
      "grad_norm": 8.421037673950195,
      "learning_rate": 1.6089703032168733e-05,
      "loss": 0.9475,
      "step": 12705
    },
    {
      "epoch": 4.0225,
      "grad_norm": 5.35003662109375,
      "learning_rate": 1.607039502059384e-05,
      "loss": 0.992,
      "step": 12710
    },
    {
      "epoch": 4.02275,
      "grad_norm": 6.303660869598389,
      "learning_rate": 1.6051093112319744e-05,
      "loss": 0.8637,
      "step": 12715
    },
    {
      "epoch": 4.023,
      "grad_norm": 6.88604736328125,
      "learning_rate": 1.6031797320539117e-05,
      "loss": 0.8702,
      "step": 12720
    },
    {
      "epoch": 4.02325,
      "grad_norm": 6.352888584136963,
      "learning_rate": 1.6012507658440467e-05,
      "loss": 0.7868,
      "step": 12725
    },
    {
      "epoch": 4.0235,
      "grad_norm": 6.067524433135986,
      "learning_rate": 1.599322413920808e-05,
      "loss": 0.9016,
      "step": 12730
    },
    {
      "epoch": 4.02375,
      "grad_norm": 5.736491680145264,
      "learning_rate": 1.597394677602207e-05,
      "loss": 0.7855,
      "step": 12735
    },
    {
      "epoch": 4.024,
      "grad_norm": 6.265193462371826,
      "learning_rate": 1.5954675582058327e-05,
      "loss": 0.9077,
      "step": 12740
    },
    {
      "epoch": 4.02425,
      "grad_norm": 5.864992141723633,
      "learning_rate": 1.593541057048853e-05,
      "loss": 0.784,
      "step": 12745
    },
    {
      "epoch": 4.0245,
      "grad_norm": 6.992696762084961,
      "learning_rate": 1.591615175448013e-05,
      "loss": 1.0423,
      "step": 12750
    },
    {
      "epoch": 4.02475,
      "grad_norm": 6.673402786254883,
      "learning_rate": 1.5896899147196353e-05,
      "loss": 0.8363,
      "step": 12755
    },
    {
      "epoch": 4.025,
      "grad_norm": 7.92485237121582,
      "learning_rate": 1.5877652761796168e-05,
      "loss": 0.8997,
      "step": 12760
    },
    {
      "epoch": 4.02525,
      "grad_norm": 4.485831260681152,
      "learning_rate": 1.5858412611434307e-05,
      "loss": 0.8657,
      "step": 12765
    },
    {
      "epoch": 4.0255,
      "grad_norm": 8.00675106048584,
      "learning_rate": 1.5839178709261225e-05,
      "loss": 0.9559,
      "step": 12770
    },
    {
      "epoch": 4.02575,
      "grad_norm": 7.546976089477539,
      "learning_rate": 1.5819951068423115e-05,
      "loss": 0.9179,
      "step": 12775
    },
    {
      "epoch": 4.026,
      "grad_norm": 6.368626594543457,
      "learning_rate": 1.580072970206189e-05,
      "loss": 0.8098,
      "step": 12780
    },
    {
      "epoch": 4.02625,
      "grad_norm": 6.2308573722839355,
      "learning_rate": 1.5781514623315164e-05,
      "loss": 0.9768,
      "step": 12785
    },
    {
      "epoch": 4.0265,
      "grad_norm": 5.872567653656006,
      "learning_rate": 1.5762305845316273e-05,
      "loss": 0.788,
      "step": 12790
    },
    {
      "epoch": 4.02675,
      "grad_norm": 6.47174072265625,
      "learning_rate": 1.5743103381194225e-05,
      "loss": 0.9704,
      "step": 12795
    },
    {
      "epoch": 4.027,
      "grad_norm": 7.040425777435303,
      "learning_rate": 1.572390724407373e-05,
      "loss": 0.8844,
      "step": 12800
    },
    {
      "epoch": 4.02725,
      "grad_norm": 6.351879119873047,
      "learning_rate": 1.5704717447075162e-05,
      "loss": 0.8593,
      "step": 12805
    },
    {
      "epoch": 4.0275,
      "grad_norm": 6.00531005859375,
      "learning_rate": 1.5685534003314573e-05,
      "loss": 0.8101,
      "step": 12810
    },
    {
      "epoch": 4.02775,
      "grad_norm": 6.309788703918457,
      "learning_rate": 1.5666356925903664e-05,
      "loss": 0.8618,
      "step": 12815
    },
    {
      "epoch": 4.028,
      "grad_norm": 7.770884990692139,
      "learning_rate": 1.564718622794978e-05,
      "loss": 0.9042,
      "step": 12820
    },
    {
      "epoch": 4.02825,
      "grad_norm": 7.7722039222717285,
      "learning_rate": 1.562802192255593e-05,
      "loss": 0.943,
      "step": 12825
    },
    {
      "epoch": 4.0285,
      "grad_norm": 6.429872512817383,
      "learning_rate": 1.5608864022820715e-05,
      "loss": 0.8046,
      "step": 12830
    },
    {
      "epoch": 4.02875,
      "grad_norm": 8.071022033691406,
      "learning_rate": 1.5589712541838384e-05,
      "loss": 0.9006,
      "step": 12835
    },
    {
      "epoch": 4.029,
      "grad_norm": 6.735805034637451,
      "learning_rate": 1.55705674926988e-05,
      "loss": 0.8806,
      "step": 12840
    },
    {
      "epoch": 4.02925,
      "grad_norm": 5.7332892417907715,
      "learning_rate": 1.5551428888487422e-05,
      "loss": 0.8598,
      "step": 12845
    },
    {
      "epoch": 4.0295,
      "grad_norm": 3.9381799697875977,
      "learning_rate": 1.55322967422853e-05,
      "loss": 0.7743,
      "step": 12850
    },
    {
      "epoch": 4.02975,
      "grad_norm": 7.3586955070495605,
      "learning_rate": 1.5513171067169082e-05,
      "loss": 0.8427,
      "step": 12855
    },
    {
      "epoch": 4.03,
      "grad_norm": 4.823983669281006,
      "learning_rate": 1.549405187621098e-05,
      "loss": 0.8065,
      "step": 12860
    },
    {
      "epoch": 4.03025,
      "grad_norm": 7.649628639221191,
      "learning_rate": 1.547493918247879e-05,
      "loss": 0.8768,
      "step": 12865
    },
    {
      "epoch": 4.0305,
      "grad_norm": 5.652050018310547,
      "learning_rate": 1.5455832999035852e-05,
      "loss": 0.8773,
      "step": 12870
    },
    {
      "epoch": 4.03075,
      "grad_norm": 5.897157669067383,
      "learning_rate": 1.5436733338941072e-05,
      "loss": 0.8897,
      "step": 12875
    },
    {
      "epoch": 4.031,
      "grad_norm": 4.350376605987549,
      "learning_rate": 1.541764021524887e-05,
      "loss": 0.7991,
      "step": 12880
    },
    {
      "epoch": 4.03125,
      "grad_norm": 5.713653087615967,
      "learning_rate": 1.5398553641009222e-05,
      "loss": 0.9228,
      "step": 12885
    },
    {
      "epoch": 4.0315,
      "grad_norm": 9.794203758239746,
      "learning_rate": 1.5379473629267628e-05,
      "loss": 0.8719,
      "step": 12890
    },
    {
      "epoch": 4.03175,
      "grad_norm": 5.41212797164917,
      "learning_rate": 1.536040019306509e-05,
      "loss": 0.8633,
      "step": 12895
    },
    {
      "epoch": 4.032,
      "grad_norm": 6.9866790771484375,
      "learning_rate": 1.5341333345438124e-05,
      "loss": 0.7815,
      "step": 12900
    },
    {
      "epoch": 4.03225,
      "grad_norm": 6.912111759185791,
      "learning_rate": 1.5322273099418737e-05,
      "loss": 0.7863,
      "step": 12905
    },
    {
      "epoch": 4.0325,
      "grad_norm": 6.8719024658203125,
      "learning_rate": 1.5303219468034436e-05,
      "loss": 0.8102,
      "step": 12910
    },
    {
      "epoch": 4.03275,
      "grad_norm": 6.667408466339111,
      "learning_rate": 1.5284172464308193e-05,
      "loss": 0.7554,
      "step": 12915
    },
    {
      "epoch": 4.033,
      "grad_norm": 6.135427474975586,
      "learning_rate": 1.5265132101258464e-05,
      "loss": 0.8585,
      "step": 12920
    },
    {
      "epoch": 4.03325,
      "grad_norm": 5.092700958251953,
      "learning_rate": 1.5246098391899138e-05,
      "loss": 0.8326,
      "step": 12925
    },
    {
      "epoch": 4.0335,
      "grad_norm": 5.453521728515625,
      "learning_rate": 1.5227071349239591e-05,
      "loss": 0.8409,
      "step": 12930
    },
    {
      "epoch": 4.03375,
      "grad_norm": 7.5594096183776855,
      "learning_rate": 1.5208050986284623e-05,
      "loss": 0.8289,
      "step": 12935
    },
    {
      "epoch": 4.034,
      "grad_norm": 6.287929534912109,
      "learning_rate": 1.5189037316034477e-05,
      "loss": 0.983,
      "step": 12940
    },
    {
      "epoch": 4.03425,
      "grad_norm": 5.910668849945068,
      "learning_rate": 1.5170030351484805e-05,
      "loss": 0.7906,
      "step": 12945
    },
    {
      "epoch": 4.0345,
      "grad_norm": 6.724816799163818,
      "learning_rate": 1.5151030105626704e-05,
      "loss": 0.8711,
      "step": 12950
    },
    {
      "epoch": 4.03475,
      "grad_norm": 4.875044345855713,
      "learning_rate": 1.5132036591446653e-05,
      "loss": 0.9212,
      "step": 12955
    },
    {
      "epoch": 4.035,
      "grad_norm": 5.939133167266846,
      "learning_rate": 1.5113049821926536e-05,
      "loss": 1.0099,
      "step": 12960
    },
    {
      "epoch": 4.03525,
      "grad_norm": 5.3191657066345215,
      "learning_rate": 1.5094069810043654e-05,
      "loss": 0.9348,
      "step": 12965
    },
    {
      "epoch": 4.0355,
      "grad_norm": 5.989516258239746,
      "learning_rate": 1.5075096568770642e-05,
      "loss": 0.8945,
      "step": 12970
    },
    {
      "epoch": 4.03575,
      "grad_norm": 5.440890789031982,
      "learning_rate": 1.5056130111075534e-05,
      "loss": 0.8958,
      "step": 12975
    },
    {
      "epoch": 4.036,
      "grad_norm": 7.302894592285156,
      "learning_rate": 1.5037170449921733e-05,
      "loss": 1.0361,
      "step": 12980
    },
    {
      "epoch": 4.03625,
      "grad_norm": 5.176839828491211,
      "learning_rate": 1.5018217598267981e-05,
      "loss": 0.8292,
      "step": 12985
    },
    {
      "epoch": 4.0365,
      "grad_norm": 5.56484317779541,
      "learning_rate": 1.4999271569068385e-05,
      "loss": 0.9265,
      "step": 12990
    },
    {
      "epoch": 4.03675,
      "grad_norm": 5.482044219970703,
      "learning_rate": 1.4980332375272366e-05,
      "loss": 0.8888,
      "step": 12995
    },
    {
      "epoch": 4.037,
      "grad_norm": 5.577310085296631,
      "learning_rate": 1.4961400029824691e-05,
      "loss": 0.953,
      "step": 13000
    },
    {
      "epoch": 4.037,
      "eval_loss": 2.0041379928588867,
      "eval_runtime": 5.3434,
      "eval_samples_per_second": 191.64,
      "eval_steps_per_second": 23.955,
      "step": 13000
    },
    {
      "epoch": 4.03725,
      "grad_norm": 5.5844221115112305,
      "learning_rate": 1.4942474545665433e-05,
      "loss": 0.9269,
      "step": 13005
    },
    {
      "epoch": 4.0375,
      "grad_norm": 7.229862213134766,
      "learning_rate": 1.4923555935729988e-05,
      "loss": 1.0438,
      "step": 13010
    },
    {
      "epoch": 4.03775,
      "grad_norm": 5.571395397186279,
      "learning_rate": 1.4904644212949053e-05,
      "loss": 0.8961,
      "step": 13015
    },
    {
      "epoch": 4.038,
      "grad_norm": 5.2191057205200195,
      "learning_rate": 1.4885739390248605e-05,
      "loss": 0.9131,
      "step": 13020
    },
    {
      "epoch": 4.03825,
      "grad_norm": 7.04653787612915,
      "learning_rate": 1.48668414805499e-05,
      "loss": 0.9321,
      "step": 13025
    },
    {
      "epoch": 4.0385,
      "grad_norm": 6.450253963470459,
      "learning_rate": 1.4847950496769503e-05,
      "loss": 0.9158,
      "step": 13030
    },
    {
      "epoch": 4.03875,
      "grad_norm": 5.985936641693115,
      "learning_rate": 1.4829066451819207e-05,
      "loss": 1.0028,
      "step": 13035
    },
    {
      "epoch": 4.039,
      "grad_norm": 4.8094987869262695,
      "learning_rate": 1.4810189358606092e-05,
      "loss": 0.9167,
      "step": 13040
    },
    {
      "epoch": 4.03925,
      "grad_norm": 7.153326988220215,
      "learning_rate": 1.4791319230032458e-05,
      "loss": 0.8942,
      "step": 13045
    },
    {
      "epoch": 4.0395,
      "grad_norm": 6.365419864654541,
      "learning_rate": 1.4772456078995872e-05,
      "loss": 0.8892,
      "step": 13050
    },
    {
      "epoch": 4.03975,
      "grad_norm": 5.073921203613281,
      "learning_rate": 1.4753599918389116e-05,
      "loss": 0.8066,
      "step": 13055
    },
    {
      "epoch": 4.04,
      "grad_norm": 5.917530536651611,
      "learning_rate": 1.4734750761100203e-05,
      "loss": 0.8475,
      "step": 13060
    },
    {
      "epoch": 4.04025,
      "grad_norm": 4.2049641609191895,
      "learning_rate": 1.4715908620012342e-05,
      "loss": 0.9003,
      "step": 13065
    },
    {
      "epoch": 4.0405,
      "grad_norm": 6.102426052093506,
      "learning_rate": 1.4697073508003973e-05,
      "loss": 0.8304,
      "step": 13070
    },
    {
      "epoch": 4.04075,
      "grad_norm": 4.777894496917725,
      "learning_rate": 1.4678245437948701e-05,
      "loss": 0.7978,
      "step": 13075
    },
    {
      "epoch": 4.041,
      "grad_norm": 6.998865127563477,
      "learning_rate": 1.4659424422715351e-05,
      "loss": 0.9621,
      "step": 13080
    },
    {
      "epoch": 4.04125,
      "grad_norm": 5.893354415893555,
      "learning_rate": 1.4640610475167898e-05,
      "loss": 0.8351,
      "step": 13085
    },
    {
      "epoch": 4.0415,
      "grad_norm": 5.915092468261719,
      "learning_rate": 1.4621803608165507e-05,
      "loss": 0.8952,
      "step": 13090
    },
    {
      "epoch": 4.04175,
      "grad_norm": 6.1471076011657715,
      "learning_rate": 1.4603003834562485e-05,
      "loss": 0.9307,
      "step": 13095
    },
    {
      "epoch": 4.042,
      "grad_norm": 6.982871055603027,
      "learning_rate": 1.4584211167208305e-05,
      "loss": 0.8982,
      "step": 13100
    },
    {
      "epoch": 4.04225,
      "grad_norm": 7.182979106903076,
      "learning_rate": 1.4565425618947588e-05,
      "loss": 0.8831,
      "step": 13105
    },
    {
      "epoch": 4.0425,
      "grad_norm": 6.362664222717285,
      "learning_rate": 1.454664720262005e-05,
      "loss": 0.9111,
      "step": 13110
    },
    {
      "epoch": 4.04275,
      "grad_norm": 6.2682318687438965,
      "learning_rate": 1.4527875931060586e-05,
      "loss": 0.9537,
      "step": 13115
    },
    {
      "epoch": 4.043,
      "grad_norm": 7.982745170593262,
      "learning_rate": 1.4509111817099177e-05,
      "loss": 0.9101,
      "step": 13120
    },
    {
      "epoch": 4.04325,
      "grad_norm": 5.796113967895508,
      "learning_rate": 1.449035487356093e-05,
      "loss": 0.9699,
      "step": 13125
    },
    {
      "epoch": 4.0435,
      "grad_norm": 7.525130748748779,
      "learning_rate": 1.4471605113266012e-05,
      "loss": 1.0449,
      "step": 13130
    },
    {
      "epoch": 4.04375,
      "grad_norm": 5.463974475860596,
      "learning_rate": 1.4452862549029722e-05,
      "loss": 0.8069,
      "step": 13135
    },
    {
      "epoch": 4.044,
      "grad_norm": 5.970080375671387,
      "learning_rate": 1.4434127193662428e-05,
      "loss": 0.8523,
      "step": 13140
    },
    {
      "epoch": 4.04425,
      "grad_norm": 4.30238151550293,
      "learning_rate": 1.4415399059969578e-05,
      "loss": 0.8349,
      "step": 13145
    },
    {
      "epoch": 4.0445,
      "grad_norm": 5.451666831970215,
      "learning_rate": 1.4396678160751653e-05,
      "loss": 0.813,
      "step": 13150
    },
    {
      "epoch": 4.04475,
      "grad_norm": 6.001274585723877,
      "learning_rate": 1.4377964508804231e-05,
      "loss": 0.8868,
      "step": 13155
    },
    {
      "epoch": 4.045,
      "grad_norm": 6.450560569763184,
      "learning_rate": 1.43592581169179e-05,
      "loss": 0.8568,
      "step": 13160
    },
    {
      "epoch": 4.04525,
      "grad_norm": 5.6280598640441895,
      "learning_rate": 1.4340558997878307e-05,
      "loss": 0.8952,
      "step": 13165
    },
    {
      "epoch": 4.0455,
      "grad_norm": 6.75787878036499,
      "learning_rate": 1.4321867164466126e-05,
      "loss": 0.8593,
      "step": 13170
    },
    {
      "epoch": 4.04575,
      "grad_norm": 4.976078987121582,
      "learning_rate": 1.4303182629457062e-05,
      "loss": 0.8826,
      "step": 13175
    },
    {
      "epoch": 4.046,
      "grad_norm": 5.735547065734863,
      "learning_rate": 1.4284505405621795e-05,
      "loss": 0.8931,
      "step": 13180
    },
    {
      "epoch": 4.04625,
      "grad_norm": 6.9202985763549805,
      "learning_rate": 1.4265835505726041e-05,
      "loss": 0.9973,
      "step": 13185
    },
    {
      "epoch": 4.0465,
      "grad_norm": 5.653613567352295,
      "learning_rate": 1.4247172942530501e-05,
      "loss": 0.8314,
      "step": 13190
    },
    {
      "epoch": 4.04675,
      "grad_norm": 4.9904608726501465,
      "learning_rate": 1.4228517728790874e-05,
      "loss": 0.8441,
      "step": 13195
    },
    {
      "epoch": 4.047,
      "grad_norm": 5.785417556762695,
      "learning_rate": 1.4209869877257808e-05,
      "loss": 0.8693,
      "step": 13200
    },
    {
      "epoch": 4.04725,
      "grad_norm": 6.05547571182251,
      "learning_rate": 1.4191229400676931e-05,
      "loss": 0.8868,
      "step": 13205
    },
    {
      "epoch": 4.0475,
      "grad_norm": 5.168112754821777,
      "learning_rate": 1.4172596311788833e-05,
      "loss": 0.7994,
      "step": 13210
    },
    {
      "epoch": 4.04775,
      "grad_norm": 6.206333637237549,
      "learning_rate": 1.415397062332906e-05,
      "loss": 0.9106,
      "step": 13215
    },
    {
      "epoch": 4.048,
      "grad_norm": 6.010116100311279,
      "learning_rate": 1.4135352348028097e-05,
      "loss": 0.9587,
      "step": 13220
    },
    {
      "epoch": 4.04825,
      "grad_norm": 6.26753568649292,
      "learning_rate": 1.4116741498611364e-05,
      "loss": 0.9176,
      "step": 13225
    },
    {
      "epoch": 4.0485,
      "grad_norm": 5.335549831390381,
      "learning_rate": 1.4098138087799184e-05,
      "loss": 0.8977,
      "step": 13230
    },
    {
      "epoch": 4.04875,
      "grad_norm": 5.441779136657715,
      "learning_rate": 1.4079542128306822e-05,
      "loss": 0.9557,
      "step": 13235
    },
    {
      "epoch": 4.049,
      "grad_norm": 6.323955535888672,
      "learning_rate": 1.4060953632844442e-05,
      "loss": 0.9028,
      "step": 13240
    },
    {
      "epoch": 4.04925,
      "grad_norm": 5.76900577545166,
      "learning_rate": 1.4042372614117116e-05,
      "loss": 0.937,
      "step": 13245
    },
    {
      "epoch": 4.0495,
      "grad_norm": 9.797934532165527,
      "learning_rate": 1.4023799084824785e-05,
      "loss": 0.8875,
      "step": 13250
    },
    {
      "epoch": 4.04975,
      "grad_norm": 4.533484935760498,
      "learning_rate": 1.4005233057662267e-05,
      "loss": 0.8616,
      "step": 13255
    },
    {
      "epoch": 4.05,
      "grad_norm": 5.890179634094238,
      "learning_rate": 1.3986674545319285e-05,
      "loss": 0.8607,
      "step": 13260
    },
    {
      "epoch": 4.05025,
      "grad_norm": 5.685474872589111,
      "learning_rate": 1.3968123560480396e-05,
      "loss": 0.8954,
      "step": 13265
    },
    {
      "epoch": 4.0505,
      "grad_norm": 6.1548614501953125,
      "learning_rate": 1.3949580115825028e-05,
      "loss": 0.8968,
      "step": 13270
    },
    {
      "epoch": 4.05075,
      "grad_norm": 7.333690643310547,
      "learning_rate": 1.3931044224027468e-05,
      "loss": 0.94,
      "step": 13275
    },
    {
      "epoch": 4.051,
      "grad_norm": 4.504556179046631,
      "learning_rate": 1.3912515897756792e-05,
      "loss": 0.8252,
      "step": 13280
    },
    {
      "epoch": 4.05125,
      "grad_norm": 5.97867488861084,
      "learning_rate": 1.3893995149676953e-05,
      "loss": 0.8984,
      "step": 13285
    },
    {
      "epoch": 4.0515,
      "grad_norm": 4.590908050537109,
      "learning_rate": 1.3875481992446704e-05,
      "loss": 0.7526,
      "step": 13290
    },
    {
      "epoch": 4.05175,
      "grad_norm": 6.5985283851623535,
      "learning_rate": 1.3856976438719626e-05,
      "loss": 0.8971,
      "step": 13295
    },
    {
      "epoch": 4.052,
      "grad_norm": 4.710810661315918,
      "learning_rate": 1.3838478501144082e-05,
      "loss": 0.7905,
      "step": 13300
    },
    {
      "epoch": 4.05225,
      "grad_norm": 5.638987064361572,
      "learning_rate": 1.3819988192363226e-05,
      "loss": 0.7929,
      "step": 13305
    },
    {
      "epoch": 4.0525,
      "grad_norm": 6.360008716583252,
      "learning_rate": 1.3801505525015016e-05,
      "loss": 0.8607,
      "step": 13310
    },
    {
      "epoch": 4.05275,
      "grad_norm": 5.8699188232421875,
      "learning_rate": 1.3783030511732187e-05,
      "loss": 0.8193,
      "step": 13315
    },
    {
      "epoch": 4.053,
      "grad_norm": 5.9085235595703125,
      "learning_rate": 1.376456316514223e-05,
      "loss": 0.8217,
      "step": 13320
    },
    {
      "epoch": 4.05325,
      "grad_norm": 6.049744606018066,
      "learning_rate": 1.3746103497867418e-05,
      "loss": 0.8346,
      "step": 13325
    },
    {
      "epoch": 4.0535,
      "grad_norm": 6.260278224945068,
      "learning_rate": 1.3727651522524737e-05,
      "loss": 0.7559,
      "step": 13330
    },
    {
      "epoch": 4.05375,
      "grad_norm": 5.0388689041137695,
      "learning_rate": 1.3709207251725947e-05,
      "loss": 0.7574,
      "step": 13335
    },
    {
      "epoch": 4.054,
      "grad_norm": 7.224246501922607,
      "learning_rate": 1.3690770698077549e-05,
      "loss": 0.7773,
      "step": 13340
    },
    {
      "epoch": 4.05425,
      "grad_norm": 6.934463024139404,
      "learning_rate": 1.3672341874180728e-05,
      "loss": 0.7931,
      "step": 13345
    },
    {
      "epoch": 4.0545,
      "grad_norm": 6.572576522827148,
      "learning_rate": 1.365392079263143e-05,
      "loss": 0.9316,
      "step": 13350
    },
    {
      "epoch": 4.05475,
      "grad_norm": 5.228384494781494,
      "learning_rate": 1.3635507466020273e-05,
      "loss": 0.874,
      "step": 13355
    },
    {
      "epoch": 4.055,
      "grad_norm": 6.624713897705078,
      "learning_rate": 1.3617101906932605e-05,
      "loss": 0.8195,
      "step": 13360
    },
    {
      "epoch": 4.05525,
      "grad_norm": 5.055110931396484,
      "learning_rate": 1.3598704127948447e-05,
      "loss": 0.8269,
      "step": 13365
    },
    {
      "epoch": 4.0555,
      "grad_norm": 5.499717712402344,
      "learning_rate": 1.3580314141642509e-05,
      "loss": 0.7842,
      "step": 13370
    },
    {
      "epoch": 4.05575,
      "grad_norm": 7.199343681335449,
      "learning_rate": 1.3561931960584185e-05,
      "loss": 0.8814,
      "step": 13375
    },
    {
      "epoch": 4.056,
      "grad_norm": 6.340075969696045,
      "learning_rate": 1.3543557597337503e-05,
      "loss": 0.9045,
      "step": 13380
    },
    {
      "epoch": 4.05625,
      "grad_norm": 7.100013256072998,
      "learning_rate": 1.3525191064461185e-05,
      "loss": 0.8627,
      "step": 13385
    },
    {
      "epoch": 4.0565,
      "grad_norm": 4.578029632568359,
      "learning_rate": 1.350683237450856e-05,
      "loss": 0.7545,
      "step": 13390
    },
    {
      "epoch": 4.05675,
      "grad_norm": 7.31742525100708,
      "learning_rate": 1.3488481540027637e-05,
      "loss": 0.8611,
      "step": 13395
    },
    {
      "epoch": 4.057,
      "grad_norm": 4.591226100921631,
      "learning_rate": 1.3470138573561042e-05,
      "loss": 0.7525,
      "step": 13400
    },
    {
      "epoch": 4.05725,
      "grad_norm": 3.580204725265503,
      "learning_rate": 1.3451803487646003e-05,
      "loss": 0.8178,
      "step": 13405
    },
    {
      "epoch": 4.0575,
      "grad_norm": 7.507380962371826,
      "learning_rate": 1.343347629481439e-05,
      "loss": 0.8772,
      "step": 13410
    },
    {
      "epoch": 4.05775,
      "grad_norm": 5.860662937164307,
      "learning_rate": 1.341515700759266e-05,
      "loss": 0.7236,
      "step": 13415
    },
    {
      "epoch": 4.058,
      "grad_norm": 4.706847190856934,
      "learning_rate": 1.3396845638501875e-05,
      "loss": 0.7147,
      "step": 13420
    },
    {
      "epoch": 4.05825,
      "grad_norm": 5.097930908203125,
      "learning_rate": 1.3378542200057697e-05,
      "loss": 0.7494,
      "step": 13425
    },
    {
      "epoch": 4.0585,
      "grad_norm": 4.820326328277588,
      "learning_rate": 1.3360246704770335e-05,
      "loss": 0.8091,
      "step": 13430
    },
    {
      "epoch": 4.05875,
      "grad_norm": 5.8957295417785645,
      "learning_rate": 1.3341959165144599e-05,
      "loss": 0.8055,
      "step": 13435
    },
    {
      "epoch": 4.059,
      "grad_norm": 8.200613975524902,
      "learning_rate": 1.3323679593679838e-05,
      "loss": 0.9691,
      "step": 13440
    },
    {
      "epoch": 4.05925,
      "grad_norm": 3.8448736667633057,
      "learning_rate": 1.3305408002869971e-05,
      "loss": 0.8176,
      "step": 13445
    },
    {
      "epoch": 4.0595,
      "grad_norm": 5.855502128601074,
      "learning_rate": 1.3287144405203473e-05,
      "loss": 0.8976,
      "step": 13450
    },
    {
      "epoch": 4.05975,
      "grad_norm": 4.99310302734375,
      "learning_rate": 1.3268888813163317e-05,
      "loss": 0.9048,
      "step": 13455
    },
    {
      "epoch": 4.06,
      "grad_norm": 6.048658847808838,
      "learning_rate": 1.3250641239227041e-05,
      "loss": 0.9791,
      "step": 13460
    },
    {
      "epoch": 4.06025,
      "grad_norm": 6.101247787475586,
      "learning_rate": 1.3232401695866687e-05,
      "loss": 0.8288,
      "step": 13465
    },
    {
      "epoch": 4.0605,
      "grad_norm": 7.226726531982422,
      "learning_rate": 1.3214170195548809e-05,
      "loss": 0.923,
      "step": 13470
    },
    {
      "epoch": 4.06075,
      "grad_norm": 5.652555465698242,
      "learning_rate": 1.3195946750734484e-05,
      "loss": 0.801,
      "step": 13475
    },
    {
      "epoch": 4.061,
      "grad_norm": 5.786508083343506,
      "learning_rate": 1.3177731373879242e-05,
      "loss": 0.8247,
      "step": 13480
    },
    {
      "epoch": 4.06125,
      "grad_norm": 7.3203020095825195,
      "learning_rate": 1.3159524077433127e-05,
      "loss": 0.77,
      "step": 13485
    },
    {
      "epoch": 4.0615,
      "grad_norm": 5.675751686096191,
      "learning_rate": 1.3141324873840652e-05,
      "loss": 0.8611,
      "step": 13490
    },
    {
      "epoch": 4.06175,
      "grad_norm": 5.591805934906006,
      "learning_rate": 1.3123133775540809e-05,
      "loss": 0.8046,
      "step": 13495
    },
    {
      "epoch": 4.062,
      "grad_norm": 5.030604362487793,
      "learning_rate": 1.310495079496703e-05,
      "loss": 0.8307,
      "step": 13500
    },
    {
      "epoch": 4.062,
      "eval_loss": 2.0376296043395996,
      "eval_runtime": 5.2944,
      "eval_samples_per_second": 193.412,
      "eval_steps_per_second": 24.177,
      "step": 13500
    },
    {
      "epoch": 4.06225,
      "grad_norm": 7.117036819458008,
      "learning_rate": 1.3086775944547236e-05,
      "loss": 0.8661,
      "step": 13505
    },
    {
      "epoch": 4.0625,
      "grad_norm": 4.54256010055542,
      "learning_rate": 1.3068609236703738e-05,
      "loss": 0.7608,
      "step": 13510
    },
    {
      "epoch": 4.06275,
      "grad_norm": 6.69818639755249,
      "learning_rate": 1.3050450683853319e-05,
      "loss": 0.7479,
      "step": 13515
    },
    {
      "epoch": 4.063,
      "grad_norm": 6.673498153686523,
      "learning_rate": 1.303230029840718e-05,
      "loss": 0.8653,
      "step": 13520
    },
    {
      "epoch": 4.06325,
      "grad_norm": 6.645796298980713,
      "learning_rate": 1.3014158092770948e-05,
      "loss": 0.9973,
      "step": 13525
    },
    {
      "epoch": 4.0635,
      "grad_norm": 4.589235305786133,
      "learning_rate": 1.299602407934464e-05,
      "loss": 0.725,
      "step": 13530
    },
    {
      "epoch": 4.06375,
      "grad_norm": 5.166536808013916,
      "learning_rate": 1.2977898270522677e-05,
      "loss": 0.8655,
      "step": 13535
    },
    {
      "epoch": 4.064,
      "grad_norm": 7.018927574157715,
      "learning_rate": 1.2959780678693878e-05,
      "loss": 0.8019,
      "step": 13540
    },
    {
      "epoch": 4.06425,
      "grad_norm": 7.0482892990112305,
      "learning_rate": 1.2941671316241454e-05,
      "loss": 0.8454,
      "step": 13545
    },
    {
      "epoch": 4.0645,
      "grad_norm": 5.560570240020752,
      "learning_rate": 1.2923570195542986e-05,
      "loss": 0.7797,
      "step": 13550
    },
    {
      "epoch": 4.06475,
      "grad_norm": 6.574538707733154,
      "learning_rate": 1.2905477328970423e-05,
      "loss": 0.8583,
      "step": 13555
    },
    {
      "epoch": 4.065,
      "grad_norm": 6.711709499359131,
      "learning_rate": 1.2887392728890052e-05,
      "loss": 0.9213,
      "step": 13560
    },
    {
      "epoch": 4.06525,
      "grad_norm": 7.14993953704834,
      "learning_rate": 1.2869316407662535e-05,
      "loss": 0.9428,
      "step": 13565
    },
    {
      "epoch": 4.0655,
      "grad_norm": 5.536820888519287,
      "learning_rate": 1.2851248377642876e-05,
      "loss": 0.8275,
      "step": 13570
    },
    {
      "epoch": 4.06575,
      "grad_norm": 5.8723273277282715,
      "learning_rate": 1.2833188651180388e-05,
      "loss": 0.788,
      "step": 13575
    },
    {
      "epoch": 4.066,
      "grad_norm": 5.495955467224121,
      "learning_rate": 1.2815137240618741e-05,
      "loss": 0.8859,
      "step": 13580
    },
    {
      "epoch": 4.06625,
      "grad_norm": 9.199131965637207,
      "learning_rate": 1.279709415829588e-05,
      "loss": 1.014,
      "step": 13585
    },
    {
      "epoch": 4.0665,
      "grad_norm": 6.224738121032715,
      "learning_rate": 1.2779059416544099e-05,
      "loss": 0.7733,
      "step": 13590
    },
    {
      "epoch": 4.06675,
      "grad_norm": 6.710019588470459,
      "learning_rate": 1.276103302768997e-05,
      "loss": 0.7788,
      "step": 13595
    },
    {
      "epoch": 4.067,
      "grad_norm": 4.733623027801514,
      "learning_rate": 1.274301500405436e-05,
      "loss": 0.7695,
      "step": 13600
    },
    {
      "epoch": 4.06725,
      "grad_norm": 4.484922409057617,
      "learning_rate": 1.2725005357952425e-05,
      "loss": 0.7125,
      "step": 13605
    },
    {
      "epoch": 4.0675,
      "grad_norm": 5.135512351989746,
      "learning_rate": 1.2707004101693576e-05,
      "loss": 0.7092,
      "step": 13610
    },
    {
      "epoch": 4.06775,
      "grad_norm": 5.399816513061523,
      "learning_rate": 1.2689011247581503e-05,
      "loss": 0.6882,
      "step": 13615
    },
    {
      "epoch": 4.068,
      "grad_norm": 5.3426337242126465,
      "learning_rate": 1.267102680791417e-05,
      "loss": 0.9717,
      "step": 13620
    },
    {
      "epoch": 4.06825,
      "grad_norm": 5.928732395172119,
      "learning_rate": 1.2653050794983746e-05,
      "loss": 0.8284,
      "step": 13625
    },
    {
      "epoch": 4.0685,
      "grad_norm": 5.89889669418335,
      "learning_rate": 1.263508322107669e-05,
      "loss": 0.8236,
      "step": 13630
    },
    {
      "epoch": 4.06875,
      "grad_norm": 4.267330646514893,
      "learning_rate": 1.2617124098473649e-05,
      "loss": 0.7679,
      "step": 13635
    },
    {
      "epoch": 4.069,
      "grad_norm": 4.02947998046875,
      "learning_rate": 1.2599173439449525e-05,
      "loss": 0.7464,
      "step": 13640
    },
    {
      "epoch": 4.06925,
      "grad_norm": 5.516480445861816,
      "learning_rate": 1.2581231256273419e-05,
      "loss": 0.7468,
      "step": 13645
    },
    {
      "epoch": 4.0695,
      "grad_norm": 4.315579891204834,
      "learning_rate": 1.2563297561208648e-05,
      "loss": 0.7568,
      "step": 13650
    },
    {
      "epoch": 4.06975,
      "grad_norm": 4.306288719177246,
      "learning_rate": 1.254537236651273e-05,
      "loss": 0.7643,
      "step": 13655
    },
    {
      "epoch": 4.07,
      "grad_norm": 5.142501354217529,
      "learning_rate": 1.2527455684437357e-05,
      "loss": 0.8174,
      "step": 13660
    },
    {
      "epoch": 4.07025,
      "grad_norm": 4.224555969238281,
      "learning_rate": 1.2509547527228416e-05,
      "loss": 0.7255,
      "step": 13665
    },
    {
      "epoch": 4.0705,
      "grad_norm": 3.6651580333709717,
      "learning_rate": 1.2491647907125958e-05,
      "loss": 0.7893,
      "step": 13670
    },
    {
      "epoch": 4.07075,
      "grad_norm": 4.6394782066345215,
      "learning_rate": 1.2473756836364207e-05,
      "loss": 0.7129,
      "step": 13675
    },
    {
      "epoch": 4.071,
      "grad_norm": 4.545435905456543,
      "learning_rate": 1.2455874327171552e-05,
      "loss": 0.7113,
      "step": 13680
    },
    {
      "epoch": 4.07125,
      "grad_norm": 3.8156538009643555,
      "learning_rate": 1.2438000391770502e-05,
      "loss": 0.6815,
      "step": 13685
    },
    {
      "epoch": 4.0715,
      "grad_norm": 4.6820454597473145,
      "learning_rate": 1.2420135042377732e-05,
      "loss": 0.7059,
      "step": 13690
    },
    {
      "epoch": 4.07175,
      "grad_norm": 4.905246734619141,
      "learning_rate": 1.2402278291204037e-05,
      "loss": 0.9049,
      "step": 13695
    },
    {
      "epoch": 4.072,
      "grad_norm": 4.53558349609375,
      "learning_rate": 1.2384430150454342e-05,
      "loss": 0.7671,
      "step": 13700
    },
    {
      "epoch": 4.07225,
      "grad_norm": 5.068803310394287,
      "learning_rate": 1.236659063232769e-05,
      "loss": 0.7267,
      "step": 13705
    },
    {
      "epoch": 4.0725,
      "grad_norm": 4.753610610961914,
      "learning_rate": 1.2348759749017216e-05,
      "loss": 0.632,
      "step": 13710
    },
    {
      "epoch": 4.07275,
      "grad_norm": 4.731206893920898,
      "learning_rate": 1.2330937512710153e-05,
      "loss": 0.7774,
      "step": 13715
    },
    {
      "epoch": 4.073,
      "grad_norm": 4.6388115882873535,
      "learning_rate": 1.2313123935587833e-05,
      "loss": 0.7154,
      "step": 13720
    },
    {
      "epoch": 4.07325,
      "grad_norm": 4.938470840454102,
      "learning_rate": 1.2295319029825674e-05,
      "loss": 0.6541,
      "step": 13725
    },
    {
      "epoch": 4.0735,
      "grad_norm": 4.105783462524414,
      "learning_rate": 1.2277522807593173e-05,
      "loss": 0.6521,
      "step": 13730
    },
    {
      "epoch": 4.07375,
      "grad_norm": 3.8089935779571533,
      "learning_rate": 1.2259735281053854e-05,
      "loss": 0.7701,
      "step": 13735
    },
    {
      "epoch": 4.074,
      "grad_norm": 4.780556678771973,
      "learning_rate": 1.2241956462365339e-05,
      "loss": 0.6899,
      "step": 13740
    },
    {
      "epoch": 4.07425,
      "grad_norm": 5.14572286605835,
      "learning_rate": 1.222418636367928e-05,
      "loss": 0.6879,
      "step": 13745
    },
    {
      "epoch": 4.0745,
      "grad_norm": 5.4974565505981445,
      "learning_rate": 1.2206424997141371e-05,
      "loss": 0.7879,
      "step": 13750
    },
    {
      "epoch": 4.07475,
      "grad_norm": 3.9298195838928223,
      "learning_rate": 1.2188672374891352e-05,
      "loss": 0.6002,
      "step": 13755
    },
    {
      "epoch": 4.075,
      "grad_norm": 3.7446401119232178,
      "learning_rate": 1.217092850906296e-05,
      "loss": 0.6154,
      "step": 13760
    },
    {
      "epoch": 4.07525,
      "grad_norm": 5.032529354095459,
      "learning_rate": 1.2153193411783953e-05,
      "loss": 0.6749,
      "step": 13765
    },
    {
      "epoch": 4.0755,
      "grad_norm": 4.829562187194824,
      "learning_rate": 1.2135467095176109e-05,
      "loss": 0.753,
      "step": 13770
    },
    {
      "epoch": 4.07575,
      "grad_norm": 4.558670520782471,
      "learning_rate": 1.2117749571355202e-05,
      "loss": 0.6716,
      "step": 13775
    },
    {
      "epoch": 4.076,
      "grad_norm": 5.20950984954834,
      "learning_rate": 1.2100040852430999e-05,
      "loss": 0.7967,
      "step": 13780
    },
    {
      "epoch": 4.07625,
      "grad_norm": 4.61221170425415,
      "learning_rate": 1.2082340950507226e-05,
      "loss": 0.7595,
      "step": 13785
    },
    {
      "epoch": 4.0765,
      "grad_norm": 5.0885443687438965,
      "learning_rate": 1.2064649877681605e-05,
      "loss": 0.7185,
      "step": 13790
    },
    {
      "epoch": 4.07675,
      "grad_norm": 3.9983561038970947,
      "learning_rate": 1.204696764604582e-05,
      "loss": 0.7114,
      "step": 13795
    },
    {
      "epoch": 4.077,
      "grad_norm": 4.387084484100342,
      "learning_rate": 1.2029294267685509e-05,
      "loss": 0.6497,
      "step": 13800
    },
    {
      "epoch": 4.07725,
      "grad_norm": 4.028090476989746,
      "learning_rate": 1.201162975468027e-05,
      "loss": 0.7002,
      "step": 13805
    },
    {
      "epoch": 4.0775,
      "grad_norm": 5.679986000061035,
      "learning_rate": 1.1993974119103621e-05,
      "loss": 0.776,
      "step": 13810
    },
    {
      "epoch": 4.07775,
      "grad_norm": 4.239671230316162,
      "learning_rate": 1.1976327373023013e-05,
      "loss": 0.7164,
      "step": 13815
    },
    {
      "epoch": 4.078,
      "grad_norm": 5.192655563354492,
      "learning_rate": 1.1958689528499839e-05,
      "loss": 0.7659,
      "step": 13820
    },
    {
      "epoch": 4.07825,
      "grad_norm": 4.251132488250732,
      "learning_rate": 1.1941060597589399e-05,
      "loss": 0.6982,
      "step": 13825
    },
    {
      "epoch": 4.0785,
      "grad_norm": 5.151986598968506,
      "learning_rate": 1.1923440592340912e-05,
      "loss": 0.624,
      "step": 13830
    },
    {
      "epoch": 4.07875,
      "grad_norm": 4.655488014221191,
      "learning_rate": 1.1905829524797465e-05,
      "loss": 0.6685,
      "step": 13835
    },
    {
      "epoch": 4.079,
      "grad_norm": 6.060754299163818,
      "learning_rate": 1.188822740699607e-05,
      "loss": 0.6665,
      "step": 13840
    },
    {
      "epoch": 4.07925,
      "grad_norm": 5.034689903259277,
      "learning_rate": 1.1870634250967605e-05,
      "loss": 0.7643,
      "step": 13845
    },
    {
      "epoch": 4.0795,
      "grad_norm": 5.207727432250977,
      "learning_rate": 1.1853050068736834e-05,
      "loss": 0.6452,
      "step": 13850
    },
    {
      "epoch": 4.07975,
      "grad_norm": 6.638949871063232,
      "learning_rate": 1.1835474872322367e-05,
      "loss": 0.7071,
      "step": 13855
    },
    {
      "epoch": 4.08,
      "grad_norm": 3.7003836631774902,
      "learning_rate": 1.1817908673736697e-05,
      "loss": 0.7221,
      "step": 13860
    },
    {
      "epoch": 4.08025,
      "grad_norm": 5.292313098907471,
      "learning_rate": 1.1800351484986141e-05,
      "loss": 0.7482,
      "step": 13865
    },
    {
      "epoch": 4.0805,
      "grad_norm": 6.201622486114502,
      "learning_rate": 1.1782803318070878e-05,
      "loss": 0.7801,
      "step": 13870
    },
    {
      "epoch": 4.08075,
      "grad_norm": 5.761427402496338,
      "learning_rate": 1.1765264184984917e-05,
      "loss": 0.7433,
      "step": 13875
    },
    {
      "epoch": 4.081,
      "grad_norm": 4.6118974685668945,
      "learning_rate": 1.1747734097716102e-05,
      "loss": 0.729,
      "step": 13880
    },
    {
      "epoch": 4.08125,
      "grad_norm": 3.8972816467285156,
      "learning_rate": 1.1730213068246057e-05,
      "loss": 0.7335,
      "step": 13885
    },
    {
      "epoch": 4.0815,
      "grad_norm": 3.2548344135284424,
      "learning_rate": 1.1712701108550254e-05,
      "loss": 0.7134,
      "step": 13890
    },
    {
      "epoch": 4.08175,
      "grad_norm": 3.7776858806610107,
      "learning_rate": 1.1695198230597951e-05,
      "loss": 0.7235,
      "step": 13895
    },
    {
      "epoch": 4.082,
      "grad_norm": 4.473651885986328,
      "learning_rate": 1.1677704446352212e-05,
      "loss": 0.7659,
      "step": 13900
    },
    {
      "epoch": 4.08225,
      "grad_norm": 4.199718952178955,
      "learning_rate": 1.1660219767769851e-05,
      "loss": 0.6903,
      "step": 13905
    },
    {
      "epoch": 4.0825,
      "grad_norm": 4.233896732330322,
      "learning_rate": 1.1642744206801502e-05,
      "loss": 0.7367,
      "step": 13910
    },
    {
      "epoch": 4.08275,
      "grad_norm": 4.414292812347412,
      "learning_rate": 1.1625277775391526e-05,
      "loss": 0.7089,
      "step": 13915
    },
    {
      "epoch": 4.083,
      "grad_norm": 4.317552089691162,
      "learning_rate": 1.1607820485478074e-05,
      "loss": 0.6625,
      "step": 13920
    },
    {
      "epoch": 4.08325,
      "grad_norm": 4.6723504066467285,
      "learning_rate": 1.1590372348993043e-05,
      "loss": 0.6494,
      "step": 13925
    },
    {
      "epoch": 4.0835,
      "grad_norm": 6.068887233734131,
      "learning_rate": 1.1572933377862071e-05,
      "loss": 0.8009,
      "step": 13930
    },
    {
      "epoch": 4.08375,
      "grad_norm": 4.248253345489502,
      "learning_rate": 1.155550358400452e-05,
      "loss": 0.6267,
      "step": 13935
    },
    {
      "epoch": 4.084,
      "grad_norm": 5.653107643127441,
      "learning_rate": 1.1538082979333495e-05,
      "loss": 0.5958,
      "step": 13940
    },
    {
      "epoch": 4.08425,
      "grad_norm": 3.6947078704833984,
      "learning_rate": 1.1520671575755823e-05,
      "loss": 0.556,
      "step": 13945
    },
    {
      "epoch": 4.0845,
      "grad_norm": 5.895198822021484,
      "learning_rate": 1.1503269385172016e-05,
      "loss": 0.7092,
      "step": 13950
    },
    {
      "epoch": 4.08475,
      "grad_norm": 3.4039995670318604,
      "learning_rate": 1.1485876419476322e-05,
      "loss": 0.5748,
      "step": 13955
    },
    {
      "epoch": 4.085,
      "grad_norm": 4.36095666885376,
      "learning_rate": 1.146849269055667e-05,
      "loss": 0.5982,
      "step": 13960
    },
    {
      "epoch": 4.08525,
      "grad_norm": 3.8191959857940674,
      "learning_rate": 1.1451118210294663e-05,
      "loss": 0.5846,
      "step": 13965
    },
    {
      "epoch": 4.0855,
      "grad_norm": 3.1932737827301025,
      "learning_rate": 1.14337529905656e-05,
      "loss": 0.5454,
      "step": 13970
    },
    {
      "epoch": 4.08575,
      "grad_norm": 4.043472766876221,
      "learning_rate": 1.1416397043238445e-05,
      "loss": 0.6583,
      "step": 13975
    },
    {
      "epoch": 4.086,
      "grad_norm": 5.057199478149414,
      "learning_rate": 1.1399050380175835e-05,
      "loss": 0.6493,
      "step": 13980
    },
    {
      "epoch": 4.08625,
      "grad_norm": 4.793231964111328,
      "learning_rate": 1.138171301323403e-05,
      "loss": 0.5757,
      "step": 13985
    },
    {
      "epoch": 4.0865,
      "grad_norm": 3.7967607975006104,
      "learning_rate": 1.136438495426298e-05,
      "loss": 0.5825,
      "step": 13990
    },
    {
      "epoch": 4.08675,
      "grad_norm": 3.9770870208740234,
      "learning_rate": 1.1347066215106225e-05,
      "loss": 0.6291,
      "step": 13995
    },
    {
      "epoch": 4.087,
      "grad_norm": 4.025580883026123,
      "learning_rate": 1.1329756807600974e-05,
      "loss": 0.6461,
      "step": 14000
    },
    {
      "epoch": 4.087,
      "eval_loss": 2.1780545711517334,
      "eval_runtime": 5.1642,
      "eval_samples_per_second": 198.287,
      "eval_steps_per_second": 24.786,
      "step": 14000
    },
    {
      "epoch": 4.08725,
      "grad_norm": 4.656620025634766,
      "learning_rate": 1.1312456743578042e-05,
      "loss": 0.6107,
      "step": 14005
    },
    {
      "epoch": 4.0875,
      "grad_norm": 4.733879089355469,
      "learning_rate": 1.1295166034861874e-05,
      "loss": 0.5929,
      "step": 14010
    },
    {
      "epoch": 4.08775,
      "grad_norm": 3.5822839736938477,
      "learning_rate": 1.1277884693270477e-05,
      "loss": 0.5981,
      "step": 14015
    },
    {
      "epoch": 4.088,
      "grad_norm": 5.338489532470703,
      "learning_rate": 1.1260612730615502e-05,
      "loss": 0.6496,
      "step": 14020
    },
    {
      "epoch": 4.08825,
      "grad_norm": 4.530335426330566,
      "learning_rate": 1.1243350158702171e-05,
      "loss": 0.6047,
      "step": 14025
    },
    {
      "epoch": 4.0885,
      "grad_norm": 5.3131937980651855,
      "learning_rate": 1.1226096989329298e-05,
      "loss": 0.6002,
      "step": 14030
    },
    {
      "epoch": 4.08875,
      "grad_norm": 4.597975254058838,
      "learning_rate": 1.1208853234289246e-05,
      "loss": 0.6884,
      "step": 14035
    },
    {
      "epoch": 4.089,
      "grad_norm": 5.086826324462891,
      "learning_rate": 1.119161890536797e-05,
      "loss": 0.6599,
      "step": 14040
    },
    {
      "epoch": 4.08925,
      "grad_norm": 4.331513404846191,
      "learning_rate": 1.1174394014344963e-05,
      "loss": 0.5725,
      "step": 14045
    },
    {
      "epoch": 4.0895,
      "grad_norm": 5.252206802368164,
      "learning_rate": 1.1157178572993276e-05,
      "loss": 0.6009,
      "step": 14050
    },
    {
      "epoch": 4.08975,
      "grad_norm": 3.893862009048462,
      "learning_rate": 1.1139972593079507e-05,
      "loss": 0.5512,
      "step": 14055
    },
    {
      "epoch": 4.09,
      "grad_norm": 5.375192165374756,
      "learning_rate": 1.1122776086363785e-05,
      "loss": 0.633,
      "step": 14060
    },
    {
      "epoch": 4.09025,
      "grad_norm": 4.371594429016113,
      "learning_rate": 1.1105589064599744e-05,
      "loss": 0.5933,
      "step": 14065
    },
    {
      "epoch": 4.0905,
      "grad_norm": 4.4134674072265625,
      "learning_rate": 1.1088411539534563e-05,
      "loss": 0.5521,
      "step": 14070
    },
    {
      "epoch": 4.09075,
      "grad_norm": 5.523902416229248,
      "learning_rate": 1.1071243522908916e-05,
      "loss": 0.5291,
      "step": 14075
    },
    {
      "epoch": 4.091,
      "grad_norm": 4.189708709716797,
      "learning_rate": 1.105408502645699e-05,
      "loss": 0.5823,
      "step": 14080
    },
    {
      "epoch": 4.09125,
      "grad_norm": 4.395153045654297,
      "learning_rate": 1.1036936061906444e-05,
      "loss": 0.5996,
      "step": 14085
    },
    {
      "epoch": 4.0915,
      "grad_norm": 5.6569132804870605,
      "learning_rate": 1.101979664097843e-05,
      "loss": 0.6764,
      "step": 14090
    },
    {
      "epoch": 4.09175,
      "grad_norm": 3.6897659301757812,
      "learning_rate": 1.1002666775387584e-05,
      "loss": 0.724,
      "step": 14095
    },
    {
      "epoch": 4.092,
      "grad_norm": 4.291216850280762,
      "learning_rate": 1.0985546476842013e-05,
      "loss": 0.5779,
      "step": 14100
    },
    {
      "epoch": 4.09225,
      "grad_norm": 4.7038893699646,
      "learning_rate": 1.0968435757043275e-05,
      "loss": 0.6595,
      "step": 14105
    },
    {
      "epoch": 4.0925,
      "grad_norm": 5.199921607971191,
      "learning_rate": 1.0951334627686394e-05,
      "loss": 0.6146,
      "step": 14110
    },
    {
      "epoch": 4.09275,
      "grad_norm": 3.8703246116638184,
      "learning_rate": 1.0934243100459817e-05,
      "loss": 0.6622,
      "step": 14115
    },
    {
      "epoch": 4.093,
      "grad_norm": 4.445384502410889,
      "learning_rate": 1.0917161187045449e-05,
      "loss": 0.6185,
      "step": 14120
    },
    {
      "epoch": 4.09325,
      "grad_norm": 4.54575777053833,
      "learning_rate": 1.0900088899118616e-05,
      "loss": 0.6536,
      "step": 14125
    },
    {
      "epoch": 4.0935,
      "grad_norm": 3.7846763134002686,
      "learning_rate": 1.0883026248348076e-05,
      "loss": 0.6446,
      "step": 14130
    },
    {
      "epoch": 4.09375,
      "grad_norm": 6.004998207092285,
      "learning_rate": 1.086597324639598e-05,
      "loss": 0.7847,
      "step": 14135
    },
    {
      "epoch": 4.094,
      "grad_norm": 3.33477783203125,
      "learning_rate": 1.0848929904917884e-05,
      "loss": 0.6136,
      "step": 14140
    },
    {
      "epoch": 4.09425,
      "grad_norm": 4.183468341827393,
      "learning_rate": 1.0831896235562764e-05,
      "loss": 0.5742,
      "step": 14145
    },
    {
      "epoch": 4.0945,
      "grad_norm": 3.267080068588257,
      "learning_rate": 1.0814872249972966e-05,
      "loss": 0.5612,
      "step": 14150
    },
    {
      "epoch": 4.09475,
      "grad_norm": 4.419958591461182,
      "learning_rate": 1.079785795978423e-05,
      "loss": 0.6471,
      "step": 14155
    },
    {
      "epoch": 4.095,
      "grad_norm": 4.620299339294434,
      "learning_rate": 1.0780853376625668e-05,
      "loss": 0.6722,
      "step": 14160
    },
    {
      "epoch": 4.09525,
      "grad_norm": 3.681732654571533,
      "learning_rate": 1.0763858512119737e-05,
      "loss": 0.6079,
      "step": 14165
    },
    {
      "epoch": 4.0955,
      "grad_norm": 4.948702335357666,
      "learning_rate": 1.0746873377882272e-05,
      "loss": 0.6807,
      "step": 14170
    },
    {
      "epoch": 4.09575,
      "grad_norm": 6.187085151672363,
      "learning_rate": 1.0729897985522455e-05,
      "loss": 0.7543,
      "step": 14175
    },
    {
      "epoch": 4.096,
      "grad_norm": 5.7120256423950195,
      "learning_rate": 1.0712932346642817e-05,
      "loss": 0.7568,
      "step": 14180
    },
    {
      "epoch": 4.09625,
      "grad_norm": 5.514833450317383,
      "learning_rate": 1.06959764728392e-05,
      "loss": 0.6879,
      "step": 14185
    },
    {
      "epoch": 4.0965,
      "grad_norm": 4.916370391845703,
      "learning_rate": 1.0679030375700774e-05,
      "loss": 0.7039,
      "step": 14190
    },
    {
      "epoch": 4.09675,
      "grad_norm": 5.620565891265869,
      "learning_rate": 1.066209406681005e-05,
      "loss": 0.7176,
      "step": 14195
    },
    {
      "epoch": 4.097,
      "grad_norm": 5.5150227546691895,
      "learning_rate": 1.064516755774283e-05,
      "loss": 0.6675,
      "step": 14200
    },
    {
      "epoch": 4.09725,
      "grad_norm": 4.223289966583252,
      "learning_rate": 1.0628250860068228e-05,
      "loss": 0.6598,
      "step": 14205
    },
    {
      "epoch": 4.0975,
      "grad_norm": 7.070479869842529,
      "learning_rate": 1.0611343985348658e-05,
      "loss": 0.7521,
      "step": 14210
    },
    {
      "epoch": 4.09775,
      "grad_norm": 5.012413501739502,
      "learning_rate": 1.0594446945139783e-05,
      "loss": 0.6804,
      "step": 14215
    },
    {
      "epoch": 4.098,
      "grad_norm": 6.167741298675537,
      "learning_rate": 1.0577559750990586e-05,
      "loss": 0.755,
      "step": 14220
    },
    {
      "epoch": 4.09825,
      "grad_norm": 4.1939544677734375,
      "learning_rate": 1.0560682414443315e-05,
      "loss": 0.621,
      "step": 14225
    },
    {
      "epoch": 4.0985,
      "grad_norm": 5.999685287475586,
      "learning_rate": 1.0543814947033447e-05,
      "loss": 0.7564,
      "step": 14230
    },
    {
      "epoch": 4.09875,
      "grad_norm": 4.706594944000244,
      "learning_rate": 1.052695736028976e-05,
      "loss": 0.6975,
      "step": 14235
    },
    {
      "epoch": 4.099,
      "grad_norm": 4.750741481781006,
      "learning_rate": 1.0510109665734236e-05,
      "loss": 0.6669,
      "step": 14240
    },
    {
      "epoch": 4.09925,
      "grad_norm": 5.410688400268555,
      "learning_rate": 1.0493271874882124e-05,
      "loss": 0.7268,
      "step": 14245
    },
    {
      "epoch": 4.0995,
      "grad_norm": 4.149649143218994,
      "learning_rate": 1.0476443999241894e-05,
      "loss": 0.6777,
      "step": 14250
    },
    {
      "epoch": 4.09975,
      "grad_norm": 4.201192378997803,
      "learning_rate": 1.0459626050315241e-05,
      "loss": 0.5743,
      "step": 14255
    },
    {
      "epoch": 4.1,
      "grad_norm": 4.063164234161377,
      "learning_rate": 1.0442818039597085e-05,
      "loss": 0.7127,
      "step": 14260
    },
    {
      "epoch": 4.10025,
      "grad_norm": 5.727099895477295,
      "learning_rate": 1.0426019978575524e-05,
      "loss": 0.7211,
      "step": 14265
    },
    {
      "epoch": 4.1005,
      "grad_norm": 4.594606876373291,
      "learning_rate": 1.0409231878731893e-05,
      "loss": 0.5713,
      "step": 14270
    },
    {
      "epoch": 4.10075,
      "grad_norm": 4.251036167144775,
      "learning_rate": 1.039245375154068e-05,
      "loss": 0.5899,
      "step": 14275
    },
    {
      "epoch": 4.101,
      "grad_norm": 5.772572040557861,
      "learning_rate": 1.0375685608469587e-05,
      "loss": 0.6413,
      "step": 14280
    },
    {
      "epoch": 4.10125,
      "grad_norm": 5.552082538604736,
      "learning_rate": 1.0358927460979493e-05,
      "loss": 0.7868,
      "step": 14285
    },
    {
      "epoch": 4.1015,
      "grad_norm": 3.812007427215576,
      "learning_rate": 1.034217932052441e-05,
      "loss": 0.5966,
      "step": 14290
    },
    {
      "epoch": 4.10175,
      "grad_norm": 3.4843146800994873,
      "learning_rate": 1.0325441198551546e-05,
      "loss": 0.5796,
      "step": 14295
    },
    {
      "epoch": 4.102,
      "grad_norm": 5.162761211395264,
      "learning_rate": 1.0308713106501253e-05,
      "loss": 0.5996,
      "step": 14300
    },
    {
      "epoch": 4.10225,
      "grad_norm": 4.5834856033325195,
      "learning_rate": 1.0291995055807019e-05,
      "loss": 0.6485,
      "step": 14305
    },
    {
      "epoch": 4.1025,
      "grad_norm": 4.069332122802734,
      "learning_rate": 1.0275287057895485e-05,
      "loss": 0.6005,
      "step": 14310
    },
    {
      "epoch": 4.10275,
      "grad_norm": 4.236843109130859,
      "learning_rate": 1.0258589124186404e-05,
      "loss": 0.5765,
      "step": 14315
    },
    {
      "epoch": 4.103,
      "grad_norm": 4.872108459472656,
      "learning_rate": 1.0241901266092643e-05,
      "loss": 0.6483,
      "step": 14320
    },
    {
      "epoch": 4.10325,
      "grad_norm": 4.283965587615967,
      "learning_rate": 1.0225223495020206e-05,
      "loss": 0.5544,
      "step": 14325
    },
    {
      "epoch": 4.1035,
      "grad_norm": 4.458291530609131,
      "learning_rate": 1.020855582236819e-05,
      "loss": 0.6583,
      "step": 14330
    },
    {
      "epoch": 4.10375,
      "grad_norm": 3.6730053424835205,
      "learning_rate": 1.0191898259528807e-05,
      "loss": 0.5295,
      "step": 14335
    },
    {
      "epoch": 4.104,
      "grad_norm": 4.025363922119141,
      "learning_rate": 1.0175250817887318e-05,
      "loss": 0.6807,
      "step": 14340
    },
    {
      "epoch": 4.10425,
      "grad_norm": 4.350643157958984,
      "learning_rate": 1.0158613508822112e-05,
      "loss": 0.6656,
      "step": 14345
    },
    {
      "epoch": 4.1045,
      "grad_norm": 3.968745708465576,
      "learning_rate": 1.0141986343704624e-05,
      "loss": 0.5933,
      "step": 14350
    },
    {
      "epoch": 4.10475,
      "grad_norm": 4.712459564208984,
      "learning_rate": 1.0125369333899364e-05,
      "loss": 0.5672,
      "step": 14355
    },
    {
      "epoch": 4.105,
      "grad_norm": 4.969752788543701,
      "learning_rate": 1.010876249076392e-05,
      "loss": 0.6412,
      "step": 14360
    },
    {
      "epoch": 4.10525,
      "grad_norm": 4.960872650146484,
      "learning_rate": 1.0092165825648895e-05,
      "loss": 0.6095,
      "step": 14365
    },
    {
      "epoch": 4.1055,
      "grad_norm": 5.248045921325684,
      "learning_rate": 1.0075579349897945e-05,
      "loss": 0.5914,
      "step": 14370
    },
    {
      "epoch": 4.10575,
      "grad_norm": 4.683914661407471,
      "learning_rate": 1.0059003074847781e-05,
      "loss": 0.5875,
      "step": 14375
    },
    {
      "epoch": 4.106,
      "grad_norm": 4.741829872131348,
      "learning_rate": 1.0042437011828133e-05,
      "loss": 0.583,
      "step": 14380
    },
    {
      "epoch": 4.10625,
      "grad_norm": 3.78967022895813,
      "learning_rate": 1.0025881172161752e-05,
      "loss": 0.5643,
      "step": 14385
    },
    {
      "epoch": 4.1065,
      "grad_norm": 5.89534330368042,
      "learning_rate": 1.0009335567164385e-05,
      "loss": 0.6141,
      "step": 14390
    },
    {
      "epoch": 4.10675,
      "grad_norm": 4.133060932159424,
      "learning_rate": 9.992800208144803e-06,
      "loss": 0.6067,
      "step": 14395
    },
    {
      "epoch": 4.107,
      "grad_norm": 5.809391498565674,
      "learning_rate": 9.97627510640477e-06,
      "loss": 0.6734,
      "step": 14400
    },
    {
      "epoch": 4.10725,
      "grad_norm": 4.356547832489014,
      "learning_rate": 9.959760273239035e-06,
      "loss": 0.5201,
      "step": 14405
    },
    {
      "epoch": 4.1075,
      "grad_norm": 4.143548965454102,
      "learning_rate": 9.943255719935344e-06,
      "loss": 0.6208,
      "step": 14410
    },
    {
      "epoch": 4.10775,
      "grad_norm": 3.7383792400360107,
      "learning_rate": 9.926761457774389e-06,
      "loss": 0.6033,
      "step": 14415
    },
    {
      "epoch": 4.108,
      "grad_norm": 5.29936408996582,
      "learning_rate": 9.910277498029838e-06,
      "loss": 0.6006,
      "step": 14420
    },
    {
      "epoch": 4.10825,
      "grad_norm": 3.9439854621887207,
      "learning_rate": 9.89380385196833e-06,
      "loss": 0.5777,
      "step": 14425
    },
    {
      "epoch": 4.1085,
      "grad_norm": 4.015584468841553,
      "learning_rate": 9.877340530849447e-06,
      "loss": 0.563,
      "step": 14430
    },
    {
      "epoch": 4.10875,
      "grad_norm": 4.567704200744629,
      "learning_rate": 9.86088754592572e-06,
      "loss": 0.5609,
      "step": 14435
    },
    {
      "epoch": 4.109,
      "grad_norm": 4.214756488800049,
      "learning_rate": 9.844444908442592e-06,
      "loss": 0.619,
      "step": 14440
    },
    {
      "epoch": 4.10925,
      "grad_norm": 4.963472843170166,
      "learning_rate": 9.828012629638464e-06,
      "loss": 0.6108,
      "step": 14445
    },
    {
      "epoch": 4.1095,
      "grad_norm": 5.302426338195801,
      "learning_rate": 9.81159072074464e-06,
      "loss": 0.5419,
      "step": 14450
    },
    {
      "epoch": 4.10975,
      "grad_norm": 3.7512130737304688,
      "learning_rate": 9.795179192985349e-06,
      "loss": 0.5503,
      "step": 14455
    },
    {
      "epoch": 4.11,
      "grad_norm": 3.843492269515991,
      "learning_rate": 9.778778057577695e-06,
      "loss": 0.5778,
      "step": 14460
    },
    {
      "epoch": 4.11025,
      "grad_norm": 4.778083324432373,
      "learning_rate": 9.762387325731723e-06,
      "loss": 0.5592,
      "step": 14465
    },
    {
      "epoch": 4.1105,
      "grad_norm": 4.23845911026001,
      "learning_rate": 9.746007008650321e-06,
      "loss": 0.5862,
      "step": 14470
    },
    {
      "epoch": 4.11075,
      "grad_norm": 5.235462665557861,
      "learning_rate": 9.729637117529295e-06,
      "loss": 0.808,
      "step": 14475
    },
    {
      "epoch": 4.111,
      "grad_norm": 4.277624130249023,
      "learning_rate": 9.713277663557304e-06,
      "loss": 0.6651,
      "step": 14480
    },
    {
      "epoch": 4.11125,
      "grad_norm": 6.122304439544678,
      "learning_rate": 9.696928657915896e-06,
      "loss": 0.7603,
      "step": 14485
    },
    {
      "epoch": 4.1115,
      "grad_norm": 4.650869846343994,
      "learning_rate": 9.680590111779441e-06,
      "loss": 0.7057,
      "step": 14490
    },
    {
      "epoch": 4.11175,
      "grad_norm": 4.674257755279541,
      "learning_rate": 9.664262036315194e-06,
      "loss": 0.7312,
      "step": 14495
    },
    {
      "epoch": 4.112,
      "grad_norm": 5.5741801261901855,
      "learning_rate": 9.647944442683235e-06,
      "loss": 0.7634,
      "step": 14500
    },
    {
      "epoch": 4.112,
      "eval_loss": 2.2134718894958496,
      "eval_runtime": 5.359,
      "eval_samples_per_second": 191.082,
      "eval_steps_per_second": 23.885,
      "step": 14500
    },
    {
      "epoch": 4.11225,
      "grad_norm": 4.447668552398682,
      "learning_rate": 9.631637342036497e-06,
      "loss": 0.6435,
      "step": 14505
    },
    {
      "epoch": 4.1125,
      "grad_norm": 4.698235511779785,
      "learning_rate": 9.615340745520713e-06,
      "loss": 0.8834,
      "step": 14510
    },
    {
      "epoch": 4.11275,
      "grad_norm": 3.861675262451172,
      "learning_rate": 9.599054664274471e-06,
      "loss": 0.6858,
      "step": 14515
    },
    {
      "epoch": 4.113,
      "grad_norm": 4.5168304443359375,
      "learning_rate": 9.582779109429132e-06,
      "loss": 0.7104,
      "step": 14520
    },
    {
      "epoch": 4.11325,
      "grad_norm": 4.764861106872559,
      "learning_rate": 9.566514092108898e-06,
      "loss": 0.7371,
      "step": 14525
    },
    {
      "epoch": 4.1135,
      "grad_norm": 4.687399864196777,
      "learning_rate": 9.550259623430752e-06,
      "loss": 0.677,
      "step": 14530
    },
    {
      "epoch": 4.11375,
      "grad_norm": 4.497411727905273,
      "learning_rate": 9.53401571450448e-06,
      "loss": 0.6287,
      "step": 14535
    },
    {
      "epoch": 4.114,
      "grad_norm": 4.563146114349365,
      "learning_rate": 9.517782376432624e-06,
      "loss": 0.6879,
      "step": 14540
    },
    {
      "epoch": 4.11425,
      "grad_norm": 5.323873996734619,
      "learning_rate": 9.501559620310523e-06,
      "loss": 0.6821,
      "step": 14545
    },
    {
      "epoch": 4.1145,
      "grad_norm": 4.927857875823975,
      "learning_rate": 9.48534745722629e-06,
      "loss": 0.7632,
      "step": 14550
    },
    {
      "epoch": 4.11475,
      "grad_norm": 4.531391143798828,
      "learning_rate": 9.469145898260767e-06,
      "loss": 0.6351,
      "step": 14555
    },
    {
      "epoch": 4.115,
      "grad_norm": 6.528877258300781,
      "learning_rate": 9.452954954487576e-06,
      "loss": 0.7937,
      "step": 14560
    },
    {
      "epoch": 4.11525,
      "grad_norm": 6.924418926239014,
      "learning_rate": 9.43677463697308e-06,
      "loss": 0.728,
      "step": 14565
    },
    {
      "epoch": 4.1155,
      "grad_norm": 4.3597846031188965,
      "learning_rate": 9.42060495677636e-06,
      "loss": 0.4957,
      "step": 14570
    },
    {
      "epoch": 4.11575,
      "grad_norm": 4.866530418395996,
      "learning_rate": 9.404445924949246e-06,
      "loss": 0.6029,
      "step": 14575
    },
    {
      "epoch": 4.116,
      "grad_norm": 4.935437202453613,
      "learning_rate": 9.388297552536288e-06,
      "loss": 0.6648,
      "step": 14580
    },
    {
      "epoch": 4.11625,
      "grad_norm": 5.141110420227051,
      "learning_rate": 9.372159850574749e-06,
      "loss": 0.6553,
      "step": 14585
    },
    {
      "epoch": 4.1165,
      "grad_norm": 5.204363822937012,
      "learning_rate": 9.356032830094583e-06,
      "loss": 0.6007,
      "step": 14590
    },
    {
      "epoch": 4.11675,
      "grad_norm": 4.861269950866699,
      "learning_rate": 9.339916502118474e-06,
      "loss": 0.6241,
      "step": 14595
    },
    {
      "epoch": 4.117,
      "grad_norm": 3.6430294513702393,
      "learning_rate": 9.323810877661756e-06,
      "loss": 0.604,
      "step": 14600
    },
    {
      "epoch": 4.11725,
      "grad_norm": 3.924320936203003,
      "learning_rate": 9.307715967732491e-06,
      "loss": 0.6421,
      "step": 14605
    },
    {
      "epoch": 4.1175,
      "grad_norm": 4.054771900177002,
      "learning_rate": 9.29163178333139e-06,
      "loss": 0.5686,
      "step": 14610
    },
    {
      "epoch": 4.11775,
      "grad_norm": 4.81535530090332,
      "learning_rate": 9.275558335451854e-06,
      "loss": 0.5722,
      "step": 14615
    },
    {
      "epoch": 4.118,
      "grad_norm": 5.305436611175537,
      "learning_rate": 9.259495635079915e-06,
      "loss": 0.583,
      "step": 14620
    },
    {
      "epoch": 4.11825,
      "grad_norm": 4.640480041503906,
      "learning_rate": 9.243443693194284e-06,
      "loss": 0.5801,
      "step": 14625
    },
    {
      "epoch": 4.1185,
      "grad_norm": 3.603695869445801,
      "learning_rate": 9.227402520766313e-06,
      "loss": 0.5193,
      "step": 14630
    },
    {
      "epoch": 4.11875,
      "grad_norm": 4.919622898101807,
      "learning_rate": 9.211372128759994e-06,
      "loss": 0.6237,
      "step": 14635
    },
    {
      "epoch": 4.119,
      "grad_norm": 4.650335311889648,
      "learning_rate": 9.195352528131953e-06,
      "loss": 0.7049,
      "step": 14640
    },
    {
      "epoch": 4.11925,
      "grad_norm": 5.1557722091674805,
      "learning_rate": 9.179343729831432e-06,
      "loss": 0.6416,
      "step": 14645
    },
    {
      "epoch": 4.1195,
      "grad_norm": 5.472177028656006,
      "learning_rate": 9.163345744800284e-06,
      "loss": 0.5701,
      "step": 14650
    },
    {
      "epoch": 4.11975,
      "grad_norm": 5.02823543548584,
      "learning_rate": 9.147358583972988e-06,
      "loss": 0.6732,
      "step": 14655
    },
    {
      "epoch": 4.12,
      "grad_norm": 4.564839839935303,
      "learning_rate": 9.131382258276617e-06,
      "loss": 0.6269,
      "step": 14660
    },
    {
      "epoch": 4.12025,
      "grad_norm": 5.038305282592773,
      "learning_rate": 9.11541677863085e-06,
      "loss": 0.5437,
      "step": 14665
    },
    {
      "epoch": 4.1205,
      "grad_norm": 3.8326010704040527,
      "learning_rate": 9.099462155947921e-06,
      "loss": 0.5711,
      "step": 14670
    },
    {
      "epoch": 4.12075,
      "grad_norm": 5.5463056564331055,
      "learning_rate": 9.083518401132679e-06,
      "loss": 0.6954,
      "step": 14675
    },
    {
      "epoch": 4.121,
      "grad_norm": 5.340569972991943,
      "learning_rate": 9.067585525082526e-06,
      "loss": 0.7042,
      "step": 14680
    },
    {
      "epoch": 4.12125,
      "grad_norm": 3.4800946712493896,
      "learning_rate": 9.051663538687432e-06,
      "loss": 0.5544,
      "step": 14685
    },
    {
      "epoch": 4.1215,
      "grad_norm": 4.389000415802002,
      "learning_rate": 9.035752452829944e-06,
      "loss": 0.6773,
      "step": 14690
    },
    {
      "epoch": 4.12175,
      "grad_norm": 5.432769775390625,
      "learning_rate": 9.019852278385102e-06,
      "loss": 0.5735,
      "step": 14695
    },
    {
      "epoch": 4.122,
      "grad_norm": 4.459991931915283,
      "learning_rate": 9.003963026220545e-06,
      "loss": 0.6453,
      "step": 14700
    },
    {
      "epoch": 4.12225,
      "grad_norm": 4.685611724853516,
      "learning_rate": 8.988084707196423e-06,
      "loss": 0.596,
      "step": 14705
    },
    {
      "epoch": 4.1225,
      "grad_norm": 5.179200172424316,
      "learning_rate": 8.972217332165422e-06,
      "loss": 0.6109,
      "step": 14710
    },
    {
      "epoch": 4.12275,
      "grad_norm": 4.869498252868652,
      "learning_rate": 8.956360911972753e-06,
      "loss": 0.6251,
      "step": 14715
    },
    {
      "epoch": 4.123,
      "grad_norm": 4.140378475189209,
      "learning_rate": 8.940515457456106e-06,
      "loss": 0.561,
      "step": 14720
    },
    {
      "epoch": 4.12325,
      "grad_norm": 4.618786334991455,
      "learning_rate": 8.924680979445713e-06,
      "loss": 0.6047,
      "step": 14725
    },
    {
      "epoch": 4.1235,
      "grad_norm": 4.050108909606934,
      "learning_rate": 8.908857488764288e-06,
      "loss": 0.633,
      "step": 14730
    },
    {
      "epoch": 4.12375,
      "grad_norm": 4.064322471618652,
      "learning_rate": 8.893044996227052e-06,
      "loss": 0.5988,
      "step": 14735
    },
    {
      "epoch": 4.124,
      "grad_norm": 4.671445846557617,
      "learning_rate": 8.877243512641666e-06,
      "loss": 0.5885,
      "step": 14740
    },
    {
      "epoch": 4.12425,
      "grad_norm": 4.878427505493164,
      "learning_rate": 8.861453048808325e-06,
      "loss": 0.5735,
      "step": 14745
    },
    {
      "epoch": 4.1245,
      "grad_norm": 4.461660861968994,
      "learning_rate": 8.845673615519637e-06,
      "loss": 0.6312,
      "step": 14750
    },
    {
      "epoch": 4.12475,
      "grad_norm": 4.23435115814209,
      "learning_rate": 8.829905223560708e-06,
      "loss": 0.5233,
      "step": 14755
    },
    {
      "epoch": 4.125,
      "grad_norm": 5.144657135009766,
      "learning_rate": 8.814147883709082e-06,
      "loss": 0.6441,
      "step": 14760
    },
    {
      "epoch": 4.12525,
      "grad_norm": 3.610673189163208,
      "learning_rate": 8.798401606734763e-06,
      "loss": 0.5914,
      "step": 14765
    },
    {
      "epoch": 4.1255,
      "grad_norm": 3.837184190750122,
      "learning_rate": 8.782666403400166e-06,
      "loss": 0.6238,
      "step": 14770
    },
    {
      "epoch": 4.12575,
      "grad_norm": 4.647897720336914,
      "learning_rate": 8.766942284460161e-06,
      "loss": 0.5278,
      "step": 14775
    },
    {
      "epoch": 4.126,
      "grad_norm": 5.113662242889404,
      "learning_rate": 8.75122926066204e-06,
      "loss": 0.5599,
      "step": 14780
    },
    {
      "epoch": 4.12625,
      "grad_norm": 3.6883656978607178,
      "learning_rate": 8.735527342745506e-06,
      "loss": 0.543,
      "step": 14785
    },
    {
      "epoch": 4.1265,
      "grad_norm": 6.171414375305176,
      "learning_rate": 8.719836541442661e-06,
      "loss": 0.6011,
      "step": 14790
    },
    {
      "epoch": 4.12675,
      "grad_norm": 4.755934715270996,
      "learning_rate": 8.704156867478036e-06,
      "loss": 0.583,
      "step": 14795
    },
    {
      "epoch": 4.127,
      "grad_norm": 3.753232479095459,
      "learning_rate": 8.688488331568523e-06,
      "loss": 0.5804,
      "step": 14800
    },
    {
      "epoch": 4.12725,
      "grad_norm": 4.917913913726807,
      "learning_rate": 8.67283094442343e-06,
      "loss": 0.5228,
      "step": 14805
    },
    {
      "epoch": 4.1275,
      "grad_norm": 3.727607250213623,
      "learning_rate": 8.657184716744427e-06,
      "loss": 0.5174,
      "step": 14810
    },
    {
      "epoch": 4.12775,
      "grad_norm": 4.581965923309326,
      "learning_rate": 8.641549659225576e-06,
      "loss": 0.6045,
      "step": 14815
    },
    {
      "epoch": 4.128,
      "grad_norm": 3.8797647953033447,
      "learning_rate": 8.625925782553274e-06,
      "loss": 0.5477,
      "step": 14820
    },
    {
      "epoch": 4.12825,
      "grad_norm": 5.738304138183594,
      "learning_rate": 8.6103130974063e-06,
      "loss": 0.5708,
      "step": 14825
    },
    {
      "epoch": 4.1285,
      "grad_norm": 4.413155555725098,
      "learning_rate": 8.594711614455787e-06,
      "loss": 0.5907,
      "step": 14830
    },
    {
      "epoch": 4.12875,
      "grad_norm": 4.029313087463379,
      "learning_rate": 8.579121344365185e-06,
      "loss": 0.71,
      "step": 14835
    },
    {
      "epoch": 4.129,
      "grad_norm": 3.5641286373138428,
      "learning_rate": 8.563542297790303e-06,
      "loss": 0.4857,
      "step": 14840
    },
    {
      "epoch": 4.12925,
      "grad_norm": 3.179870843887329,
      "learning_rate": 8.547974485379285e-06,
      "loss": 0.5324,
      "step": 14845
    },
    {
      "epoch": 4.1295,
      "grad_norm": 3.4427480697631836,
      "learning_rate": 8.532417917772565e-06,
      "loss": 0.5552,
      "step": 14850
    },
    {
      "epoch": 4.12975,
      "grad_norm": 4.420849800109863,
      "learning_rate": 8.516872605602916e-06,
      "loss": 0.4956,
      "step": 14855
    },
    {
      "epoch": 4.13,
      "grad_norm": 4.85842752456665,
      "learning_rate": 8.501338559495414e-06,
      "loss": 0.5717,
      "step": 14860
    },
    {
      "epoch": 4.13025,
      "grad_norm": 4.376033306121826,
      "learning_rate": 8.485815790067442e-06,
      "loss": 0.5415,
      "step": 14865
    },
    {
      "epoch": 4.1305,
      "grad_norm": 3.7145278453826904,
      "learning_rate": 8.47030430792865e-06,
      "loss": 0.6018,
      "step": 14870
    },
    {
      "epoch": 4.13075,
      "grad_norm": 4.6316633224487305,
      "learning_rate": 8.45480412368101e-06,
      "loss": 0.592,
      "step": 14875
    },
    {
      "epoch": 4.131,
      "grad_norm": 5.142543792724609,
      "learning_rate": 8.439315247918727e-06,
      "loss": 0.5411,
      "step": 14880
    },
    {
      "epoch": 4.13125,
      "grad_norm": 4.99965763092041,
      "learning_rate": 8.423837691228315e-06,
      "loss": 0.5496,
      "step": 14885
    },
    {
      "epoch": 4.1315,
      "grad_norm": 4.055758476257324,
      "learning_rate": 8.408371464188536e-06,
      "loss": 0.5103,
      "step": 14890
    },
    {
      "epoch": 4.13175,
      "grad_norm": 4.181704521179199,
      "learning_rate": 8.392916577370421e-06,
      "loss": 0.5832,
      "step": 14895
    },
    {
      "epoch": 4.132,
      "grad_norm": 3.6568565368652344,
      "learning_rate": 8.377473041337222e-06,
      "loss": 0.5326,
      "step": 14900
    },
    {
      "epoch": 4.13225,
      "grad_norm": 5.8994245529174805,
      "learning_rate": 8.362040866644457e-06,
      "loss": 0.5693,
      "step": 14905
    },
    {
      "epoch": 4.1325,
      "grad_norm": 3.9843225479125977,
      "learning_rate": 8.346620063839874e-06,
      "loss": 0.5686,
      "step": 14910
    },
    {
      "epoch": 4.13275,
      "grad_norm": 3.7738101482391357,
      "learning_rate": 8.331210643463455e-06,
      "loss": 0.57,
      "step": 14915
    },
    {
      "epoch": 4.133,
      "grad_norm": 3.0760369300842285,
      "learning_rate": 8.315812616047375e-06,
      "loss": 0.4681,
      "step": 14920
    },
    {
      "epoch": 4.13325,
      "grad_norm": 5.855804443359375,
      "learning_rate": 8.300425992116057e-06,
      "loss": 0.6206,
      "step": 14925
    },
    {
      "epoch": 4.1335,
      "grad_norm": 3.917670488357544,
      "learning_rate": 8.285050782186097e-06,
      "loss": 0.4961,
      "step": 14930
    },
    {
      "epoch": 4.13375,
      "grad_norm": 3.1710093021392822,
      "learning_rate": 8.269686996766315e-06,
      "loss": 0.4454,
      "step": 14935
    },
    {
      "epoch": 4.134,
      "grad_norm": 4.4252543449401855,
      "learning_rate": 8.254334646357714e-06,
      "loss": 0.537,
      "step": 14940
    },
    {
      "epoch": 4.13425,
      "grad_norm": 4.033843517303467,
      "learning_rate": 8.238993741453488e-06,
      "loss": 0.5195,
      "step": 14945
    },
    {
      "epoch": 4.1345,
      "grad_norm": 3.366896867752075,
      "learning_rate": 8.223664292538985e-06,
      "loss": 0.5632,
      "step": 14950
    },
    {
      "epoch": 4.13475,
      "grad_norm": 5.5923333168029785,
      "learning_rate": 8.208346310091747e-06,
      "loss": 0.574,
      "step": 14955
    },
    {
      "epoch": 4.135,
      "grad_norm": 5.071911334991455,
      "learning_rate": 8.193039804581471e-06,
      "loss": 0.5899,
      "step": 14960
    },
    {
      "epoch": 4.13525,
      "grad_norm": 7.267291069030762,
      "learning_rate": 8.177744786470012e-06,
      "loss": 0.5901,
      "step": 14965
    },
    {
      "epoch": 4.1355,
      "grad_norm": 4.226772785186768,
      "learning_rate": 8.162461266211371e-06,
      "loss": 0.58,
      "step": 14970
    },
    {
      "epoch": 4.13575,
      "grad_norm": 5.271418571472168,
      "learning_rate": 8.147189254251678e-06,
      "loss": 0.5928,
      "step": 14975
    },
    {
      "epoch": 4.136,
      "grad_norm": 6.115705490112305,
      "learning_rate": 8.131928761029217e-06,
      "loss": 0.5581,
      "step": 14980
    },
    {
      "epoch": 4.13625,
      "grad_norm": 3.407331705093384,
      "learning_rate": 8.116679796974388e-06,
      "loss": 0.5604,
      "step": 14985
    },
    {
      "epoch": 4.1365,
      "grad_norm": 3.837200164794922,
      "learning_rate": 8.101442372509722e-06,
      "loss": 0.6439,
      "step": 14990
    },
    {
      "epoch": 4.13675,
      "grad_norm": 3.9525351524353027,
      "learning_rate": 8.086216498049856e-06,
      "loss": 0.5559,
      "step": 14995
    },
    {
      "epoch": 4.1370000000000005,
      "grad_norm": 6.760684967041016,
      "learning_rate": 8.07100218400152e-06,
      "loss": 0.6596,
      "step": 15000
    },
    {
      "epoch": 4.1370000000000005,
      "eval_loss": 2.2364377975463867,
      "eval_runtime": 5.2012,
      "eval_samples_per_second": 196.878,
      "eval_steps_per_second": 24.61,
      "step": 15000
    },
    {
      "epoch": 4.13725,
      "grad_norm": 4.686863422393799,
      "learning_rate": 8.05579944076356e-06,
      "loss": 0.645,
      "step": 15005
    },
    {
      "epoch": 4.1375,
      "grad_norm": 4.16525936126709,
      "learning_rate": 8.040608278726907e-06,
      "loss": 0.4907,
      "step": 15010
    },
    {
      "epoch": 4.13775,
      "grad_norm": 5.959747791290283,
      "learning_rate": 8.025428708274584e-06,
      "loss": 0.5617,
      "step": 15015
    },
    {
      "epoch": 4.138,
      "grad_norm": 3.814178705215454,
      "learning_rate": 8.010260739781684e-06,
      "loss": 0.596,
      "step": 15020
    },
    {
      "epoch": 4.13825,
      "grad_norm": 5.289906024932861,
      "learning_rate": 7.995104383615352e-06,
      "loss": 0.5293,
      "step": 15025
    },
    {
      "epoch": 4.1385,
      "grad_norm": 5.0775532722473145,
      "learning_rate": 7.979959650134827e-06,
      "loss": 0.6764,
      "step": 15030
    },
    {
      "epoch": 4.13875,
      "grad_norm": 4.511828899383545,
      "learning_rate": 7.964826549691396e-06,
      "loss": 0.6435,
      "step": 15035
    },
    {
      "epoch": 4.139,
      "grad_norm": 3.9715347290039062,
      "learning_rate": 7.949705092628382e-06,
      "loss": 0.5367,
      "step": 15040
    },
    {
      "epoch": 4.13925,
      "grad_norm": 4.667386054992676,
      "learning_rate": 7.934595289281172e-06,
      "loss": 0.5978,
      "step": 15045
    },
    {
      "epoch": 4.1395,
      "grad_norm": 3.471841335296631,
      "learning_rate": 7.919497149977156e-06,
      "loss": 0.5148,
      "step": 15050
    },
    {
      "epoch": 4.13975,
      "grad_norm": 3.9852452278137207,
      "learning_rate": 7.90441068503578e-06,
      "loss": 0.5761,
      "step": 15055
    },
    {
      "epoch": 4.14,
      "grad_norm": 4.935863494873047,
      "learning_rate": 7.889335904768497e-06,
      "loss": 0.5219,
      "step": 15060
    },
    {
      "epoch": 4.14025,
      "grad_norm": 5.378962993621826,
      "learning_rate": 7.87427281947879e-06,
      "loss": 0.5904,
      "step": 15065
    },
    {
      "epoch": 4.1405,
      "grad_norm": 4.371437072753906,
      "learning_rate": 7.859221439462123e-06,
      "loss": 0.5305,
      "step": 15070
    },
    {
      "epoch": 4.14075,
      "grad_norm": 4.684712886810303,
      "learning_rate": 7.844181775005965e-06,
      "loss": 0.5553,
      "step": 15075
    },
    {
      "epoch": 4.141,
      "grad_norm": 3.6882784366607666,
      "learning_rate": 7.829153836389796e-06,
      "loss": 0.5116,
      "step": 15080
    },
    {
      "epoch": 4.14125,
      "grad_norm": 4.7946553230285645,
      "learning_rate": 7.814137633885068e-06,
      "loss": 0.5425,
      "step": 15085
    },
    {
      "epoch": 4.1415,
      "grad_norm": 3.940222978591919,
      "learning_rate": 7.799133177755214e-06,
      "loss": 0.5126,
      "step": 15090
    },
    {
      "epoch": 4.14175,
      "grad_norm": 3.7091281414031982,
      "learning_rate": 7.784140478255647e-06,
      "loss": 0.4926,
      "step": 15095
    },
    {
      "epoch": 4.142,
      "grad_norm": 4.1784234046936035,
      "learning_rate": 7.76915954563372e-06,
      "loss": 0.5664,
      "step": 15100
    },
    {
      "epoch": 4.14225,
      "grad_norm": 5.409041881561279,
      "learning_rate": 7.754190390128766e-06,
      "loss": 0.5445,
      "step": 15105
    },
    {
      "epoch": 4.1425,
      "grad_norm": 3.940187931060791,
      "learning_rate": 7.739233021972067e-06,
      "loss": 0.4855,
      "step": 15110
    },
    {
      "epoch": 4.14275,
      "grad_norm": 3.0881507396698,
      "learning_rate": 7.724287451386833e-06,
      "loss": 0.5514,
      "step": 15115
    },
    {
      "epoch": 4.143,
      "grad_norm": 3.8793399333953857,
      "learning_rate": 7.709353688588234e-06,
      "loss": 0.5025,
      "step": 15120
    },
    {
      "epoch": 4.14325,
      "grad_norm": 3.9071736335754395,
      "learning_rate": 7.69443174378334e-06,
      "loss": 0.5357,
      "step": 15125
    },
    {
      "epoch": 4.1435,
      "grad_norm": 5.668573379516602,
      "learning_rate": 7.679521627171166e-06,
      "loss": 0.6965,
      "step": 15130
    },
    {
      "epoch": 4.14375,
      "grad_norm": 4.496301174163818,
      "learning_rate": 7.664623348942637e-06,
      "loss": 0.5225,
      "step": 15135
    },
    {
      "epoch": 4.144,
      "grad_norm": 4.803035259246826,
      "learning_rate": 7.64973691928059e-06,
      "loss": 0.4969,
      "step": 15140
    },
    {
      "epoch": 4.1442499999999995,
      "grad_norm": 3.2352778911590576,
      "learning_rate": 7.634862348359758e-06,
      "loss": 0.5537,
      "step": 15145
    },
    {
      "epoch": 4.1445,
      "grad_norm": 4.027238845825195,
      "learning_rate": 7.619999646346762e-06,
      "loss": 0.4588,
      "step": 15150
    },
    {
      "epoch": 4.14475,
      "grad_norm": 3.5976204872131348,
      "learning_rate": 7.605148823400135e-06,
      "loss": 0.4864,
      "step": 15155
    },
    {
      "epoch": 4.145,
      "grad_norm": 5.266451358795166,
      "learning_rate": 7.590309889670253e-06,
      "loss": 0.5483,
      "step": 15160
    },
    {
      "epoch": 4.14525,
      "grad_norm": 3.524285078048706,
      "learning_rate": 7.5754828552993955e-06,
      "loss": 0.4353,
      "step": 15165
    },
    {
      "epoch": 4.1455,
      "grad_norm": 5.671149253845215,
      "learning_rate": 7.560667730421714e-06,
      "loss": 0.5137,
      "step": 15170
    },
    {
      "epoch": 4.14575,
      "grad_norm": 3.804666519165039,
      "learning_rate": 7.545864525163188e-06,
      "loss": 0.5336,
      "step": 15175
    },
    {
      "epoch": 4.146,
      "grad_norm": 3.8627116680145264,
      "learning_rate": 7.531073249641674e-06,
      "loss": 0.4748,
      "step": 15180
    },
    {
      "epoch": 4.14625,
      "grad_norm": 3.4364585876464844,
      "learning_rate": 7.5162939139668735e-06,
      "loss": 0.4729,
      "step": 15185
    },
    {
      "epoch": 4.1465,
      "grad_norm": 10.909317970275879,
      "learning_rate": 7.501526528240322e-06,
      "loss": 1.1452,
      "step": 15190
    },
    {
      "epoch": 4.14675,
      "grad_norm": 9.88249683380127,
      "learning_rate": 7.486771102555392e-06,
      "loss": 1.3801,
      "step": 15195
    },
    {
      "epoch": 4.147,
      "grad_norm": 10.654837608337402,
      "learning_rate": 7.472027646997276e-06,
      "loss": 1.2359,
      "step": 15200
    },
    {
      "epoch": 4.14725,
      "grad_norm": 9.060257911682129,
      "learning_rate": 7.457296171642977e-06,
      "loss": 1.0924,
      "step": 15205
    },
    {
      "epoch": 4.1475,
      "grad_norm": 9.409539222717285,
      "learning_rate": 7.442576686561328e-06,
      "loss": 1.0531,
      "step": 15210
    },
    {
      "epoch": 4.14775,
      "grad_norm": 10.321905136108398,
      "learning_rate": 7.4278692018129535e-06,
      "loss": 1.0309,
      "step": 15215
    },
    {
      "epoch": 4.148,
      "grad_norm": 9.433445930480957,
      "learning_rate": 7.413173727450295e-06,
      "loss": 1.0153,
      "step": 15220
    },
    {
      "epoch": 4.14825,
      "grad_norm": 8.426295280456543,
      "learning_rate": 7.3984902735175525e-06,
      "loss": 0.9276,
      "step": 15225
    },
    {
      "epoch": 4.1485,
      "grad_norm": 6.7446746826171875,
      "learning_rate": 7.383818850050733e-06,
      "loss": 0.9891,
      "step": 15230
    },
    {
      "epoch": 4.14875,
      "grad_norm": 11.77286434173584,
      "learning_rate": 7.369159467077621e-06,
      "loss": 0.962,
      "step": 15235
    },
    {
      "epoch": 4.149,
      "grad_norm": 6.9888176918029785,
      "learning_rate": 7.354512134617764e-06,
      "loss": 0.9078,
      "step": 15240
    },
    {
      "epoch": 4.14925,
      "grad_norm": 6.145602226257324,
      "learning_rate": 7.339876862682482e-06,
      "loss": 0.8491,
      "step": 15245
    },
    {
      "epoch": 4.1495,
      "grad_norm": 7.165505409240723,
      "learning_rate": 7.325253661274842e-06,
      "loss": 0.904,
      "step": 15250
    },
    {
      "epoch": 4.14975,
      "grad_norm": 6.1988205909729,
      "learning_rate": 7.3106425403896535e-06,
      "loss": 0.8387,
      "step": 15255
    },
    {
      "epoch": 4.15,
      "grad_norm": 6.94688081741333,
      "learning_rate": 7.296043510013486e-06,
      "loss": 0.8398,
      "step": 15260
    },
    {
      "epoch": 4.15025,
      "grad_norm": 7.09096622467041,
      "learning_rate": 7.2814565801246436e-06,
      "loss": 0.8326,
      "step": 15265
    },
    {
      "epoch": 4.1505,
      "grad_norm": 6.906554698944092,
      "learning_rate": 7.266881760693159e-06,
      "loss": 0.9618,
      "step": 15270
    },
    {
      "epoch": 4.15075,
      "grad_norm": 10.336700439453125,
      "learning_rate": 7.252319061680771e-06,
      "loss": 0.8723,
      "step": 15275
    },
    {
      "epoch": 4.151,
      "grad_norm": 7.152406215667725,
      "learning_rate": 7.23776849304095e-06,
      "loss": 0.9048,
      "step": 15280
    },
    {
      "epoch": 4.15125,
      "grad_norm": 6.9770050048828125,
      "learning_rate": 7.223230064718881e-06,
      "loss": 1.0613,
      "step": 15285
    },
    {
      "epoch": 4.1515,
      "grad_norm": 5.29350471496582,
      "learning_rate": 7.208703786651435e-06,
      "loss": 0.9159,
      "step": 15290
    },
    {
      "epoch": 4.15175,
      "grad_norm": 8.638554573059082,
      "learning_rate": 7.194189668767198e-06,
      "loss": 0.9018,
      "step": 15295
    },
    {
      "epoch": 4.152,
      "grad_norm": 7.608952045440674,
      "learning_rate": 7.179687720986422e-06,
      "loss": 1.0108,
      "step": 15300
    },
    {
      "epoch": 4.15225,
      "grad_norm": 9.22871208190918,
      "learning_rate": 7.165197953221048e-06,
      "loss": 0.8942,
      "step": 15305
    },
    {
      "epoch": 4.1525,
      "grad_norm": 12.213257789611816,
      "learning_rate": 7.150720375374698e-06,
      "loss": 0.9369,
      "step": 15310
    },
    {
      "epoch": 4.15275,
      "grad_norm": 9.62246036529541,
      "learning_rate": 7.1362549973426645e-06,
      "loss": 1.0635,
      "step": 15315
    },
    {
      "epoch": 4.153,
      "grad_norm": 8.23684024810791,
      "learning_rate": 7.121801829011904e-06,
      "loss": 1.3554,
      "step": 15320
    },
    {
      "epoch": 4.15325,
      "grad_norm": 6.938995838165283,
      "learning_rate": 7.107360880261002e-06,
      "loss": 0.7583,
      "step": 15325
    },
    {
      "epoch": 5.00025,
      "grad_norm": 5.948956489562988,
      "learning_rate": 7.092932160960222e-06,
      "loss": 0.6617,
      "step": 15330
    },
    {
      "epoch": 5.0005,
      "grad_norm": 6.202661514282227,
      "learning_rate": 7.078515680971456e-06,
      "loss": 0.6873,
      "step": 15335
    },
    {
      "epoch": 5.00075,
      "grad_norm": 5.316576957702637,
      "learning_rate": 7.064111450148242e-06,
      "loss": 0.6898,
      "step": 15340
    },
    {
      "epoch": 5.001,
      "grad_norm": 6.838443756103516,
      "learning_rate": 7.0497194783357174e-06,
      "loss": 0.6666,
      "step": 15345
    },
    {
      "epoch": 5.00125,
      "grad_norm": 5.387817859649658,
      "learning_rate": 7.035339775370675e-06,
      "loss": 0.7278,
      "step": 15350
    },
    {
      "epoch": 5.0015,
      "grad_norm": 5.9826154708862305,
      "learning_rate": 7.020972351081493e-06,
      "loss": 0.7001,
      "step": 15355
    },
    {
      "epoch": 5.00175,
      "grad_norm": 4.710649013519287,
      "learning_rate": 7.006617215288178e-06,
      "loss": 0.6099,
      "step": 15360
    },
    {
      "epoch": 5.002,
      "grad_norm": 5.244078636169434,
      "learning_rate": 6.992274377802327e-06,
      "loss": 0.6474,
      "step": 15365
    },
    {
      "epoch": 5.00225,
      "grad_norm": 5.534056186676025,
      "learning_rate": 6.977943848427143e-06,
      "loss": 0.6566,
      "step": 15370
    },
    {
      "epoch": 5.0025,
      "grad_norm": 8.119464874267578,
      "learning_rate": 6.963625636957396e-06,
      "loss": 0.7256,
      "step": 15375
    },
    {
      "epoch": 5.00275,
      "grad_norm": 7.872198581695557,
      "learning_rate": 6.94931975317945e-06,
      "loss": 0.7558,
      "step": 15380
    },
    {
      "epoch": 5.003,
      "grad_norm": 7.869570255279541,
      "learning_rate": 6.935026206871248e-06,
      "loss": 0.7508,
      "step": 15385
    },
    {
      "epoch": 5.00325,
      "grad_norm": 6.943602561950684,
      "learning_rate": 6.9207450078022986e-06,
      "loss": 0.7635,
      "step": 15390
    },
    {
      "epoch": 5.0035,
      "grad_norm": 5.545767307281494,
      "learning_rate": 6.90647616573365e-06,
      "loss": 0.6939,
      "step": 15395
    },
    {
      "epoch": 5.00375,
      "grad_norm": 5.032163143157959,
      "learning_rate": 6.8922196904179395e-06,
      "loss": 0.6392,
      "step": 15400
    },
    {
      "epoch": 5.004,
      "grad_norm": 7.000614643096924,
      "learning_rate": 6.877975591599314e-06,
      "loss": 0.7221,
      "step": 15405
    },
    {
      "epoch": 5.00425,
      "grad_norm": 8.974245071411133,
      "learning_rate": 6.863743879013493e-06,
      "loss": 0.7504,
      "step": 15410
    },
    {
      "epoch": 5.0045,
      "grad_norm": 6.480360507965088,
      "learning_rate": 6.849524562387716e-06,
      "loss": 0.6855,
      "step": 15415
    },
    {
      "epoch": 5.00475,
      "grad_norm": 8.07741928100586,
      "learning_rate": 6.835317651440762e-06,
      "loss": 0.7612,
      "step": 15420
    },
    {
      "epoch": 5.005,
      "grad_norm": 7.422358512878418,
      "learning_rate": 6.8211231558829e-06,
      "loss": 0.7776,
      "step": 15425
    },
    {
      "epoch": 5.00525,
      "grad_norm": 7.135222911834717,
      "learning_rate": 6.806941085415944e-06,
      "loss": 0.7948,
      "step": 15430
    },
    {
      "epoch": 5.0055,
      "grad_norm": 6.587981700897217,
      "learning_rate": 6.792771449733215e-06,
      "loss": 0.8079,
      "step": 15435
    },
    {
      "epoch": 5.00575,
      "grad_norm": 7.5208353996276855,
      "learning_rate": 6.778614258519505e-06,
      "loss": 0.7485,
      "step": 15440
    },
    {
      "epoch": 5.006,
      "grad_norm": 7.569764137268066,
      "learning_rate": 6.764469521451136e-06,
      "loss": 0.777,
      "step": 15445
    },
    {
      "epoch": 5.00625,
      "grad_norm": 7.236279487609863,
      "learning_rate": 6.750337248195907e-06,
      "loss": 0.7561,
      "step": 15450
    },
    {
      "epoch": 5.0065,
      "grad_norm": 7.127793312072754,
      "learning_rate": 6.736217448413073e-06,
      "loss": 0.8018,
      "step": 15455
    },
    {
      "epoch": 5.00675,
      "grad_norm": 6.396053791046143,
      "learning_rate": 6.722110131753398e-06,
      "loss": 0.6746,
      "step": 15460
    },
    {
      "epoch": 5.007,
      "grad_norm": 6.899335861206055,
      "learning_rate": 6.708015307859098e-06,
      "loss": 0.6656,
      "step": 15465
    },
    {
      "epoch": 5.00725,
      "grad_norm": 7.229771614074707,
      "learning_rate": 6.693932986363857e-06,
      "loss": 0.7263,
      "step": 15470
    },
    {
      "epoch": 5.0075,
      "grad_norm": 7.504376411437988,
      "learning_rate": 6.6798631768928e-06,
      "loss": 0.8071,
      "step": 15475
    },
    {
      "epoch": 5.00775,
      "grad_norm": 7.052850246429443,
      "learning_rate": 6.665805889062518e-06,
      "loss": 0.763,
      "step": 15480
    },
    {
      "epoch": 5.008,
      "grad_norm": 6.5388264656066895,
      "learning_rate": 6.6517611324810245e-06,
      "loss": 0.7476,
      "step": 15485
    },
    {
      "epoch": 5.00825,
      "grad_norm": 6.672894477844238,
      "learning_rate": 6.637728916747782e-06,
      "loss": 0.7622,
      "step": 15490
    },
    {
      "epoch": 5.0085,
      "grad_norm": 8.247053146362305,
      "learning_rate": 6.623709251453677e-06,
      "loss": 0.7165,
      "step": 15495
    },
    {
      "epoch": 5.00875,
      "grad_norm": 6.562661170959473,
      "learning_rate": 6.6097021461810325e-06,
      "loss": 0.6572,
      "step": 15500
    },
    {
      "epoch": 5.00875,
      "eval_loss": 2.105620861053467,
      "eval_runtime": 5.1277,
      "eval_samples_per_second": 199.7,
      "eval_steps_per_second": 24.963,
      "step": 15500
    },
    {
      "epoch": 5.009,
      "grad_norm": 8.018153190612793,
      "learning_rate": 6.5957076105035514e-06,
      "loss": 0.7239,
      "step": 15505
    },
    {
      "epoch": 5.00925,
      "grad_norm": 5.688753604888916,
      "learning_rate": 6.581725653986376e-06,
      "loss": 0.6568,
      "step": 15510
    },
    {
      "epoch": 5.0095,
      "grad_norm": 6.4727911949157715,
      "learning_rate": 6.5677562861860415e-06,
      "loss": 0.688,
      "step": 15515
    },
    {
      "epoch": 5.00975,
      "grad_norm": 8.86405086517334,
      "learning_rate": 6.5537995166504885e-06,
      "loss": 0.7618,
      "step": 15520
    },
    {
      "epoch": 5.01,
      "grad_norm": 5.773629665374756,
      "learning_rate": 6.539855354919019e-06,
      "loss": 0.6605,
      "step": 15525
    },
    {
      "epoch": 5.01025,
      "grad_norm": 5.920487880706787,
      "learning_rate": 6.525923810522355e-06,
      "loss": 0.6785,
      "step": 15530
    },
    {
      "epoch": 5.0105,
      "grad_norm": 5.928590774536133,
      "learning_rate": 6.512004892982559e-06,
      "loss": 0.7109,
      "step": 15535
    },
    {
      "epoch": 5.01075,
      "grad_norm": 5.351207733154297,
      "learning_rate": 6.498098611813089e-06,
      "loss": 0.631,
      "step": 15540
    },
    {
      "epoch": 5.011,
      "grad_norm": 5.925400257110596,
      "learning_rate": 6.484204976518751e-06,
      "loss": 0.6734,
      "step": 15545
    },
    {
      "epoch": 5.01125,
      "grad_norm": 5.325272083282471,
      "learning_rate": 6.470323996595728e-06,
      "loss": 0.6277,
      "step": 15550
    },
    {
      "epoch": 5.0115,
      "grad_norm": 5.872792720794678,
      "learning_rate": 6.456455681531523e-06,
      "loss": 0.6531,
      "step": 15555
    },
    {
      "epoch": 5.01175,
      "grad_norm": 6.907093048095703,
      "learning_rate": 6.442600040805002e-06,
      "loss": 0.7083,
      "step": 15560
    },
    {
      "epoch": 5.012,
      "grad_norm": 5.406282901763916,
      "learning_rate": 6.428757083886366e-06,
      "loss": 0.7346,
      "step": 15565
    },
    {
      "epoch": 5.01225,
      "grad_norm": 5.246354579925537,
      "learning_rate": 6.414926820237158e-06,
      "loss": 0.873,
      "step": 15570
    },
    {
      "epoch": 5.0125,
      "grad_norm": 5.520455360412598,
      "learning_rate": 6.40110925931022e-06,
      "loss": 0.7367,
      "step": 15575
    },
    {
      "epoch": 5.01275,
      "grad_norm": 6.061600208282471,
      "learning_rate": 6.387304410549719e-06,
      "loss": 0.7194,
      "step": 15580
    },
    {
      "epoch": 5.013,
      "grad_norm": 6.716187477111816,
      "learning_rate": 6.373512283391145e-06,
      "loss": 0.7095,
      "step": 15585
    },
    {
      "epoch": 5.01325,
      "grad_norm": 6.8930253982543945,
      "learning_rate": 6.359732887261288e-06,
      "loss": 0.7204,
      "step": 15590
    },
    {
      "epoch": 5.0135,
      "grad_norm": 7.044866561889648,
      "learning_rate": 6.345966231578238e-06,
      "loss": 0.659,
      "step": 15595
    },
    {
      "epoch": 5.01375,
      "grad_norm": 4.555063724517822,
      "learning_rate": 6.332212325751377e-06,
      "loss": 0.6921,
      "step": 15600
    },
    {
      "epoch": 5.014,
      "grad_norm": 5.948360443115234,
      "learning_rate": 6.318471179181357e-06,
      "loss": 0.5472,
      "step": 15605
    },
    {
      "epoch": 5.01425,
      "grad_norm": 4.368066787719727,
      "learning_rate": 6.30474280126013e-06,
      "loss": 0.5542,
      "step": 15610
    },
    {
      "epoch": 5.0145,
      "grad_norm": 3.9325270652770996,
      "learning_rate": 6.2910272013709135e-06,
      "loss": 0.522,
      "step": 15615
    },
    {
      "epoch": 5.01475,
      "grad_norm": 4.55904483795166,
      "learning_rate": 6.277324388888198e-06,
      "loss": 0.5289,
      "step": 15620
    },
    {
      "epoch": 5.015,
      "grad_norm": 2.225430488586426,
      "learning_rate": 6.263634373177716e-06,
      "loss": 0.435,
      "step": 15625
    },
    {
      "epoch": 5.01525,
      "grad_norm": 2.1454718112945557,
      "learning_rate": 6.2499571635964594e-06,
      "loss": 0.5403,
      "step": 15630
    },
    {
      "epoch": 5.0155,
      "grad_norm": 3.270900011062622,
      "learning_rate": 6.2362927694926795e-06,
      "loss": 0.4097,
      "step": 15635
    },
    {
      "epoch": 5.01575,
      "grad_norm": 2.1782588958740234,
      "learning_rate": 6.222641200205861e-06,
      "loss": 0.3581,
      "step": 15640
    },
    {
      "epoch": 5.016,
      "grad_norm": 2.122097969055176,
      "learning_rate": 6.209002465066724e-06,
      "loss": 0.4051,
      "step": 15645
    },
    {
      "epoch": 5.01625,
      "grad_norm": 2.1968250274658203,
      "learning_rate": 6.195376573397218e-06,
      "loss": 0.4018,
      "step": 15650
    },
    {
      "epoch": 5.0165,
      "grad_norm": 8.974024772644043,
      "learning_rate": 6.181763534510502e-06,
      "loss": 0.4314,
      "step": 15655
    },
    {
      "epoch": 5.01675,
      "grad_norm": 7.235648155212402,
      "learning_rate": 6.168163357710963e-06,
      "loss": 0.6986,
      "step": 15660
    },
    {
      "epoch": 5.017,
      "grad_norm": 7.920470714569092,
      "learning_rate": 6.154576052294195e-06,
      "loss": 0.7121,
      "step": 15665
    },
    {
      "epoch": 5.01725,
      "grad_norm": 7.796031951904297,
      "learning_rate": 6.141001627547002e-06,
      "loss": 0.7658,
      "step": 15670
    },
    {
      "epoch": 5.0175,
      "grad_norm": 7.93540620803833,
      "learning_rate": 6.127440092747366e-06,
      "loss": 0.7263,
      "step": 15675
    },
    {
      "epoch": 5.01775,
      "grad_norm": 9.802356719970703,
      "learning_rate": 6.113891457164464e-06,
      "loss": 0.6976,
      "step": 15680
    },
    {
      "epoch": 5.018,
      "grad_norm": 8.726174354553223,
      "learning_rate": 6.100355730058666e-06,
      "loss": 0.7311,
      "step": 15685
    },
    {
      "epoch": 5.01825,
      "grad_norm": 8.699933052062988,
      "learning_rate": 6.086832920681509e-06,
      "loss": 0.7546,
      "step": 15690
    },
    {
      "epoch": 5.0185,
      "grad_norm": 6.864535808563232,
      "learning_rate": 6.073323038275713e-06,
      "loss": 0.7445,
      "step": 15695
    },
    {
      "epoch": 5.01875,
      "grad_norm": 7.179891109466553,
      "learning_rate": 6.05982609207516e-06,
      "loss": 0.6846,
      "step": 15700
    },
    {
      "epoch": 5.019,
      "grad_norm": 7.793757915496826,
      "learning_rate": 6.046342091304871e-06,
      "loss": 0.7072,
      "step": 15705
    },
    {
      "epoch": 5.01925,
      "grad_norm": 6.240440845489502,
      "learning_rate": 6.032871045181041e-06,
      "loss": 0.6974,
      "step": 15710
    },
    {
      "epoch": 5.0195,
      "grad_norm": 6.522034645080566,
      "learning_rate": 6.019412962911006e-06,
      "loss": 0.7273,
      "step": 15715
    },
    {
      "epoch": 5.01975,
      "grad_norm": 5.6500139236450195,
      "learning_rate": 6.005967853693229e-06,
      "loss": 0.6607,
      "step": 15720
    },
    {
      "epoch": 5.02,
      "grad_norm": 7.292403697967529,
      "learning_rate": 5.99253572671733e-06,
      "loss": 0.7622,
      "step": 15725
    },
    {
      "epoch": 5.02025,
      "grad_norm": 8.547069549560547,
      "learning_rate": 5.979116591164022e-06,
      "loss": 0.8078,
      "step": 15730
    },
    {
      "epoch": 5.0205,
      "grad_norm": 6.082088470458984,
      "learning_rate": 5.96571045620517e-06,
      "loss": 0.7681,
      "step": 15735
    },
    {
      "epoch": 5.02075,
      "grad_norm": 7.277447700500488,
      "learning_rate": 5.952317331003737e-06,
      "loss": 0.7278,
      "step": 15740
    },
    {
      "epoch": 5.021,
      "grad_norm": 8.876973152160645,
      "learning_rate": 5.9389372247138e-06,
      "loss": 0.7519,
      "step": 15745
    },
    {
      "epoch": 5.02125,
      "grad_norm": 5.182973384857178,
      "learning_rate": 5.925570146480544e-06,
      "loss": 0.7007,
      "step": 15750
    },
    {
      "epoch": 5.0215,
      "grad_norm": 6.989216327667236,
      "learning_rate": 5.912216105440221e-06,
      "loss": 0.7408,
      "step": 15755
    },
    {
      "epoch": 5.02175,
      "grad_norm": 8.540633201599121,
      "learning_rate": 5.898875110720212e-06,
      "loss": 0.663,
      "step": 15760
    },
    {
      "epoch": 5.022,
      "grad_norm": 6.073109149932861,
      "learning_rate": 5.885547171438946e-06,
      "loss": 0.6586,
      "step": 15765
    },
    {
      "epoch": 5.02225,
      "grad_norm": 8.420819282531738,
      "learning_rate": 5.872232296705948e-06,
      "loss": 0.7215,
      "step": 15770
    },
    {
      "epoch": 5.0225,
      "grad_norm": 5.3622355461120605,
      "learning_rate": 5.85893049562182e-06,
      "loss": 0.7884,
      "step": 15775
    },
    {
      "epoch": 5.02275,
      "grad_norm": 6.6617112159729,
      "learning_rate": 5.845641777278199e-06,
      "loss": 0.6861,
      "step": 15780
    },
    {
      "epoch": 5.023,
      "grad_norm": 7.297975540161133,
      "learning_rate": 5.8323661507578095e-06,
      "loss": 0.7033,
      "step": 15785
    },
    {
      "epoch": 5.02325,
      "grad_norm": 6.826648712158203,
      "learning_rate": 5.819103625134417e-06,
      "loss": 0.6552,
      "step": 15790
    },
    {
      "epoch": 5.0235,
      "grad_norm": 6.069108486175537,
      "learning_rate": 5.80585420947283e-06,
      "loss": 0.7432,
      "step": 15795
    },
    {
      "epoch": 5.02375,
      "grad_norm": 6.261683940887451,
      "learning_rate": 5.79261791282891e-06,
      "loss": 0.647,
      "step": 15800
    },
    {
      "epoch": 5.024,
      "grad_norm": 5.092941761016846,
      "learning_rate": 5.779394744249522e-06,
      "loss": 0.7515,
      "step": 15805
    },
    {
      "epoch": 5.02425,
      "grad_norm": 5.628532886505127,
      "learning_rate": 5.766184712772596e-06,
      "loss": 0.637,
      "step": 15810
    },
    {
      "epoch": 5.0245,
      "grad_norm": 7.843773365020752,
      "learning_rate": 5.7529878274270476e-06,
      "loss": 0.8804,
      "step": 15815
    },
    {
      "epoch": 5.02475,
      "grad_norm": 6.8080339431762695,
      "learning_rate": 5.739804097232831e-06,
      "loss": 0.6849,
      "step": 15820
    },
    {
      "epoch": 5.025,
      "grad_norm": 7.4867048263549805,
      "learning_rate": 5.726633531200906e-06,
      "loss": 0.729,
      "step": 15825
    },
    {
      "epoch": 5.02525,
      "grad_norm": 4.803474426269531,
      "learning_rate": 5.713476138333221e-06,
      "loss": 0.6941,
      "step": 15830
    },
    {
      "epoch": 5.0255,
      "grad_norm": 7.883881092071533,
      "learning_rate": 5.700331927622735e-06,
      "loss": 0.7525,
      "step": 15835
    },
    {
      "epoch": 5.02575,
      "grad_norm": 8.556862831115723,
      "learning_rate": 5.687200908053389e-06,
      "loss": 0.7422,
      "step": 15840
    },
    {
      "epoch": 5.026,
      "grad_norm": 6.213218688964844,
      "learning_rate": 5.674083088600113e-06,
      "loss": 0.6584,
      "step": 15845
    },
    {
      "epoch": 5.02625,
      "grad_norm": 5.299666881561279,
      "learning_rate": 5.660978478228823e-06,
      "loss": 0.7893,
      "step": 15850
    },
    {
      "epoch": 5.0265,
      "grad_norm": 6.258795261383057,
      "learning_rate": 5.647887085896386e-06,
      "loss": 0.6379,
      "step": 15855
    },
    {
      "epoch": 5.02675,
      "grad_norm": 6.1598310470581055,
      "learning_rate": 5.634808920550638e-06,
      "loss": 0.8537,
      "step": 15860
    },
    {
      "epoch": 5.027,
      "grad_norm": 6.198187351226807,
      "learning_rate": 5.621743991130393e-06,
      "loss": 0.7362,
      "step": 15865
    },
    {
      "epoch": 5.02725,
      "grad_norm": 6.284202575683594,
      "learning_rate": 5.608692306565402e-06,
      "loss": 0.6877,
      "step": 15870
    },
    {
      "epoch": 5.0275,
      "grad_norm": 5.855053901672363,
      "learning_rate": 5.5956538757763685e-06,
      "loss": 0.6653,
      "step": 15875
    },
    {
      "epoch": 5.02775,
      "grad_norm": 6.22044038772583,
      "learning_rate": 5.58262870767495e-06,
      "loss": 0.6805,
      "step": 15880
    },
    {
      "epoch": 5.028,
      "grad_norm": 7.40553617477417,
      "learning_rate": 5.569616811163705e-06,
      "loss": 0.7423,
      "step": 15885
    },
    {
      "epoch": 5.02825,
      "grad_norm": 7.903422832489014,
      "learning_rate": 5.5566181951361516e-06,
      "loss": 0.7779,
      "step": 15890
    },
    {
      "epoch": 5.0285,
      "grad_norm": 5.709680080413818,
      "learning_rate": 5.543632868476723e-06,
      "loss": 0.6446,
      "step": 15895
    },
    {
      "epoch": 5.02875,
      "grad_norm": 7.2659220695495605,
      "learning_rate": 5.530660840060775e-06,
      "loss": 0.7169,
      "step": 15900
    },
    {
      "epoch": 5.029,
      "grad_norm": 7.002531051635742,
      "learning_rate": 5.5177021187545556e-06,
      "loss": 0.7045,
      "step": 15905
    },
    {
      "epoch": 5.02925,
      "grad_norm": 5.632417678833008,
      "learning_rate": 5.504756713415224e-06,
      "loss": 0.6794,
      "step": 15910
    },
    {
      "epoch": 5.0295,
      "grad_norm": 4.286761283874512,
      "learning_rate": 5.491824632890849e-06,
      "loss": 0.6298,
      "step": 15915
    },
    {
      "epoch": 5.02975,
      "grad_norm": 6.714603900909424,
      "learning_rate": 5.478905886020389e-06,
      "loss": 0.6875,
      "step": 15920
    },
    {
      "epoch": 5.03,
      "grad_norm": 4.958011150360107,
      "learning_rate": 5.466000481633682e-06,
      "loss": 0.6603,
      "step": 15925
    },
    {
      "epoch": 5.03025,
      "grad_norm": 6.465264320373535,
      "learning_rate": 5.453108428551457e-06,
      "loss": 0.715,
      "step": 15930
    },
    {
      "epoch": 5.0305,
      "grad_norm": 5.32750129699707,
      "learning_rate": 5.440229735585298e-06,
      "loss": 0.7023,
      "step": 15935
    },
    {
      "epoch": 5.03075,
      "grad_norm": 5.861049652099609,
      "learning_rate": 5.4273644115376775e-06,
      "loss": 0.7214,
      "step": 15940
    },
    {
      "epoch": 5.031,
      "grad_norm": 4.648535251617432,
      "learning_rate": 5.4145124652019295e-06,
      "loss": 0.6359,
      "step": 15945
    },
    {
      "epoch": 5.03125,
      "grad_norm": 5.556199073791504,
      "learning_rate": 5.401673905362226e-06,
      "loss": 0.7388,
      "step": 15950
    },
    {
      "epoch": 5.0315,
      "grad_norm": 7.442005634307861,
      "learning_rate": 5.388848740793612e-06,
      "loss": 0.7052,
      "step": 15955
    },
    {
      "epoch": 5.03175,
      "grad_norm": 5.459322452545166,
      "learning_rate": 5.376036980261956e-06,
      "loss": 0.7001,
      "step": 15960
    },
    {
      "epoch": 5.032,
      "grad_norm": 6.453864574432373,
      "learning_rate": 5.363238632523979e-06,
      "loss": 0.6275,
      "step": 15965
    },
    {
      "epoch": 5.03225,
      "grad_norm": 6.855364799499512,
      "learning_rate": 5.350453706327235e-06,
      "loss": 0.6499,
      "step": 15970
    },
    {
      "epoch": 5.0325,
      "grad_norm": 6.378211498260498,
      "learning_rate": 5.3376822104101e-06,
      "loss": 0.6714,
      "step": 15975
    },
    {
      "epoch": 5.03275,
      "grad_norm": 6.281109809875488,
      "learning_rate": 5.324924153501773e-06,
      "loss": 0.6021,
      "step": 15980
    },
    {
      "epoch": 5.033,
      "grad_norm": 6.142333030700684,
      "learning_rate": 5.312179544322257e-06,
      "loss": 0.6965,
      "step": 15985
    },
    {
      "epoch": 5.03325,
      "grad_norm": 4.689981937408447,
      "learning_rate": 5.299448391582379e-06,
      "loss": 0.6788,
      "step": 15990
    },
    {
      "epoch": 5.0335,
      "grad_norm": 4.956271648406982,
      "learning_rate": 5.286730703983764e-06,
      "loss": 0.6764,
      "step": 15995
    },
    {
      "epoch": 5.03375,
      "grad_norm": 7.0115861892700195,
      "learning_rate": 5.2740264902188225e-06,
      "loss": 0.6741,
      "step": 16000
    },
    {
      "epoch": 5.03375,
      "eval_loss": 2.105889320373535,
      "eval_runtime": 5.3589,
      "eval_samples_per_second": 191.084,
      "eval_steps_per_second": 23.885,
      "step": 16000
    },
    {
      "epoch": 5.034,
      "grad_norm": 6.1572346687316895,
      "learning_rate": 5.261335758970779e-06,
      "loss": 0.7728,
      "step": 16005
    },
    {
      "epoch": 5.03425,
      "grad_norm": 5.520827770233154,
      "learning_rate": 5.24865851891361e-06,
      "loss": 0.6608,
      "step": 16010
    },
    {
      "epoch": 5.0345,
      "grad_norm": 7.2338762283325195,
      "learning_rate": 5.2359947787121e-06,
      "loss": 0.6928,
      "step": 16015
    },
    {
      "epoch": 5.03475,
      "grad_norm": 4.7216796875,
      "learning_rate": 5.223344547021794e-06,
      "loss": 0.7292,
      "step": 16020
    },
    {
      "epoch": 5.035,
      "grad_norm": 5.78682279586792,
      "learning_rate": 5.2107078324890095e-06,
      "loss": 0.8025,
      "step": 16025
    },
    {
      "epoch": 5.03525,
      "grad_norm": 5.64008092880249,
      "learning_rate": 5.198084643750825e-06,
      "loss": 0.7343,
      "step": 16030
    },
    {
      "epoch": 5.0355,
      "grad_norm": 6.45496940612793,
      "learning_rate": 5.18547498943506e-06,
      "loss": 0.7101,
      "step": 16035
    },
    {
      "epoch": 5.03575,
      "grad_norm": 5.5314621925354,
      "learning_rate": 5.172878878160306e-06,
      "loss": 0.725,
      "step": 16040
    },
    {
      "epoch": 5.036,
      "grad_norm": 7.763432502746582,
      "learning_rate": 5.160296318535876e-06,
      "loss": 0.8272,
      "step": 16045
    },
    {
      "epoch": 5.03625,
      "grad_norm": 5.261684894561768,
      "learning_rate": 5.147727319161833e-06,
      "loss": 0.6663,
      "step": 16050
    },
    {
      "epoch": 5.0365,
      "grad_norm": 5.698896884918213,
      "learning_rate": 5.135171888628975e-06,
      "loss": 0.7382,
      "step": 16055
    },
    {
      "epoch": 5.03675,
      "grad_norm": 5.151010990142822,
      "learning_rate": 5.122630035518816e-06,
      "loss": 0.6876,
      "step": 16060
    },
    {
      "epoch": 5.037,
      "grad_norm": 5.7149977684021,
      "learning_rate": 5.110101768403591e-06,
      "loss": 0.737,
      "step": 16065
    },
    {
      "epoch": 5.03725,
      "grad_norm": 5.989070892333984,
      "learning_rate": 5.097587095846259e-06,
      "loss": 0.7192,
      "step": 16070
    },
    {
      "epoch": 5.0375,
      "grad_norm": 8.007974624633789,
      "learning_rate": 5.0850860264004795e-06,
      "loss": 0.8166,
      "step": 16075
    },
    {
      "epoch": 5.03775,
      "grad_norm": 6.5250067710876465,
      "learning_rate": 5.07259856861062e-06,
      "loss": 0.7073,
      "step": 16080
    },
    {
      "epoch": 5.038,
      "grad_norm": 5.528914928436279,
      "learning_rate": 5.060124731011734e-06,
      "loss": 0.7238,
      "step": 16085
    },
    {
      "epoch": 5.03825,
      "grad_norm": 6.196845531463623,
      "learning_rate": 5.047664522129566e-06,
      "loss": 0.7192,
      "step": 16090
    },
    {
      "epoch": 5.0385,
      "grad_norm": 6.747508525848389,
      "learning_rate": 5.035217950480559e-06,
      "loss": 0.7101,
      "step": 16095
    },
    {
      "epoch": 5.03875,
      "grad_norm": 5.639694690704346,
      "learning_rate": 5.022785024571825e-06,
      "loss": 0.7879,
      "step": 16100
    },
    {
      "epoch": 5.039,
      "grad_norm": 4.507657527923584,
      "learning_rate": 5.010365752901158e-06,
      "loss": 0.7303,
      "step": 16105
    },
    {
      "epoch": 5.03925,
      "grad_norm": 7.366714954376221,
      "learning_rate": 4.9979601439570034e-06,
      "loss": 0.6944,
      "step": 16110
    },
    {
      "epoch": 5.0395,
      "grad_norm": 5.805784225463867,
      "learning_rate": 4.985568206218483e-06,
      "loss": 0.6798,
      "step": 16115
    },
    {
      "epoch": 5.03975,
      "grad_norm": 5.190306663513184,
      "learning_rate": 4.973189948155368e-06,
      "loss": 0.628,
      "step": 16120
    },
    {
      "epoch": 5.04,
      "grad_norm": 5.570779323577881,
      "learning_rate": 4.960825378228082e-06,
      "loss": 0.6493,
      "step": 16125
    },
    {
      "epoch": 5.04025,
      "grad_norm": 4.00474739074707,
      "learning_rate": 4.948474504887698e-06,
      "loss": 0.7194,
      "step": 16130
    },
    {
      "epoch": 5.0405,
      "grad_norm": 5.652838230133057,
      "learning_rate": 4.936137336575916e-06,
      "loss": 0.6565,
      "step": 16135
    },
    {
      "epoch": 5.04075,
      "grad_norm": 5.0983476638793945,
      "learning_rate": 4.923813881725067e-06,
      "loss": 0.6248,
      "step": 16140
    },
    {
      "epoch": 5.041,
      "grad_norm": 7.2374067306518555,
      "learning_rate": 4.911504148758123e-06,
      "loss": 0.7445,
      "step": 16145
    },
    {
      "epoch": 5.04125,
      "grad_norm": 5.388174057006836,
      "learning_rate": 4.899208146088668e-06,
      "loss": 0.6461,
      "step": 16150
    },
    {
      "epoch": 5.0415,
      "grad_norm": 6.222433567047119,
      "learning_rate": 4.886925882120916e-06,
      "loss": 0.6793,
      "step": 16155
    },
    {
      "epoch": 5.04175,
      "grad_norm": 6.110579490661621,
      "learning_rate": 4.87465736524966e-06,
      "loss": 0.7162,
      "step": 16160
    },
    {
      "epoch": 5.042,
      "grad_norm": 6.43356990814209,
      "learning_rate": 4.862402603860325e-06,
      "loss": 0.6976,
      "step": 16165
    },
    {
      "epoch": 5.04225,
      "grad_norm": 6.331966876983643,
      "learning_rate": 4.850161606328924e-06,
      "loss": 0.6741,
      "step": 16170
    },
    {
      "epoch": 5.0425,
      "grad_norm": 6.961396217346191,
      "learning_rate": 4.837934381022061e-06,
      "loss": 0.7155,
      "step": 16175
    },
    {
      "epoch": 5.04275,
      "grad_norm": 5.980475902557373,
      "learning_rate": 4.82572093629694e-06,
      "loss": 0.7808,
      "step": 16180
    },
    {
      "epoch": 5.043,
      "grad_norm": 7.024538040161133,
      "learning_rate": 4.813521280501329e-06,
      "loss": 0.7207,
      "step": 16185
    },
    {
      "epoch": 5.04325,
      "grad_norm": 5.316833019256592,
      "learning_rate": 4.801335421973566e-06,
      "loss": 0.7484,
      "step": 16190
    },
    {
      "epoch": 5.0435,
      "grad_norm": 7.561807632446289,
      "learning_rate": 4.789163369042582e-06,
      "loss": 0.8201,
      "step": 16195
    },
    {
      "epoch": 5.04375,
      "grad_norm": 4.771280765533447,
      "learning_rate": 4.777005130027856e-06,
      "loss": 0.6325,
      "step": 16200
    },
    {
      "epoch": 5.044,
      "grad_norm": 5.727668762207031,
      "learning_rate": 4.764860713239441e-06,
      "loss": 0.6733,
      "step": 16205
    },
    {
      "epoch": 5.04425,
      "grad_norm": 4.101443767547607,
      "learning_rate": 4.752730126977917e-06,
      "loss": 0.6667,
      "step": 16210
    },
    {
      "epoch": 5.0445,
      "grad_norm": 5.0951642990112305,
      "learning_rate": 4.740613379534428e-06,
      "loss": 0.6319,
      "step": 16215
    },
    {
      "epoch": 5.04475,
      "grad_norm": 6.015196323394775,
      "learning_rate": 4.728510479190662e-06,
      "loss": 0.6868,
      "step": 16220
    },
    {
      "epoch": 5.045,
      "grad_norm": 6.380338668823242,
      "learning_rate": 4.71642143421884e-06,
      "loss": 0.6664,
      "step": 16225
    },
    {
      "epoch": 5.04525,
      "grad_norm": 5.151717185974121,
      "learning_rate": 4.704346252881697e-06,
      "loss": 0.7147,
      "step": 16230
    },
    {
      "epoch": 5.0455,
      "grad_norm": 6.381590843200684,
      "learning_rate": 4.692284943432523e-06,
      "loss": 0.6697,
      "step": 16235
    },
    {
      "epoch": 5.04575,
      "grad_norm": 4.526590824127197,
      "learning_rate": 4.680237514115091e-06,
      "loss": 0.694,
      "step": 16240
    },
    {
      "epoch": 5.046,
      "grad_norm": 5.827450275421143,
      "learning_rate": 4.668203973163715e-06,
      "loss": 0.7109,
      "step": 16245
    },
    {
      "epoch": 5.04625,
      "grad_norm": 6.970215320587158,
      "learning_rate": 4.656184328803206e-06,
      "loss": 0.7891,
      "step": 16250
    },
    {
      "epoch": 5.0465,
      "grad_norm": 5.83404541015625,
      "learning_rate": 4.644178589248885e-06,
      "loss": 0.6609,
      "step": 16255
    },
    {
      "epoch": 5.04675,
      "grad_norm": 4.997272491455078,
      "learning_rate": 4.632186762706548e-06,
      "loss": 0.6642,
      "step": 16260
    },
    {
      "epoch": 5.047,
      "grad_norm": 5.706333160400391,
      "learning_rate": 4.620208857372502e-06,
      "loss": 0.6777,
      "step": 16265
    },
    {
      "epoch": 5.04725,
      "grad_norm": 5.659661769866943,
      "learning_rate": 4.608244881433529e-06,
      "loss": 0.688,
      "step": 16270
    },
    {
      "epoch": 5.0475,
      "grad_norm": 5.244316101074219,
      "learning_rate": 4.596294843066906e-06,
      "loss": 0.6297,
      "step": 16275
    },
    {
      "epoch": 5.04775,
      "grad_norm": 6.187366485595703,
      "learning_rate": 4.584358750440354e-06,
      "loss": 0.7178,
      "step": 16280
    },
    {
      "epoch": 5.048,
      "grad_norm": 5.906211853027344,
      "learning_rate": 4.572436611712097e-06,
      "loss": 0.7477,
      "step": 16285
    },
    {
      "epoch": 5.04825,
      "grad_norm": 5.741772651672363,
      "learning_rate": 4.560528435030787e-06,
      "loss": 0.7298,
      "step": 16290
    },
    {
      "epoch": 5.0485,
      "grad_norm": 5.015162944793701,
      "learning_rate": 4.548634228535559e-06,
      "loss": 0.6957,
      "step": 16295
    },
    {
      "epoch": 5.04875,
      "grad_norm": 5.575600624084473,
      "learning_rate": 4.536754000355991e-06,
      "loss": 0.7548,
      "step": 16300
    },
    {
      "epoch": 5.049,
      "grad_norm": 7.233753204345703,
      "learning_rate": 4.524887758612115e-06,
      "loss": 0.689,
      "step": 16305
    },
    {
      "epoch": 5.04925,
      "grad_norm": 5.232849597930908,
      "learning_rate": 4.513035511414379e-06,
      "loss": 0.7388,
      "step": 16310
    },
    {
      "epoch": 5.0495,
      "grad_norm": 9.251976013183594,
      "learning_rate": 4.501197266863691e-06,
      "loss": 0.684,
      "step": 16315
    },
    {
      "epoch": 5.04975,
      "grad_norm": 4.29508638381958,
      "learning_rate": 4.489373033051386e-06,
      "loss": 0.6793,
      "step": 16320
    },
    {
      "epoch": 5.05,
      "grad_norm": 5.918606281280518,
      "learning_rate": 4.477562818059203e-06,
      "loss": 0.6921,
      "step": 16325
    },
    {
      "epoch": 5.05025,
      "grad_norm": 5.663646697998047,
      "learning_rate": 4.465766629959317e-06,
      "loss": 0.7306,
      "step": 16330
    },
    {
      "epoch": 5.0505,
      "grad_norm": 6.301334857940674,
      "learning_rate": 4.4539844768143214e-06,
      "loss": 0.7087,
      "step": 16335
    },
    {
      "epoch": 5.05075,
      "grad_norm": 7.752318382263184,
      "learning_rate": 4.442216366677191e-06,
      "loss": 0.7519,
      "step": 16340
    },
    {
      "epoch": 5.051,
      "grad_norm": 4.206521034240723,
      "learning_rate": 4.430462307591326e-06,
      "loss": 0.6807,
      "step": 16345
    },
    {
      "epoch": 5.05125,
      "grad_norm": 5.971557140350342,
      "learning_rate": 4.418722307590514e-06,
      "loss": 0.6912,
      "step": 16350
    },
    {
      "epoch": 5.0515,
      "grad_norm": 4.531147480010986,
      "learning_rate": 4.406996374698944e-06,
      "loss": 0.5884,
      "step": 16355
    },
    {
      "epoch": 5.05175,
      "grad_norm": 6.254164695739746,
      "learning_rate": 4.395284516931161e-06,
      "loss": 0.7092,
      "step": 16360
    },
    {
      "epoch": 5.052,
      "grad_norm": 4.691154956817627,
      "learning_rate": 4.383586742292128e-06,
      "loss": 0.6204,
      "step": 16365
    },
    {
      "epoch": 5.05225,
      "grad_norm": 5.96050500869751,
      "learning_rate": 4.37190305877715e-06,
      "loss": 0.6222,
      "step": 16370
    },
    {
      "epoch": 5.0525,
      "grad_norm": 7.035305976867676,
      "learning_rate": 4.360233474371919e-06,
      "loss": 0.6693,
      "step": 16375
    },
    {
      "epoch": 5.05275,
      "grad_norm": 5.327893257141113,
      "learning_rate": 4.348577997052489e-06,
      "loss": 0.628,
      "step": 16380
    },
    {
      "epoch": 5.053,
      "grad_norm": 5.617794036865234,
      "learning_rate": 4.336936634785271e-06,
      "loss": 0.6525,
      "step": 16385
    },
    {
      "epoch": 5.05325,
      "grad_norm": 6.2503485679626465,
      "learning_rate": 4.3253093955270165e-06,
      "loss": 0.6598,
      "step": 16390
    },
    {
      "epoch": 5.0535,
      "grad_norm": 5.95748233795166,
      "learning_rate": 4.313696287224839e-06,
      "loss": 0.6033,
      "step": 16395
    },
    {
      "epoch": 5.05375,
      "grad_norm": 5.196137428283691,
      "learning_rate": 4.302097317816187e-06,
      "loss": 0.6106,
      "step": 16400
    },
    {
      "epoch": 5.054,
      "grad_norm": 7.613922595977783,
      "learning_rate": 4.290512495228857e-06,
      "loss": 0.6505,
      "step": 16405
    },
    {
      "epoch": 5.05425,
      "grad_norm": 7.248477935791016,
      "learning_rate": 4.278941827380953e-06,
      "loss": 0.6419,
      "step": 16410
    },
    {
      "epoch": 5.0545,
      "grad_norm": 7.01298713684082,
      "learning_rate": 4.267385322180928e-06,
      "loss": 0.7463,
      "step": 16415
    },
    {
      "epoch": 5.05475,
      "grad_norm": 5.065356731414795,
      "learning_rate": 4.255842987527531e-06,
      "loss": 0.6845,
      "step": 16420
    },
    {
      "epoch": 5.055,
      "grad_norm": 6.912601470947266,
      "learning_rate": 4.244314831309845e-06,
      "loss": 0.6534,
      "step": 16425
    },
    {
      "epoch": 5.05525,
      "grad_norm": 5.240400314331055,
      "learning_rate": 4.23280086140726e-06,
      "loss": 0.6509,
      "step": 16430
    },
    {
      "epoch": 5.0555,
      "grad_norm": 4.859752655029297,
      "learning_rate": 4.221301085689466e-06,
      "loss": 0.627,
      "step": 16435
    },
    {
      "epoch": 5.05575,
      "grad_norm": 6.7885236740112305,
      "learning_rate": 4.209815512016446e-06,
      "loss": 0.6927,
      "step": 16440
    },
    {
      "epoch": 5.056,
      "grad_norm": 5.885239124298096,
      "learning_rate": 4.198344148238481e-06,
      "loss": 0.7326,
      "step": 16445
    },
    {
      "epoch": 5.05625,
      "grad_norm": 6.556428909301758,
      "learning_rate": 4.186887002196146e-06,
      "loss": 0.679,
      "step": 16450
    },
    {
      "epoch": 5.0565,
      "grad_norm": 4.19002103805542,
      "learning_rate": 4.175444081720292e-06,
      "loss": 0.5986,
      "step": 16455
    },
    {
      "epoch": 5.05675,
      "grad_norm": 6.828560829162598,
      "learning_rate": 4.164015394632045e-06,
      "loss": 0.6986,
      "step": 16460
    },
    {
      "epoch": 5.057,
      "grad_norm": 4.864161491394043,
      "learning_rate": 4.152600948742802e-06,
      "loss": 0.6082,
      "step": 16465
    },
    {
      "epoch": 5.05725,
      "grad_norm": 3.523578405380249,
      "learning_rate": 4.141200751854227e-06,
      "loss": 0.6624,
      "step": 16470
    },
    {
      "epoch": 5.0575,
      "grad_norm": 8.061908721923828,
      "learning_rate": 4.129814811758251e-06,
      "loss": 0.7124,
      "step": 16475
    },
    {
      "epoch": 5.05775,
      "grad_norm": 5.894069671630859,
      "learning_rate": 4.11844313623706e-06,
      "loss": 0.5765,
      "step": 16480
    },
    {
      "epoch": 5.058,
      "grad_norm": 4.695410251617432,
      "learning_rate": 4.107085733063088e-06,
      "loss": 0.58,
      "step": 16485
    },
    {
      "epoch": 5.05825,
      "grad_norm": 5.121982574462891,
      "learning_rate": 4.095742609999004e-06,
      "loss": 0.6047,
      "step": 16490
    },
    {
      "epoch": 5.0585,
      "grad_norm": 4.601768493652344,
      "learning_rate": 4.084413774797729e-06,
      "loss": 0.6605,
      "step": 16495
    },
    {
      "epoch": 5.05875,
      "grad_norm": 4.845969200134277,
      "learning_rate": 4.073099235202421e-06,
      "loss": 0.6329,
      "step": 16500
    },
    {
      "epoch": 5.05875,
      "eval_loss": 2.096879005432129,
      "eval_runtime": 5.3519,
      "eval_samples_per_second": 191.334,
      "eval_steps_per_second": 23.917,
      "step": 16500
    },
    {
      "epoch": 5.059,
      "grad_norm": 8.533443450927734,
      "learning_rate": 4.061798998946459e-06,
      "loss": 0.7666,
      "step": 16505
    },
    {
      "epoch": 5.05925,
      "grad_norm": 3.81260347366333,
      "learning_rate": 4.050513073753448e-06,
      "loss": 0.6596,
      "step": 16510
    },
    {
      "epoch": 5.0595,
      "grad_norm": 5.639314651489258,
      "learning_rate": 4.039241467337201e-06,
      "loss": 0.7047,
      "step": 16515
    },
    {
      "epoch": 5.05975,
      "grad_norm": 5.096421718597412,
      "learning_rate": 4.027984187401765e-06,
      "loss": 0.7482,
      "step": 16520
    },
    {
      "epoch": 5.06,
      "grad_norm": 5.907792091369629,
      "learning_rate": 4.016741241641381e-06,
      "loss": 0.8087,
      "step": 16525
    },
    {
      "epoch": 5.06025,
      "grad_norm": 6.176156044006348,
      "learning_rate": 4.005512637740499e-06,
      "loss": 0.6552,
      "step": 16530
    },
    {
      "epoch": 5.0605,
      "grad_norm": 7.670425891876221,
      "learning_rate": 3.994298383373768e-06,
      "loss": 0.7311,
      "step": 16535
    },
    {
      "epoch": 5.06075,
      "grad_norm": 5.719435214996338,
      "learning_rate": 3.9830984862060135e-06,
      "loss": 0.6427,
      "step": 16540
    },
    {
      "epoch": 5.061,
      "grad_norm": 6.03698205947876,
      "learning_rate": 3.971912953892268e-06,
      "loss": 0.6678,
      "step": 16545
    },
    {
      "epoch": 5.06125,
      "grad_norm": 7.198057651519775,
      "learning_rate": 3.960741794077735e-06,
      "loss": 0.6075,
      "step": 16550
    },
    {
      "epoch": 5.0615,
      "grad_norm": 5.79524564743042,
      "learning_rate": 3.9495850143978026e-06,
      "loss": 0.6959,
      "step": 16555
    },
    {
      "epoch": 5.06175,
      "grad_norm": 4.969898223876953,
      "learning_rate": 3.93844262247802e-06,
      "loss": 0.6526,
      "step": 16560
    },
    {
      "epoch": 5.062,
      "grad_norm": 4.8750104904174805,
      "learning_rate": 3.9273146259341e-06,
      "loss": 0.6586,
      "step": 16565
    },
    {
      "epoch": 5.06225,
      "grad_norm": 6.91746187210083,
      "learning_rate": 3.91620103237193e-06,
      "loss": 0.6788,
      "step": 16570
    },
    {
      "epoch": 5.0625,
      "grad_norm": 4.1765851974487305,
      "learning_rate": 3.905101849387547e-06,
      "loss": 0.617,
      "step": 16575
    },
    {
      "epoch": 5.06275,
      "grad_norm": 6.022116184234619,
      "learning_rate": 3.894017084567136e-06,
      "loss": 0.6161,
      "step": 16580
    },
    {
      "epoch": 5.063,
      "grad_norm": 6.702216148376465,
      "learning_rate": 3.882946745487035e-06,
      "loss": 0.6839,
      "step": 16585
    },
    {
      "epoch": 5.06325,
      "grad_norm": 7.054340362548828,
      "learning_rate": 3.871890839713707e-06,
      "loss": 0.8399,
      "step": 16590
    },
    {
      "epoch": 5.0635,
      "grad_norm": 4.743434429168701,
      "learning_rate": 3.860849374803762e-06,
      "loss": 0.5911,
      "step": 16595
    },
    {
      "epoch": 5.06375,
      "grad_norm": 4.934958457946777,
      "learning_rate": 3.849822358303948e-06,
      "loss": 0.7075,
      "step": 16600
    },
    {
      "epoch": 5.064,
      "grad_norm": 6.778921127319336,
      "learning_rate": 3.838809797751111e-06,
      "loss": 0.6695,
      "step": 16605
    },
    {
      "epoch": 5.06425,
      "grad_norm": 6.114503860473633,
      "learning_rate": 3.827811700672243e-06,
      "loss": 0.6763,
      "step": 16610
    },
    {
      "epoch": 5.0645,
      "grad_norm": 6.177475452423096,
      "learning_rate": 3.816828074584433e-06,
      "loss": 0.6335,
      "step": 16615
    },
    {
      "epoch": 5.06475,
      "grad_norm": 6.37515115737915,
      "learning_rate": 3.8058589269948873e-06,
      "loss": 0.6883,
      "step": 16620
    },
    {
      "epoch": 5.065,
      "grad_norm": 6.709290981292725,
      "learning_rate": 3.7949042654009142e-06,
      "loss": 0.7556,
      "step": 16625
    },
    {
      "epoch": 5.06525,
      "grad_norm": 7.1048102378845215,
      "learning_rate": 3.783964097289927e-06,
      "loss": 0.7543,
      "step": 16630
    },
    {
      "epoch": 5.0655,
      "grad_norm": 5.016740322113037,
      "learning_rate": 3.773038430139428e-06,
      "loss": 0.6471,
      "step": 16635
    },
    {
      "epoch": 5.06575,
      "grad_norm": 5.6215715408325195,
      "learning_rate": 3.7621272714169984e-06,
      "loss": 0.6207,
      "step": 16640
    },
    {
      "epoch": 5.066,
      "grad_norm": 5.426814079284668,
      "learning_rate": 3.7512306285803212e-06,
      "loss": 0.7082,
      "step": 16645
    },
    {
      "epoch": 5.06625,
      "grad_norm": 9.968376159667969,
      "learning_rate": 3.740348509077143e-06,
      "loss": 0.8248,
      "step": 16650
    },
    {
      "epoch": 5.0665,
      "grad_norm": 6.073897838592529,
      "learning_rate": 3.7294809203452907e-06,
      "loss": 0.6135,
      "step": 16655
    },
    {
      "epoch": 5.06675,
      "grad_norm": 5.928537368774414,
      "learning_rate": 3.718627869812666e-06,
      "loss": 0.6128,
      "step": 16660
    },
    {
      "epoch": 5.067,
      "grad_norm": 4.020851135253906,
      "learning_rate": 3.7077893648972157e-06,
      "loss": 0.6233,
      "step": 16665
    },
    {
      "epoch": 5.06725,
      "grad_norm": 4.040884494781494,
      "learning_rate": 3.6969654130069615e-06,
      "loss": 0.5701,
      "step": 16670
    },
    {
      "epoch": 5.0675,
      "grad_norm": 4.8736677169799805,
      "learning_rate": 3.6861560215399716e-06,
      "loss": 0.5684,
      "step": 16675
    },
    {
      "epoch": 5.06775,
      "grad_norm": 4.792431354522705,
      "learning_rate": 3.675361197884361e-06,
      "loss": 0.5589,
      "step": 16680
    },
    {
      "epoch": 5.068,
      "grad_norm": 4.890685081481934,
      "learning_rate": 3.6645809494183003e-06,
      "loss": 0.7989,
      "step": 16685
    },
    {
      "epoch": 5.06825,
      "grad_norm": 6.725776672363281,
      "learning_rate": 3.6538152835099813e-06,
      "loss": 0.683,
      "step": 16690
    },
    {
      "epoch": 5.0685,
      "grad_norm": 6.831414699554443,
      "learning_rate": 3.643064207517624e-06,
      "loss": 0.6947,
      "step": 16695
    },
    {
      "epoch": 5.06875,
      "grad_norm": 4.907339096069336,
      "learning_rate": 3.6323277287895e-06,
      "loss": 0.6422,
      "step": 16700
    },
    {
      "epoch": 5.069,
      "grad_norm": 5.253940582275391,
      "learning_rate": 3.621605854663887e-06,
      "loss": 0.6278,
      "step": 16705
    },
    {
      "epoch": 5.06925,
      "grad_norm": 5.920804500579834,
      "learning_rate": 3.610898592469092e-06,
      "loss": 0.6177,
      "step": 16710
    },
    {
      "epoch": 5.0695,
      "grad_norm": 5.245398998260498,
      "learning_rate": 3.6002059495234165e-06,
      "loss": 0.647,
      "step": 16715
    },
    {
      "epoch": 5.06975,
      "grad_norm": 5.081485748291016,
      "learning_rate": 3.5895279331351866e-06,
      "loss": 0.6409,
      "step": 16720
    },
    {
      "epoch": 5.07,
      "grad_norm": 6.10357141494751,
      "learning_rate": 3.578864550602726e-06,
      "loss": 0.6786,
      "step": 16725
    },
    {
      "epoch": 5.07025,
      "grad_norm": 4.521333694458008,
      "learning_rate": 3.5682158092143536e-06,
      "loss": 0.6093,
      "step": 16730
    },
    {
      "epoch": 5.0705,
      "grad_norm": 3.955556869506836,
      "learning_rate": 3.5575817162483926e-06,
      "loss": 0.6629,
      "step": 16735
    },
    {
      "epoch": 5.07075,
      "grad_norm": 4.786252975463867,
      "learning_rate": 3.54696227897314e-06,
      "loss": 0.5733,
      "step": 16740
    },
    {
      "epoch": 5.071,
      "grad_norm": 4.767240047454834,
      "learning_rate": 3.536357504646873e-06,
      "loss": 0.5823,
      "step": 16745
    },
    {
      "epoch": 5.07125,
      "grad_norm": 3.749314308166504,
      "learning_rate": 3.5257674005178625e-06,
      "loss": 0.5498,
      "step": 16750
    },
    {
      "epoch": 5.0715,
      "grad_norm": 4.466799736022949,
      "learning_rate": 3.5151919738243416e-06,
      "loss": 0.5541,
      "step": 16755
    },
    {
      "epoch": 5.07175,
      "grad_norm": 4.812297344207764,
      "learning_rate": 3.504631231794525e-06,
      "loss": 0.7333,
      "step": 16760
    },
    {
      "epoch": 5.072,
      "grad_norm": 4.352900505065918,
      "learning_rate": 3.494085181646567e-06,
      "loss": 0.62,
      "step": 16765
    },
    {
      "epoch": 5.07225,
      "grad_norm": 4.967430114746094,
      "learning_rate": 3.4835538305885956e-06,
      "loss": 0.5885,
      "step": 16770
    },
    {
      "epoch": 5.0725,
      "grad_norm": 4.750173091888428,
      "learning_rate": 3.473037185818695e-06,
      "loss": 0.5059,
      "step": 16775
    },
    {
      "epoch": 5.07275,
      "grad_norm": 4.366738796234131,
      "learning_rate": 3.462535254524893e-06,
      "loss": 0.6308,
      "step": 16780
    },
    {
      "epoch": 5.073,
      "grad_norm": 4.579869270324707,
      "learning_rate": 3.4520480438851618e-06,
      "loss": 0.5644,
      "step": 16785
    },
    {
      "epoch": 5.07325,
      "grad_norm": 4.542019367218018,
      "learning_rate": 3.441575561067406e-06,
      "loss": 0.5227,
      "step": 16790
    },
    {
      "epoch": 5.0735,
      "grad_norm": 4.497802734375,
      "learning_rate": 3.4311178132294674e-06,
      "loss": 0.5516,
      "step": 16795
    },
    {
      "epoch": 5.07375,
      "grad_norm": 3.740156412124634,
      "learning_rate": 3.4206748075191197e-06,
      "loss": 0.6204,
      "step": 16800
    },
    {
      "epoch": 5.074,
      "grad_norm": 4.460127353668213,
      "learning_rate": 3.410246551074059e-06,
      "loss": 0.5673,
      "step": 16805
    },
    {
      "epoch": 5.07425,
      "grad_norm": 5.30394983291626,
      "learning_rate": 3.3998330510219083e-06,
      "loss": 0.5638,
      "step": 16810
    },
    {
      "epoch": 5.0745,
      "grad_norm": 5.419213771820068,
      "learning_rate": 3.389434314480186e-06,
      "loss": 0.6524,
      "step": 16815
    },
    {
      "epoch": 5.07475,
      "grad_norm": 4.274205684661865,
      "learning_rate": 3.379050348556331e-06,
      "loss": 0.4961,
      "step": 16820
    },
    {
      "epoch": 5.075,
      "grad_norm": 4.03331184387207,
      "learning_rate": 3.368681160347692e-06,
      "loss": 0.4994,
      "step": 16825
    },
    {
      "epoch": 5.07525,
      "grad_norm": 5.140792369842529,
      "learning_rate": 3.358326756941513e-06,
      "loss": 0.5536,
      "step": 16830
    },
    {
      "epoch": 5.0755,
      "grad_norm": 4.753535747528076,
      "learning_rate": 3.3479871454149214e-06,
      "loss": 0.5928,
      "step": 16835
    },
    {
      "epoch": 5.07575,
      "grad_norm": 4.189321994781494,
      "learning_rate": 3.3376623328349564e-06,
      "loss": 0.5314,
      "step": 16840
    },
    {
      "epoch": 5.076,
      "grad_norm": 4.7937331199646,
      "learning_rate": 3.327352326258515e-06,
      "loss": 0.647,
      "step": 16845
    },
    {
      "epoch": 5.07625,
      "grad_norm": 4.894420623779297,
      "learning_rate": 3.3170571327324013e-06,
      "loss": 0.6226,
      "step": 16850
    },
    {
      "epoch": 5.0765,
      "grad_norm": 5.603827476501465,
      "learning_rate": 3.3067767592932787e-06,
      "loss": 0.5881,
      "step": 16855
    },
    {
      "epoch": 5.07675,
      "grad_norm": 4.092952728271484,
      "learning_rate": 3.29651121296769e-06,
      "loss": 0.5943,
      "step": 16860
    },
    {
      "epoch": 5.077,
      "grad_norm": 4.234659671783447,
      "learning_rate": 3.2862605007720287e-06,
      "loss": 0.5325,
      "step": 16865
    },
    {
      "epoch": 5.07725,
      "grad_norm": 3.751288890838623,
      "learning_rate": 3.2760246297125706e-06,
      "loss": 0.5586,
      "step": 16870
    },
    {
      "epoch": 5.0775,
      "grad_norm": 5.811582565307617,
      "learning_rate": 3.2658036067854307e-06,
      "loss": 0.6301,
      "step": 16875
    },
    {
      "epoch": 5.07775,
      "grad_norm": 3.9917891025543213,
      "learning_rate": 3.2555974389765905e-06,
      "loss": 0.5839,
      "step": 16880
    },
    {
      "epoch": 5.078,
      "grad_norm": 5.115178108215332,
      "learning_rate": 3.245406133261858e-06,
      "loss": 0.6235,
      "step": 16885
    },
    {
      "epoch": 5.07825,
      "grad_norm": 4.512652397155762,
      "learning_rate": 3.2352296966069064e-06,
      "loss": 0.5691,
      "step": 16890
    },
    {
      "epoch": 5.0785,
      "grad_norm": 5.060573101043701,
      "learning_rate": 3.2250681359672214e-06,
      "loss": 0.505,
      "step": 16895
    },
    {
      "epoch": 5.07875,
      "grad_norm": 4.623410701751709,
      "learning_rate": 3.2149214582881395e-06,
      "loss": 0.5524,
      "step": 16900
    },
    {
      "epoch": 5.079,
      "grad_norm": 5.643314838409424,
      "learning_rate": 3.204789670504821e-06,
      "loss": 0.5412,
      "step": 16905
    },
    {
      "epoch": 5.07925,
      "grad_norm": 4.424880504608154,
      "learning_rate": 3.1946727795422516e-06,
      "loss": 0.6229,
      "step": 16910
    },
    {
      "epoch": 5.0795,
      "grad_norm": 5.319294452667236,
      "learning_rate": 3.184570792315217e-06,
      "loss": 0.5267,
      "step": 16915
    },
    {
      "epoch": 5.07975,
      "grad_norm": 6.047942161560059,
      "learning_rate": 3.174483715728341e-06,
      "loss": 0.577,
      "step": 16920
    },
    {
      "epoch": 5.08,
      "grad_norm": 3.875962257385254,
      "learning_rate": 3.16441155667605e-06,
      "loss": 0.5934,
      "step": 16925
    },
    {
      "epoch": 5.08025,
      "grad_norm": 5.198245048522949,
      "learning_rate": 3.154354322042555e-06,
      "loss": 0.5996,
      "step": 16930
    },
    {
      "epoch": 5.0805,
      "grad_norm": 6.506491661071777,
      "learning_rate": 3.1443120187018895e-06,
      "loss": 0.659,
      "step": 16935
    },
    {
      "epoch": 5.08075,
      "grad_norm": 5.198446750640869,
      "learning_rate": 3.134284653517877e-06,
      "loss": 0.6081,
      "step": 16940
    },
    {
      "epoch": 5.081,
      "grad_norm": 4.941203594207764,
      "learning_rate": 3.1242722333441193e-06,
      "loss": 0.5949,
      "step": 16945
    },
    {
      "epoch": 5.08125,
      "grad_norm": 4.027247905731201,
      "learning_rate": 3.114274765024011e-06,
      "loss": 0.5846,
      "step": 16950
    },
    {
      "epoch": 5.0815,
      "grad_norm": 3.1620116233825684,
      "learning_rate": 3.104292255390734e-06,
      "loss": 0.5714,
      "step": 16955
    },
    {
      "epoch": 5.08175,
      "grad_norm": 4.226747989654541,
      "learning_rate": 3.0943247112672407e-06,
      "loss": 0.5765,
      "step": 16960
    },
    {
      "epoch": 5.082,
      "grad_norm": 4.695590496063232,
      "learning_rate": 3.0843721394662407e-06,
      "loss": 0.613,
      "step": 16965
    },
    {
      "epoch": 5.08225,
      "grad_norm": 4.0531005859375,
      "learning_rate": 3.0744345467902393e-06,
      "loss": 0.5476,
      "step": 16970
    },
    {
      "epoch": 5.0825,
      "grad_norm": 4.104189395904541,
      "learning_rate": 3.0645119400314755e-06,
      "loss": 0.589,
      "step": 16975
    },
    {
      "epoch": 5.08275,
      "grad_norm": 4.478397369384766,
      "learning_rate": 3.0546043259719604e-06,
      "loss": 0.573,
      "step": 16980
    },
    {
      "epoch": 5.083,
      "grad_norm": 4.0030131340026855,
      "learning_rate": 3.044711711383455e-06,
      "loss": 0.5232,
      "step": 16985
    },
    {
      "epoch": 5.08325,
      "grad_norm": 4.394650459289551,
      "learning_rate": 3.034834103027473e-06,
      "loss": 0.5332,
      "step": 16990
    },
    {
      "epoch": 5.0835,
      "grad_norm": 5.0500359535217285,
      "learning_rate": 3.024971507655261e-06,
      "loss": 0.6354,
      "step": 16995
    },
    {
      "epoch": 5.08375,
      "grad_norm": 4.1405348777771,
      "learning_rate": 3.0151239320078072e-06,
      "loss": 0.5003,
      "step": 17000
    },
    {
      "epoch": 5.08375,
      "eval_loss": 2.1873788833618164,
      "eval_runtime": 5.4139,
      "eval_samples_per_second": 189.144,
      "eval_steps_per_second": 23.643,
      "step": 17000
    },
    {
      "epoch": 5.084,
      "grad_norm": 4.826240062713623,
      "learning_rate": 3.005291382815842e-06,
      "loss": 0.4755,
      "step": 17005
    },
    {
      "epoch": 5.08425,
      "grad_norm": 3.66451358795166,
      "learning_rate": 2.995473866799822e-06,
      "loss": 0.4501,
      "step": 17010
    },
    {
      "epoch": 5.0845,
      "grad_norm": 5.279922962188721,
      "learning_rate": 2.9856713906699254e-06,
      "loss": 0.5577,
      "step": 17015
    },
    {
      "epoch": 5.08475,
      "grad_norm": 3.3102431297302246,
      "learning_rate": 2.9758839611260524e-06,
      "loss": 0.4642,
      "step": 17020
    },
    {
      "epoch": 5.085,
      "grad_norm": 4.225133895874023,
      "learning_rate": 2.9661115848578114e-06,
      "loss": 0.4874,
      "step": 17025
    },
    {
      "epoch": 5.08525,
      "grad_norm": 4.919534206390381,
      "learning_rate": 2.956354268544534e-06,
      "loss": 0.4672,
      "step": 17030
    },
    {
      "epoch": 5.0855,
      "grad_norm": 3.418120861053467,
      "learning_rate": 2.9466120188552547e-06,
      "loss": 0.4578,
      "step": 17035
    },
    {
      "epoch": 5.08575,
      "grad_norm": 3.7639994621276855,
      "learning_rate": 2.936884842448717e-06,
      "loss": 0.5288,
      "step": 17040
    },
    {
      "epoch": 5.086,
      "grad_norm": 4.489089012145996,
      "learning_rate": 2.927172745973339e-06,
      "loss": 0.5182,
      "step": 17045
    },
    {
      "epoch": 5.08625,
      "grad_norm": 4.416871070861816,
      "learning_rate": 2.917475736067257e-06,
      "loss": 0.4714,
      "step": 17050
    },
    {
      "epoch": 5.0865,
      "grad_norm": 3.390901565551758,
      "learning_rate": 2.9077938193582844e-06,
      "loss": 0.4688,
      "step": 17055
    },
    {
      "epoch": 5.08675,
      "grad_norm": 3.8699259757995605,
      "learning_rate": 2.898127002463916e-06,
      "loss": 0.5061,
      "step": 17060
    },
    {
      "epoch": 5.087,
      "grad_norm": 3.530217409133911,
      "learning_rate": 2.888475291991344e-06,
      "loss": 0.5196,
      "step": 17065
    },
    {
      "epoch": 5.08725,
      "grad_norm": 3.9675681591033936,
      "learning_rate": 2.878838694537403e-06,
      "loss": 0.4921,
      "step": 17070
    },
    {
      "epoch": 5.0875,
      "grad_norm": 4.428412437438965,
      "learning_rate": 2.8692172166886215e-06,
      "loss": 0.4847,
      "step": 17075
    },
    {
      "epoch": 5.08775,
      "grad_norm": 3.5475070476531982,
      "learning_rate": 2.8596108650211924e-06,
      "loss": 0.5136,
      "step": 17080
    },
    {
      "epoch": 5.088,
      "grad_norm": 4.954488277435303,
      "learning_rate": 2.850019646100968e-06,
      "loss": 0.5618,
      "step": 17085
    },
    {
      "epoch": 5.08825,
      "grad_norm": 3.826594114303589,
      "learning_rate": 2.8404435664834557e-06,
      "loss": 0.4912,
      "step": 17090
    },
    {
      "epoch": 5.0885,
      "grad_norm": 5.142799377441406,
      "learning_rate": 2.8308826327138103e-06,
      "loss": 0.4904,
      "step": 17095
    },
    {
      "epoch": 5.08875,
      "grad_norm": 4.057790756225586,
      "learning_rate": 2.8213368513268416e-06,
      "loss": 0.5436,
      "step": 17100
    },
    {
      "epoch": 5.089,
      "grad_norm": 4.689639091491699,
      "learning_rate": 2.8118062288470025e-06,
      "loss": 0.5461,
      "step": 17105
    },
    {
      "epoch": 5.08925,
      "grad_norm": 3.5600452423095703,
      "learning_rate": 2.8022907717883907e-06,
      "loss": 0.46,
      "step": 17110
    },
    {
      "epoch": 5.0895,
      "grad_norm": 4.619615077972412,
      "learning_rate": 2.79279048665472e-06,
      "loss": 0.4836,
      "step": 17115
    },
    {
      "epoch": 5.08975,
      "grad_norm": 3.358036756515503,
      "learning_rate": 2.783305379939352e-06,
      "loss": 0.4561,
      "step": 17120
    },
    {
      "epoch": 5.09,
      "grad_norm": 4.941054821014404,
      "learning_rate": 2.7738354581252653e-06,
      "loss": 0.5031,
      "step": 17125
    },
    {
      "epoch": 5.09025,
      "grad_norm": 4.052411079406738,
      "learning_rate": 2.7643807276850593e-06,
      "loss": 0.479,
      "step": 17130
    },
    {
      "epoch": 5.0905,
      "grad_norm": 3.6654629707336426,
      "learning_rate": 2.7549411950809566e-06,
      "loss": 0.4484,
      "step": 17135
    },
    {
      "epoch": 5.09075,
      "grad_norm": 5.046319484710693,
      "learning_rate": 2.745516866764797e-06,
      "loss": 0.4295,
      "step": 17140
    },
    {
      "epoch": 5.091,
      "grad_norm": 3.9825072288513184,
      "learning_rate": 2.736107749178002e-06,
      "loss": 0.4701,
      "step": 17145
    },
    {
      "epoch": 5.09125,
      "grad_norm": 4.1712822914123535,
      "learning_rate": 2.7267138487516273e-06,
      "loss": 0.4649,
      "step": 17150
    },
    {
      "epoch": 5.0915,
      "grad_norm": 5.487410068511963,
      "learning_rate": 2.717335171906313e-06,
      "loss": 0.5372,
      "step": 17155
    },
    {
      "epoch": 5.09175,
      "grad_norm": 3.503453493118286,
      "learning_rate": 2.7079717250522996e-06,
      "loss": 0.5758,
      "step": 17160
    },
    {
      "epoch": 5.092,
      "grad_norm": 4.495259761810303,
      "learning_rate": 2.6986235145894073e-06,
      "loss": 0.4659,
      "step": 17165
    },
    {
      "epoch": 5.09225,
      "grad_norm": 4.353601932525635,
      "learning_rate": 2.6892905469070554e-06,
      "loss": 0.5181,
      "step": 17170
    },
    {
      "epoch": 5.0925,
      "grad_norm": 4.934479713439941,
      "learning_rate": 2.6799728283842335e-06,
      "loss": 0.4945,
      "step": 17175
    },
    {
      "epoch": 5.09275,
      "grad_norm": 3.481801986694336,
      "learning_rate": 2.6706703653895188e-06,
      "loss": 0.5378,
      "step": 17180
    },
    {
      "epoch": 5.093,
      "grad_norm": 3.606746196746826,
      "learning_rate": 2.661383164281056e-06,
      "loss": 0.5041,
      "step": 17185
    },
    {
      "epoch": 5.09325,
      "grad_norm": 4.542306423187256,
      "learning_rate": 2.652111231406565e-06,
      "loss": 0.5218,
      "step": 17190
    },
    {
      "epoch": 5.0935,
      "grad_norm": 3.4923887252807617,
      "learning_rate": 2.642854573103315e-06,
      "loss": 0.5062,
      "step": 17195
    },
    {
      "epoch": 5.09375,
      "grad_norm": 5.973040580749512,
      "learning_rate": 2.633613195698148e-06,
      "loss": 0.6201,
      "step": 17200
    },
    {
      "epoch": 5.094,
      "grad_norm": 3.2425520420074463,
      "learning_rate": 2.624387105507464e-06,
      "loss": 0.4872,
      "step": 17205
    },
    {
      "epoch": 5.09425,
      "grad_norm": 3.8817853927612305,
      "learning_rate": 2.6151763088372e-06,
      "loss": 0.4583,
      "step": 17210
    },
    {
      "epoch": 5.0945,
      "grad_norm": 3.1364667415618896,
      "learning_rate": 2.6059808119828526e-06,
      "loss": 0.4376,
      "step": 17215
    },
    {
      "epoch": 5.09475,
      "grad_norm": 3.8639004230499268,
      "learning_rate": 2.5968006212294645e-06,
      "loss": 0.5063,
      "step": 17220
    },
    {
      "epoch": 5.095,
      "grad_norm": 3.7874529361724854,
      "learning_rate": 2.5876357428515937e-06,
      "loss": 0.5479,
      "step": 17225
    },
    {
      "epoch": 5.09525,
      "grad_norm": 3.3489770889282227,
      "learning_rate": 2.578486183113357e-06,
      "loss": 0.482,
      "step": 17230
    },
    {
      "epoch": 5.0955,
      "grad_norm": 4.738099575042725,
      "learning_rate": 2.569351948268392e-06,
      "loss": 0.5535,
      "step": 17235
    },
    {
      "epoch": 5.09575,
      "grad_norm": 6.113231182098389,
      "learning_rate": 2.5602330445598677e-06,
      "loss": 0.6371,
      "step": 17240
    },
    {
      "epoch": 5.096,
      "grad_norm": 5.774627685546875,
      "learning_rate": 2.55112947822046e-06,
      "loss": 0.6297,
      "step": 17245
    },
    {
      "epoch": 5.09625,
      "grad_norm": 5.484418869018555,
      "learning_rate": 2.5420412554723744e-06,
      "loss": 0.5769,
      "step": 17250
    },
    {
      "epoch": 5.0965,
      "grad_norm": 4.933892250061035,
      "learning_rate": 2.5329683825273233e-06,
      "loss": 0.5895,
      "step": 17255
    },
    {
      "epoch": 5.09675,
      "grad_norm": 5.408175468444824,
      "learning_rate": 2.523910865586529e-06,
      "loss": 0.6025,
      "step": 17260
    },
    {
      "epoch": 5.097,
      "grad_norm": 5.675920009613037,
      "learning_rate": 2.514868710840723e-06,
      "loss": 0.5726,
      "step": 17265
    },
    {
      "epoch": 5.09725,
      "grad_norm": 3.7493736743927,
      "learning_rate": 2.505841924470137e-06,
      "loss": 0.5537,
      "step": 17270
    },
    {
      "epoch": 5.0975,
      "grad_norm": 8.082110404968262,
      "learning_rate": 2.4968305126444795e-06,
      "loss": 0.6612,
      "step": 17275
    },
    {
      "epoch": 5.09775,
      "grad_norm": 4.969089031219482,
      "learning_rate": 2.487834481522977e-06,
      "loss": 0.5594,
      "step": 17280
    },
    {
      "epoch": 5.098,
      "grad_norm": 5.639377593994141,
      "learning_rate": 2.4788538372543304e-06,
      "loss": 0.638,
      "step": 17285
    },
    {
      "epoch": 5.09825,
      "grad_norm": 3.7394580841064453,
      "learning_rate": 2.4698885859767262e-06,
      "loss": 0.5105,
      "step": 17290
    },
    {
      "epoch": 5.0985,
      "grad_norm": 5.904512882232666,
      "learning_rate": 2.4609387338178245e-06,
      "loss": 0.6251,
      "step": 17295
    },
    {
      "epoch": 5.09875,
      "grad_norm": 4.409672260284424,
      "learning_rate": 2.452004286894774e-06,
      "loss": 0.5569,
      "step": 17300
    },
    {
      "epoch": 5.099,
      "grad_norm": 5.046052932739258,
      "learning_rate": 2.44308525131417e-06,
      "loss": 0.5403,
      "step": 17305
    },
    {
      "epoch": 5.09925,
      "grad_norm": 5.054805278778076,
      "learning_rate": 2.4341816331721003e-06,
      "loss": 0.59,
      "step": 17310
    },
    {
      "epoch": 5.0995,
      "grad_norm": 4.219480514526367,
      "learning_rate": 2.4252934385540993e-06,
      "loss": 0.5454,
      "step": 17315
    },
    {
      "epoch": 5.09975,
      "grad_norm": 4.372147560119629,
      "learning_rate": 2.4164206735351717e-06,
      "loss": 0.4604,
      "step": 17320
    },
    {
      "epoch": 5.1,
      "grad_norm": 4.000115871429443,
      "learning_rate": 2.407563344179761e-06,
      "loss": 0.5825,
      "step": 17325
    },
    {
      "epoch": 5.10025,
      "grad_norm": 5.743343353271484,
      "learning_rate": 2.3987214565417694e-06,
      "loss": 0.5738,
      "step": 17330
    },
    {
      "epoch": 5.1005,
      "grad_norm": 4.561987400054932,
      "learning_rate": 2.389895016664542e-06,
      "loss": 0.4651,
      "step": 17335
    },
    {
      "epoch": 5.10075,
      "grad_norm": 3.5732455253601074,
      "learning_rate": 2.3810840305808798e-06,
      "loss": 0.4668,
      "step": 17340
    },
    {
      "epoch": 5.101,
      "grad_norm": 5.537109375,
      "learning_rate": 2.3722885043129943e-06,
      "loss": 0.5136,
      "step": 17345
    },
    {
      "epoch": 5.10125,
      "grad_norm": 5.382415771484375,
      "learning_rate": 2.3635084438725474e-06,
      "loss": 0.6346,
      "step": 17350
    },
    {
      "epoch": 5.1015,
      "grad_norm": 3.7601065635681152,
      "learning_rate": 2.35474385526063e-06,
      "loss": 0.4754,
      "step": 17355
    },
    {
      "epoch": 5.10175,
      "grad_norm": 3.2676784992218018,
      "learning_rate": 2.3459947444677554e-06,
      "loss": 0.4615,
      "step": 17360
    },
    {
      "epoch": 5.102,
      "grad_norm": 4.457973957061768,
      "learning_rate": 2.3372611174738573e-06,
      "loss": 0.48,
      "step": 17365
    },
    {
      "epoch": 5.10225,
      "grad_norm": 4.191104412078857,
      "learning_rate": 2.328542980248294e-06,
      "loss": 0.5058,
      "step": 17370
    },
    {
      "epoch": 5.1025,
      "grad_norm": 3.626401662826538,
      "learning_rate": 2.319840338749821e-06,
      "loss": 0.4764,
      "step": 17375
    },
    {
      "epoch": 5.10275,
      "grad_norm": 3.7046761512756348,
      "learning_rate": 2.3111531989266132e-06,
      "loss": 0.4682,
      "step": 17380
    },
    {
      "epoch": 5.103,
      "grad_norm": 4.836760997772217,
      "learning_rate": 2.3024815667162526e-06,
      "loss": 0.5105,
      "step": 17385
    },
    {
      "epoch": 5.10325,
      "grad_norm": 3.747926950454712,
      "learning_rate": 2.2938254480457194e-06,
      "loss": 0.4639,
      "step": 17390
    },
    {
      "epoch": 5.1035,
      "grad_norm": 4.237752437591553,
      "learning_rate": 2.2851848488313872e-06,
      "loss": 0.5312,
      "step": 17395
    },
    {
      "epoch": 5.10375,
      "grad_norm": 3.5439722537994385,
      "learning_rate": 2.2765597749790175e-06,
      "loss": 0.4373,
      "step": 17400
    },
    {
      "epoch": 5.104,
      "grad_norm": 3.7062056064605713,
      "learning_rate": 2.2679502323837724e-06,
      "loss": 0.5431,
      "step": 17405
    },
    {
      "epoch": 5.10425,
      "grad_norm": 3.7717957496643066,
      "learning_rate": 2.2593562269301927e-06,
      "loss": 0.5396,
      "step": 17410
    },
    {
      "epoch": 5.1045,
      "grad_norm": 3.550063371658325,
      "learning_rate": 2.250777764492201e-06,
      "loss": 0.4709,
      "step": 17415
    },
    {
      "epoch": 5.10475,
      "grad_norm": 4.36636209487915,
      "learning_rate": 2.2422148509331027e-06,
      "loss": 0.4541,
      "step": 17420
    },
    {
      "epoch": 5.105,
      "grad_norm": 4.56001615524292,
      "learning_rate": 2.233667492105551e-06,
      "loss": 0.5037,
      "step": 17425
    },
    {
      "epoch": 5.10525,
      "grad_norm": 4.206531524658203,
      "learning_rate": 2.2251356938515987e-06,
      "loss": 0.4976,
      "step": 17430
    },
    {
      "epoch": 5.1055,
      "grad_norm": 4.530598163604736,
      "learning_rate": 2.216619462002642e-06,
      "loss": 0.4795,
      "step": 17435
    },
    {
      "epoch": 5.10575,
      "grad_norm": 4.104304790496826,
      "learning_rate": 2.2081188023794554e-06,
      "loss": 0.4805,
      "step": 17440
    },
    {
      "epoch": 5.106,
      "grad_norm": 4.685826301574707,
      "learning_rate": 2.199633720792152e-06,
      "loss": 0.4813,
      "step": 17445
    },
    {
      "epoch": 5.10625,
      "grad_norm": 3.834886074066162,
      "learning_rate": 2.1911642230402012e-06,
      "loss": 0.4588,
      "step": 17450
    },
    {
      "epoch": 5.1065,
      "grad_norm": 5.219570159912109,
      "learning_rate": 2.1827103149124313e-06,
      "loss": 0.4984,
      "step": 17455
    },
    {
      "epoch": 5.10675,
      "grad_norm": 3.342909574508667,
      "learning_rate": 2.1742720021870053e-06,
      "loss": 0.4845,
      "step": 17460
    },
    {
      "epoch": 5.107,
      "grad_norm": 5.393327713012695,
      "learning_rate": 2.1658492906314334e-06,
      "loss": 0.5436,
      "step": 17465
    },
    {
      "epoch": 5.10725,
      "grad_norm": 4.038838863372803,
      "learning_rate": 2.1574421860025617e-06,
      "loss": 0.4206,
      "step": 17470
    },
    {
      "epoch": 5.1075,
      "grad_norm": 3.5356695652008057,
      "learning_rate": 2.149050694046559e-06,
      "loss": 0.4837,
      "step": 17475
    },
    {
      "epoch": 5.10775,
      "grad_norm": 3.359549045562744,
      "learning_rate": 2.1406748204989336e-06,
      "loss": 0.4941,
      "step": 17480
    },
    {
      "epoch": 5.108,
      "grad_norm": 5.082559585571289,
      "learning_rate": 2.1323145710845232e-06,
      "loss": 0.4842,
      "step": 17485
    },
    {
      "epoch": 5.10825,
      "grad_norm": 3.350027084350586,
      "learning_rate": 2.123969951517471e-06,
      "loss": 0.4659,
      "step": 17490
    },
    {
      "epoch": 5.1085,
      "grad_norm": 4.025864601135254,
      "learning_rate": 2.1156409675012535e-06,
      "loss": 0.4574,
      "step": 17495
    },
    {
      "epoch": 5.10875,
      "grad_norm": 3.9124839305877686,
      "learning_rate": 2.107327624728642e-06,
      "loss": 0.4472,
      "step": 17500
    },
    {
      "epoch": 5.10875,
      "eval_loss": 2.2560553550720215,
      "eval_runtime": 5.2539,
      "eval_samples_per_second": 194.903,
      "eval_steps_per_second": 24.363,
      "step": 17500
    },
    {
      "epoch": 5.109,
      "grad_norm": 4.073120594024658,
      "learning_rate": 2.099029928881735e-06,
      "loss": 0.4902,
      "step": 17505
    },
    {
      "epoch": 5.10925,
      "grad_norm": 4.484469890594482,
      "learning_rate": 2.0907478856319274e-06,
      "loss": 0.4848,
      "step": 17510
    },
    {
      "epoch": 5.1095,
      "grad_norm": 4.647768020629883,
      "learning_rate": 2.0824815006399213e-06,
      "loss": 0.4365,
      "step": 17515
    },
    {
      "epoch": 5.10975,
      "grad_norm": 3.985701322555542,
      "learning_rate": 2.0742307795557137e-06,
      "loss": 0.4479,
      "step": 17520
    },
    {
      "epoch": 5.11,
      "grad_norm": 3.7990219593048096,
      "learning_rate": 2.0659957280185856e-06,
      "loss": 0.4622,
      "step": 17525
    },
    {
      "epoch": 5.11025,
      "grad_norm": 4.572803974151611,
      "learning_rate": 2.057776351657126e-06,
      "loss": 0.4465,
      "step": 17530
    },
    {
      "epoch": 5.1105,
      "grad_norm": 4.082496166229248,
      "learning_rate": 2.0495726560891936e-06,
      "loss": 0.4834,
      "step": 17535
    },
    {
      "epoch": 5.11075,
      "grad_norm": 5.150050163269043,
      "learning_rate": 2.041384646921943e-06,
      "loss": 0.6642,
      "step": 17540
    },
    {
      "epoch": 5.111,
      "grad_norm": 4.058382987976074,
      "learning_rate": 2.033212329751805e-06,
      "loss": 0.5554,
      "step": 17545
    },
    {
      "epoch": 5.11125,
      "grad_norm": 5.990663051605225,
      "learning_rate": 2.0250557101644696e-06,
      "loss": 0.6398,
      "step": 17550
    },
    {
      "epoch": 5.1115,
      "grad_norm": 4.630749702453613,
      "learning_rate": 2.016914793734914e-06,
      "loss": 0.6057,
      "step": 17555
    },
    {
      "epoch": 5.11175,
      "grad_norm": 4.727547645568848,
      "learning_rate": 2.008789586027382e-06,
      "loss": 0.6301,
      "step": 17560
    },
    {
      "epoch": 5.112,
      "grad_norm": 5.797506332397461,
      "learning_rate": 2.0006800925953722e-06,
      "loss": 0.6621,
      "step": 17565
    },
    {
      "epoch": 5.11225,
      "grad_norm": 4.527849197387695,
      "learning_rate": 1.992586318981654e-06,
      "loss": 0.5374,
      "step": 17570
    },
    {
      "epoch": 5.1125,
      "grad_norm": 4.997738361358643,
      "learning_rate": 1.984508270718238e-06,
      "loss": 0.7433,
      "step": 17575
    },
    {
      "epoch": 5.11275,
      "grad_norm": 4.109188556671143,
      "learning_rate": 1.976445953326392e-06,
      "loss": 0.5825,
      "step": 17580
    },
    {
      "epoch": 5.113,
      "grad_norm": 4.072070598602295,
      "learning_rate": 1.96839937231664e-06,
      "loss": 0.5934,
      "step": 17585
    },
    {
      "epoch": 5.11325,
      "grad_norm": 4.767783164978027,
      "learning_rate": 1.960368533188739e-06,
      "loss": 0.6134,
      "step": 17590
    },
    {
      "epoch": 5.1135,
      "grad_norm": 4.481319427490234,
      "learning_rate": 1.9523534414317006e-06,
      "loss": 0.5653,
      "step": 17595
    },
    {
      "epoch": 5.11375,
      "grad_norm": 4.434763431549072,
      "learning_rate": 1.9443541025237545e-06,
      "loss": 0.5147,
      "step": 17600
    },
    {
      "epoch": 5.114,
      "grad_norm": 4.503805160522461,
      "learning_rate": 1.9363705219323765e-06,
      "loss": 0.573,
      "step": 17605
    },
    {
      "epoch": 5.11425,
      "grad_norm": 5.265501499176025,
      "learning_rate": 1.928402705114271e-06,
      "loss": 0.5767,
      "step": 17610
    },
    {
      "epoch": 5.1145,
      "grad_norm": 4.758930206298828,
      "learning_rate": 1.920450657515366e-06,
      "loss": 0.6277,
      "step": 17615
    },
    {
      "epoch": 5.11475,
      "grad_norm": 4.357718467712402,
      "learning_rate": 1.91251438457081e-06,
      "loss": 0.5296,
      "step": 17620
    },
    {
      "epoch": 5.115,
      "grad_norm": 6.7628703117370605,
      "learning_rate": 1.9045938917049727e-06,
      "loss": 0.6626,
      "step": 17625
    },
    {
      "epoch": 5.11525,
      "grad_norm": 7.014016628265381,
      "learning_rate": 1.8966891843314293e-06,
      "loss": 0.5949,
      "step": 17630
    },
    {
      "epoch": 5.1155,
      "grad_norm": 3.992910385131836,
      "learning_rate": 1.8888002678529747e-06,
      "loss": 0.4075,
      "step": 17635
    },
    {
      "epoch": 5.11575,
      "grad_norm": 4.419414043426514,
      "learning_rate": 1.8809271476616153e-06,
      "loss": 0.4934,
      "step": 17640
    },
    {
      "epoch": 5.116,
      "grad_norm": 4.947132587432861,
      "learning_rate": 1.8730698291385518e-06,
      "loss": 0.5392,
      "step": 17645
    },
    {
      "epoch": 5.11625,
      "grad_norm": 5.281271457672119,
      "learning_rate": 1.8652283176541829e-06,
      "loss": 0.5298,
      "step": 17650
    },
    {
      "epoch": 5.1165,
      "grad_norm": 4.604473114013672,
      "learning_rate": 1.8574026185681105e-06,
      "loss": 0.4864,
      "step": 17655
    },
    {
      "epoch": 5.11675,
      "grad_norm": 4.661632537841797,
      "learning_rate": 1.8495927372291278e-06,
      "loss": 0.5077,
      "step": 17660
    },
    {
      "epoch": 5.117,
      "grad_norm": 3.4171440601348877,
      "learning_rate": 1.841798678975215e-06,
      "loss": 0.4856,
      "step": 17665
    },
    {
      "epoch": 5.11725,
      "grad_norm": 3.5975630283355713,
      "learning_rate": 1.8340204491335389e-06,
      "loss": 0.5096,
      "step": 17670
    },
    {
      "epoch": 5.1175,
      "grad_norm": 3.559537410736084,
      "learning_rate": 1.8262580530204437e-06,
      "loss": 0.4537,
      "step": 17675
    },
    {
      "epoch": 5.11775,
      "grad_norm": 4.598680019378662,
      "learning_rate": 1.8185114959414495e-06,
      "loss": 0.469,
      "step": 17680
    },
    {
      "epoch": 5.118,
      "grad_norm": 5.376321792602539,
      "learning_rate": 1.8107807831912598e-06,
      "loss": 0.4732,
      "step": 17685
    },
    {
      "epoch": 5.11825,
      "grad_norm": 4.397357940673828,
      "learning_rate": 1.8030659200537425e-06,
      "loss": 0.4654,
      "step": 17690
    },
    {
      "epoch": 5.1185,
      "grad_norm": 3.3828682899475098,
      "learning_rate": 1.7953669118019407e-06,
      "loss": 0.4253,
      "step": 17695
    },
    {
      "epoch": 5.11875,
      "grad_norm": 4.422677993774414,
      "learning_rate": 1.787683763698042e-06,
      "loss": 0.5066,
      "step": 17700
    },
    {
      "epoch": 5.119,
      "grad_norm": 3.96467924118042,
      "learning_rate": 1.7800164809934157e-06,
      "loss": 0.5788,
      "step": 17705
    },
    {
      "epoch": 5.11925,
      "grad_norm": 5.4266767501831055,
      "learning_rate": 1.7723650689285693e-06,
      "loss": 0.5263,
      "step": 17710
    },
    {
      "epoch": 5.1195,
      "grad_norm": 4.794499397277832,
      "learning_rate": 1.764729532733181e-06,
      "loss": 0.4669,
      "step": 17715
    },
    {
      "epoch": 5.11975,
      "grad_norm": 5.393982887268066,
      "learning_rate": 1.7571098776260565e-06,
      "loss": 0.558,
      "step": 17720
    },
    {
      "epoch": 5.12,
      "grad_norm": 4.360844612121582,
      "learning_rate": 1.749506108815166e-06,
      "loss": 0.5235,
      "step": 17725
    },
    {
      "epoch": 5.12025,
      "grad_norm": 5.269559860229492,
      "learning_rate": 1.7419182314976106e-06,
      "loss": 0.4653,
      "step": 17730
    },
    {
      "epoch": 5.1205,
      "grad_norm": 3.684373617172241,
      "learning_rate": 1.7343462508596308e-06,
      "loss": 0.4739,
      "step": 17735
    },
    {
      "epoch": 5.12075,
      "grad_norm": 4.806042194366455,
      "learning_rate": 1.7267901720766061e-06,
      "loss": 0.5711,
      "step": 17740
    },
    {
      "epoch": 5.121,
      "grad_norm": 5.556678771972656,
      "learning_rate": 1.7192500003130525e-06,
      "loss": 0.5935,
      "step": 17745
    },
    {
      "epoch": 5.12125,
      "grad_norm": 3.3491287231445312,
      "learning_rate": 1.7117257407225918e-06,
      "loss": 0.4623,
      "step": 17750
    },
    {
      "epoch": 5.1215,
      "grad_norm": 4.066516399383545,
      "learning_rate": 1.7042173984479937e-06,
      "loss": 0.5626,
      "step": 17755
    },
    {
      "epoch": 5.12175,
      "grad_norm": 6.092516899108887,
      "learning_rate": 1.6967249786211364e-06,
      "loss": 0.4749,
      "step": 17760
    },
    {
      "epoch": 5.122,
      "grad_norm": 4.215873718261719,
      "learning_rate": 1.6892484863630264e-06,
      "loss": 0.5271,
      "step": 17765
    },
    {
      "epoch": 5.12225,
      "grad_norm": 4.450173854827881,
      "learning_rate": 1.681787926783762e-06,
      "loss": 0.4787,
      "step": 17770
    },
    {
      "epoch": 5.1225,
      "grad_norm": 4.647473335266113,
      "learning_rate": 1.6743433049825785e-06,
      "loss": 0.4921,
      "step": 17775
    },
    {
      "epoch": 5.12275,
      "grad_norm": 4.299100399017334,
      "learning_rate": 1.6669146260477913e-06,
      "loss": 0.5127,
      "step": 17780
    },
    {
      "epoch": 5.123,
      "grad_norm": 3.8933584690093994,
      "learning_rate": 1.659501895056842e-06,
      "loss": 0.4675,
      "step": 17785
    },
    {
      "epoch": 5.12325,
      "grad_norm": 4.447449684143066,
      "learning_rate": 1.652105117076258e-06,
      "loss": 0.5017,
      "step": 17790
    },
    {
      "epoch": 5.1235,
      "grad_norm": 3.8677589893341064,
      "learning_rate": 1.6447242971616732e-06,
      "loss": 0.5243,
      "step": 17795
    },
    {
      "epoch": 5.12375,
      "grad_norm": 4.014198303222656,
      "learning_rate": 1.6373594403577997e-06,
      "loss": 0.4983,
      "step": 17800
    },
    {
      "epoch": 5.124,
      "grad_norm": 4.141797065734863,
      "learning_rate": 1.6300105516984548e-06,
      "loss": 0.4778,
      "step": 17805
    },
    {
      "epoch": 5.12425,
      "grad_norm": 4.195465564727783,
      "learning_rate": 1.6226776362065322e-06,
      "loss": 0.4654,
      "step": 17810
    },
    {
      "epoch": 5.1245,
      "grad_norm": 4.113515377044678,
      "learning_rate": 1.6153606988940085e-06,
      "loss": 0.5274,
      "step": 17815
    },
    {
      "epoch": 5.12475,
      "grad_norm": 4.091386795043945,
      "learning_rate": 1.6080597447619444e-06,
      "loss": 0.4407,
      "step": 17820
    },
    {
      "epoch": 5.125,
      "grad_norm": 4.618134021759033,
      "learning_rate": 1.6007747788004734e-06,
      "loss": 0.534,
      "step": 17825
    },
    {
      "epoch": 5.12525,
      "grad_norm": 3.185849905014038,
      "learning_rate": 1.5935058059887958e-06,
      "loss": 0.4901,
      "step": 17830
    },
    {
      "epoch": 5.1255,
      "grad_norm": 3.5527594089508057,
      "learning_rate": 1.586252831295193e-06,
      "loss": 0.5091,
      "step": 17835
    },
    {
      "epoch": 5.12575,
      "grad_norm": 4.8049516677856445,
      "learning_rate": 1.5790158596769994e-06,
      "loss": 0.4349,
      "step": 17840
    },
    {
      "epoch": 5.126,
      "grad_norm": 4.943800449371338,
      "learning_rate": 1.5717948960806223e-06,
      "loss": 0.4622,
      "step": 17845
    },
    {
      "epoch": 5.12625,
      "grad_norm": 3.491065740585327,
      "learning_rate": 1.5645899454415137e-06,
      "loss": 0.4518,
      "step": 17850
    },
    {
      "epoch": 5.1265,
      "grad_norm": 6.19301176071167,
      "learning_rate": 1.557401012684201e-06,
      "loss": 0.5101,
      "step": 17855
    },
    {
      "epoch": 5.12675,
      "grad_norm": 4.4613261222839355,
      "learning_rate": 1.5502281027222426e-06,
      "loss": 0.4806,
      "step": 17860
    },
    {
      "epoch": 5.127,
      "grad_norm": 3.5299148559570312,
      "learning_rate": 1.543071220458256e-06,
      "loss": 0.4789,
      "step": 17865
    },
    {
      "epoch": 5.12725,
      "grad_norm": 4.369135856628418,
      "learning_rate": 1.535930370783903e-06,
      "loss": 0.4391,
      "step": 17870
    },
    {
      "epoch": 5.1275,
      "grad_norm": 3.4919066429138184,
      "learning_rate": 1.528805558579896e-06,
      "loss": 0.4274,
      "step": 17875
    },
    {
      "epoch": 5.12775,
      "grad_norm": 4.433239459991455,
      "learning_rate": 1.521696788715965e-06,
      "loss": 0.488,
      "step": 17880
    },
    {
      "epoch": 5.128,
      "grad_norm": 3.525540590286255,
      "learning_rate": 1.5146040660508897e-06,
      "loss": 0.4549,
      "step": 17885
    },
    {
      "epoch": 5.12825,
      "grad_norm": 4.721678733825684,
      "learning_rate": 1.5075273954324814e-06,
      "loss": 0.4604,
      "step": 17890
    },
    {
      "epoch": 5.1285,
      "grad_norm": 4.43776273727417,
      "learning_rate": 1.5004667816975847e-06,
      "loss": 0.4983,
      "step": 17895
    },
    {
      "epoch": 5.12875,
      "grad_norm": 4.0095391273498535,
      "learning_rate": 1.4934222296720478e-06,
      "loss": 0.5821,
      "step": 17900
    },
    {
      "epoch": 5.129,
      "grad_norm": 3.2680156230926514,
      "learning_rate": 1.4863937441707693e-06,
      "loss": 0.402,
      "step": 17905
    },
    {
      "epoch": 5.12925,
      "grad_norm": 2.9762320518493652,
      "learning_rate": 1.4793813299976423e-06,
      "loss": 0.4567,
      "step": 17910
    },
    {
      "epoch": 5.1295,
      "grad_norm": 3.3339717388153076,
      "learning_rate": 1.4723849919455884e-06,
      "loss": 0.4631,
      "step": 17915
    },
    {
      "epoch": 5.12975,
      "grad_norm": 3.855529308319092,
      "learning_rate": 1.465404734796541e-06,
      "loss": 0.4104,
      "step": 17920
    },
    {
      "epoch": 5.13,
      "grad_norm": 4.664697647094727,
      "learning_rate": 1.458440563321445e-06,
      "loss": 0.4758,
      "step": 17925
    },
    {
      "epoch": 5.13025,
      "grad_norm": 4.18117618560791,
      "learning_rate": 1.4514924822802367e-06,
      "loss": 0.4545,
      "step": 17930
    },
    {
      "epoch": 5.1305,
      "grad_norm": 3.51603627204895,
      "learning_rate": 1.4445604964218651e-06,
      "loss": 0.4905,
      "step": 17935
    },
    {
      "epoch": 5.13075,
      "grad_norm": 4.13687801361084,
      "learning_rate": 1.4376446104842816e-06,
      "loss": 0.4875,
      "step": 17940
    },
    {
      "epoch": 5.131,
      "grad_norm": 5.5844340324401855,
      "learning_rate": 1.4307448291944303e-06,
      "loss": 0.4686,
      "step": 17945
    },
    {
      "epoch": 5.13125,
      "grad_norm": 4.3604044914245605,
      "learning_rate": 1.4238611572682414e-06,
      "loss": 0.4562,
      "step": 17950
    },
    {
      "epoch": 5.1315,
      "grad_norm": 3.838331460952759,
      "learning_rate": 1.4169935994106404e-06,
      "loss": 0.4357,
      "step": 17955
    },
    {
      "epoch": 5.13175,
      "grad_norm": 3.612205743789673,
      "learning_rate": 1.410142160315539e-06,
      "loss": 0.5008,
      "step": 17960
    },
    {
      "epoch": 5.132,
      "grad_norm": 3.679586887359619,
      "learning_rate": 1.4033068446658331e-06,
      "loss": 0.4472,
      "step": 17965
    },
    {
      "epoch": 5.13225,
      "grad_norm": 5.661870002746582,
      "learning_rate": 1.396487657133394e-06,
      "loss": 0.4818,
      "step": 17970
    },
    {
      "epoch": 5.1325,
      "grad_norm": 3.6628589630126953,
      "learning_rate": 1.3896846023790793e-06,
      "loss": 0.4788,
      "step": 17975
    },
    {
      "epoch": 5.13275,
      "grad_norm": 3.361786365509033,
      "learning_rate": 1.3828976850527004e-06,
      "loss": 0.4886,
      "step": 17980
    },
    {
      "epoch": 5.133,
      "grad_norm": 3.0980122089385986,
      "learning_rate": 1.3761269097930602e-06,
      "loss": 0.4088,
      "step": 17985
    },
    {
      "epoch": 5.13325,
      "grad_norm": 6.310459613800049,
      "learning_rate": 1.3693722812279153e-06,
      "loss": 0.539,
      "step": 17990
    },
    {
      "epoch": 5.1335,
      "grad_norm": 3.71394419670105,
      "learning_rate": 1.3626338039739977e-06,
      "loss": 0.4334,
      "step": 17995
    },
    {
      "epoch": 5.13375,
      "grad_norm": 2.8959341049194336,
      "learning_rate": 1.3559114826369874e-06,
      "loss": 0.3791,
      "step": 18000
    },
    {
      "epoch": 5.13375,
      "eval_loss": 2.2831902503967285,
      "eval_runtime": 5.4013,
      "eval_samples_per_second": 189.586,
      "eval_steps_per_second": 23.698,
      "step": 18000
    },
    {
      "epoch": 5.134,
      "grad_norm": 4.290957927703857,
      "learning_rate": 1.3492053218115252e-06,
      "loss": 0.4587,
      "step": 18005
    },
    {
      "epoch": 5.13425,
      "grad_norm": 3.4991745948791504,
      "learning_rate": 1.3425153260812118e-06,
      "loss": 0.432,
      "step": 18010
    },
    {
      "epoch": 5.1345,
      "grad_norm": 3.1971945762634277,
      "learning_rate": 1.3358415000185947e-06,
      "loss": 0.4854,
      "step": 18015
    },
    {
      "epoch": 5.13475,
      "grad_norm": 4.825328350067139,
      "learning_rate": 1.3291838481851754e-06,
      "loss": 0.468,
      "step": 18020
    },
    {
      "epoch": 5.135,
      "grad_norm": 4.623330116271973,
      "learning_rate": 1.3225423751313942e-06,
      "loss": 0.4832,
      "step": 18025
    },
    {
      "epoch": 5.13525,
      "grad_norm": 5.928822040557861,
      "learning_rate": 1.315917085396634e-06,
      "loss": 0.4785,
      "step": 18030
    },
    {
      "epoch": 5.1355,
      "grad_norm": 3.6739587783813477,
      "learning_rate": 1.3093079835092141e-06,
      "loss": 0.473,
      "step": 18035
    },
    {
      "epoch": 5.13575,
      "grad_norm": 4.388523101806641,
      "learning_rate": 1.3027150739864018e-06,
      "loss": 0.4834,
      "step": 18040
    },
    {
      "epoch": 5.136,
      "grad_norm": 5.772043704986572,
      "learning_rate": 1.296138361334387e-06,
      "loss": 0.4804,
      "step": 18045
    },
    {
      "epoch": 5.13625,
      "grad_norm": 3.1595592498779297,
      "learning_rate": 1.2895778500482875e-06,
      "loss": 0.4603,
      "step": 18050
    },
    {
      "epoch": 5.1365,
      "grad_norm": 3.6893529891967773,
      "learning_rate": 1.2830335446121473e-06,
      "loss": 0.5206,
      "step": 18055
    },
    {
      "epoch": 5.13675,
      "grad_norm": 3.5942487716674805,
      "learning_rate": 1.2765054494989436e-06,
      "loss": 0.4463,
      "step": 18060
    },
    {
      "epoch": 5.1370000000000005,
      "grad_norm": 5.998137950897217,
      "learning_rate": 1.2699935691705634e-06,
      "loss": 0.5396,
      "step": 18065
    },
    {
      "epoch": 5.13725,
      "grad_norm": 4.136460304260254,
      "learning_rate": 1.2634979080778208e-06,
      "loss": 0.5215,
      "step": 18070
    },
    {
      "epoch": 5.1375,
      "grad_norm": 4.181515693664551,
      "learning_rate": 1.2570184706604372e-06,
      "loss": 0.4131,
      "step": 18075
    },
    {
      "epoch": 5.13775,
      "grad_norm": 5.621184825897217,
      "learning_rate": 1.2505552613470444e-06,
      "loss": 0.4667,
      "step": 18080
    },
    {
      "epoch": 5.138,
      "grad_norm": 3.7026638984680176,
      "learning_rate": 1.2441082845551844e-06,
      "loss": 0.4896,
      "step": 18085
    },
    {
      "epoch": 5.13825,
      "grad_norm": 4.963851451873779,
      "learning_rate": 1.2376775446913124e-06,
      "loss": 0.4313,
      "step": 18090
    },
    {
      "epoch": 5.1385,
      "grad_norm": 4.481583118438721,
      "learning_rate": 1.2312630461507712e-06,
      "loss": 0.564,
      "step": 18095
    },
    {
      "epoch": 5.13875,
      "grad_norm": 3.9788107872009277,
      "learning_rate": 1.2248647933178148e-06,
      "loss": 0.5161,
      "step": 18100
    },
    {
      "epoch": 5.139,
      "grad_norm": 3.1908252239227295,
      "learning_rate": 1.218482790565581e-06,
      "loss": 0.4349,
      "step": 18105
    },
    {
      "epoch": 5.13925,
      "grad_norm": 4.33655309677124,
      "learning_rate": 1.212117042256114e-06,
      "loss": 0.4771,
      "step": 18110
    },
    {
      "epoch": 5.1395,
      "grad_norm": 3.348439931869507,
      "learning_rate": 1.2057675527403417e-06,
      "loss": 0.4222,
      "step": 18115
    },
    {
      "epoch": 5.13975,
      "grad_norm": 3.6628224849700928,
      "learning_rate": 1.1994343263580844e-06,
      "loss": 0.4663,
      "step": 18120
    },
    {
      "epoch": 5.14,
      "grad_norm": 4.315247535705566,
      "learning_rate": 1.1931173674380402e-06,
      "loss": 0.4386,
      "step": 18125
    },
    {
      "epoch": 5.14025,
      "grad_norm": 4.770240306854248,
      "learning_rate": 1.1868166802977865e-06,
      "loss": 0.477,
      "step": 18130
    },
    {
      "epoch": 5.1405,
      "grad_norm": 4.082789421081543,
      "learning_rate": 1.1805322692437892e-06,
      "loss": 0.4433,
      "step": 18135
    },
    {
      "epoch": 5.14075,
      "grad_norm": 3.9485714435577393,
      "learning_rate": 1.1742641385713793e-06,
      "loss": 0.4587,
      "step": 18140
    },
    {
      "epoch": 5.141,
      "grad_norm": 3.316173553466797,
      "learning_rate": 1.1680122925647658e-06,
      "loss": 0.4141,
      "step": 18145
    },
    {
      "epoch": 5.14125,
      "grad_norm": 4.2391510009765625,
      "learning_rate": 1.1617767354970332e-06,
      "loss": 0.4373,
      "step": 18150
    },
    {
      "epoch": 5.1415,
      "grad_norm": 3.4316978454589844,
      "learning_rate": 1.1555574716301199e-06,
      "loss": 0.4325,
      "step": 18155
    },
    {
      "epoch": 5.14175,
      "grad_norm": 3.3583779335021973,
      "learning_rate": 1.1493545052148308e-06,
      "loss": 0.4133,
      "step": 18160
    },
    {
      "epoch": 5.142,
      "grad_norm": 3.74611759185791,
      "learning_rate": 1.1431678404908413e-06,
      "loss": 0.4615,
      "step": 18165
    },
    {
      "epoch": 5.14225,
      "grad_norm": 4.562849044799805,
      "learning_rate": 1.1369974816866775e-06,
      "loss": 0.4441,
      "step": 18170
    },
    {
      "epoch": 5.1425,
      "grad_norm": 3.3940114974975586,
      "learning_rate": 1.130843433019721e-06,
      "loss": 0.3968,
      "step": 18175
    },
    {
      "epoch": 5.14275,
      "grad_norm": 2.9685962200164795,
      "learning_rate": 1.1247056986962045e-06,
      "loss": 0.4559,
      "step": 18180
    },
    {
      "epoch": 5.143,
      "grad_norm": 3.5995702743530273,
      "learning_rate": 1.1185842829112142e-06,
      "loss": 0.4322,
      "step": 18185
    },
    {
      "epoch": 5.14325,
      "grad_norm": 3.679683208465576,
      "learning_rate": 1.1124791898486753e-06,
      "loss": 0.4576,
      "step": 18190
    },
    {
      "epoch": 5.1435,
      "grad_norm": 5.375760555267334,
      "learning_rate": 1.1063904236813639e-06,
      "loss": 0.6057,
      "step": 18195
    },
    {
      "epoch": 5.14375,
      "grad_norm": 3.74472975730896,
      "learning_rate": 1.100317988570898e-06,
      "loss": 0.4425,
      "step": 18200
    },
    {
      "epoch": 5.144,
      "grad_norm": 4.46480655670166,
      "learning_rate": 1.0942618886677219e-06,
      "loss": 0.4144,
      "step": 18205
    },
    {
      "epoch": 5.1442499999999995,
      "grad_norm": 3.1219401359558105,
      "learning_rate": 1.08822212811113e-06,
      "loss": 0.4576,
      "step": 18210
    },
    {
      "epoch": 5.1445,
      "grad_norm": 3.5663952827453613,
      "learning_rate": 1.0821987110292365e-06,
      "loss": 0.3888,
      "step": 18215
    },
    {
      "epoch": 5.14475,
      "grad_norm": 3.1716055870056152,
      "learning_rate": 1.0761916415389955e-06,
      "loss": 0.4135,
      "step": 18220
    },
    {
      "epoch": 5.145,
      "grad_norm": 5.04396390914917,
      "learning_rate": 1.0702009237461814e-06,
      "loss": 0.4586,
      "step": 18225
    },
    {
      "epoch": 5.14525,
      "grad_norm": 2.9784622192382812,
      "learning_rate": 1.0642265617453905e-06,
      "loss": 0.3729,
      "step": 18230
    },
    {
      "epoch": 5.1455,
      "grad_norm": 5.529727935791016,
      "learning_rate": 1.0582685596200403e-06,
      "loss": 0.4422,
      "step": 18235
    },
    {
      "epoch": 5.14575,
      "grad_norm": 3.725497007369995,
      "learning_rate": 1.0523269214423731e-06,
      "loss": 0.4384,
      "step": 18240
    },
    {
      "epoch": 5.146,
      "grad_norm": 4.016532897949219,
      "learning_rate": 1.0464016512734432e-06,
      "loss": 0.4228,
      "step": 18245
    },
    {
      "epoch": 5.14625,
      "grad_norm": 2.9851338863372803,
      "learning_rate": 1.0404927531631137e-06,
      "loss": 0.406,
      "step": 18250
    },
    {
      "epoch": 5.1465,
      "grad_norm": 11.58692741394043,
      "learning_rate": 1.034600231150068e-06,
      "loss": 0.9658,
      "step": 18255
    },
    {
      "epoch": 5.14675,
      "grad_norm": 10.501190185546875,
      "learning_rate": 1.028724089261779e-06,
      "loss": 1.2002,
      "step": 18260
    },
    {
      "epoch": 5.147,
      "grad_norm": 12.295038223266602,
      "learning_rate": 1.0228643315145419e-06,
      "loss": 1.1217,
      "step": 18265
    },
    {
      "epoch": 5.14725,
      "grad_norm": 10.348236083984375,
      "learning_rate": 1.0170209619134425e-06,
      "loss": 1.0109,
      "step": 18270
    },
    {
      "epoch": 5.1475,
      "grad_norm": 11.610356330871582,
      "learning_rate": 1.0111939844523716e-06,
      "loss": 1.0017,
      "step": 18275
    },
    {
      "epoch": 5.14775,
      "grad_norm": 12.668301582336426,
      "learning_rate": 1.0053834031140102e-06,
      "loss": 1.0008,
      "step": 18280
    },
    {
      "epoch": 5.148,
      "grad_norm": 12.275579452514648,
      "learning_rate": 9.99589221869837e-07,
      "loss": 0.9899,
      "step": 18285
    },
    {
      "epoch": 5.14825,
      "grad_norm": 10.22431468963623,
      "learning_rate": 9.938114446801178e-07,
      "loss": 0.8999,
      "step": 18290
    },
    {
      "epoch": 5.1485,
      "grad_norm": 10.363418579101562,
      "learning_rate": 9.880500754939125e-07,
      "loss": 0.9737,
      "step": 18295
    },
    {
      "epoch": 5.14875,
      "grad_norm": 14.998533248901367,
      "learning_rate": 9.823051182490633e-07,
      "loss": 0.9412,
      "step": 18300
    },
    {
      "epoch": 5.149,
      "grad_norm": 10.190248489379883,
      "learning_rate": 9.76576576872193e-07,
      "loss": 0.8949,
      "step": 18305
    },
    {
      "epoch": 5.14925,
      "grad_norm": 9.480664253234863,
      "learning_rate": 9.708644552787027e-07,
      "loss": 0.8446,
      "step": 18310
    },
    {
      "epoch": 5.1495,
      "grad_norm": 10.633968353271484,
      "learning_rate": 9.651687573727753e-07,
      "loss": 0.9009,
      "step": 18315
    },
    {
      "epoch": 5.14975,
      "grad_norm": 9.5704345703125,
      "learning_rate": 9.594894870473713e-07,
      "loss": 0.8308,
      "step": 18320
    },
    {
      "epoch": 5.15,
      "grad_norm": 10.78795337677002,
      "learning_rate": 9.538266481842112e-07,
      "loss": 0.829,
      "step": 18325
    },
    {
      "epoch": 5.15025,
      "grad_norm": 10.78833293914795,
      "learning_rate": 9.481802446537985e-07,
      "loss": 0.8406,
      "step": 18330
    },
    {
      "epoch": 5.1505,
      "grad_norm": 9.081649780273438,
      "learning_rate": 9.425502803153907e-07,
      "loss": 0.953,
      "step": 18335
    },
    {
      "epoch": 5.15075,
      "grad_norm": 14.207785606384277,
      "learning_rate": 9.369367590170181e-07,
      "loss": 0.8542,
      "step": 18340
    },
    {
      "epoch": 5.151,
      "grad_norm": 9.621356964111328,
      "learning_rate": 9.31339684595467e-07,
      "loss": 0.8892,
      "step": 18345
    },
    {
      "epoch": 5.15125,
      "grad_norm": 10.237824440002441,
      "learning_rate": 9.257590608762912e-07,
      "loss": 1.0641,
      "step": 18350
    },
    {
      "epoch": 5.1515,
      "grad_norm": 7.841197490692139,
      "learning_rate": 9.201948916737924e-07,
      "loss": 0.8922,
      "step": 18355
    },
    {
      "epoch": 5.15175,
      "grad_norm": 10.909608840942383,
      "learning_rate": 9.146471807910228e-07,
      "loss": 0.8913,
      "step": 18360
    },
    {
      "epoch": 5.152,
      "grad_norm": 10.372666358947754,
      "learning_rate": 9.09115932019794e-07,
      "loss": 0.9699,
      "step": 18365
    },
    {
      "epoch": 5.15225,
      "grad_norm": 11.635059356689453,
      "learning_rate": 9.036011491406648e-07,
      "loss": 0.8633,
      "step": 18370
    },
    {
      "epoch": 5.1525,
      "grad_norm": 15.36176872253418,
      "learning_rate": 8.981028359229316e-07,
      "loss": 0.9241,
      "step": 18375
    },
    {
      "epoch": 5.15275,
      "grad_norm": 11.135882377624512,
      "learning_rate": 8.926209961246463e-07,
      "loss": 0.9909,
      "step": 18380
    },
    {
      "epoch": 5.153,
      "grad_norm": 9.230291366577148,
      "learning_rate": 8.871556334925895e-07,
      "loss": 1.2715,
      "step": 18385
    },
    {
      "epoch": 5.15325,
      "grad_norm": 7.480597019195557,
      "learning_rate": 8.817067517622845e-07,
      "loss": 0.6762,
      "step": 18390
    },
    {
      "epoch": 6.00025,
      "grad_norm": 6.603722095489502,
      "learning_rate": 8.762743546579937e-07,
      "loss": 0.6111,
      "step": 18395
    },
    {
      "epoch": 6.0005,
      "grad_norm": 7.040330410003662,
      "learning_rate": 8.708584458927083e-07,
      "loss": 0.6611,
      "step": 18400
    },
    {
      "epoch": 6.00075,
      "grad_norm": 6.028264045715332,
      "learning_rate": 8.65459029168153e-07,
      "loss": 0.6605,
      "step": 18405
    },
    {
      "epoch": 6.001,
      "grad_norm": 7.798669815063477,
      "learning_rate": 8.60076108174776e-07,
      "loss": 0.6594,
      "step": 18410
    },
    {
      "epoch": 6.00125,
      "grad_norm": 7.342360019683838,
      "learning_rate": 8.547096865917537e-07,
      "loss": 0.7282,
      "step": 18415
    },
    {
      "epoch": 6.0015,
      "grad_norm": 7.561140060424805,
      "learning_rate": 8.493597680869797e-07,
      "loss": 0.7013,
      "step": 18420
    },
    {
      "epoch": 6.00175,
      "grad_norm": 6.050628185272217,
      "learning_rate": 8.440263563170819e-07,
      "loss": 0.6426,
      "step": 18425
    },
    {
      "epoch": 6.002,
      "grad_norm": 7.242065906524658,
      "learning_rate": 8.387094549273916e-07,
      "loss": 0.6739,
      "step": 18430
    },
    {
      "epoch": 6.00225,
      "grad_norm": 8.087265968322754,
      "learning_rate": 8.334090675519601e-07,
      "loss": 0.672,
      "step": 18435
    },
    {
      "epoch": 6.0025,
      "grad_norm": 8.681352615356445,
      "learning_rate": 8.281251978135534e-07,
      "loss": 0.7014,
      "step": 18440
    },
    {
      "epoch": 6.00275,
      "grad_norm": 8.690567016601562,
      "learning_rate": 8.228578493236466e-07,
      "loss": 0.751,
      "step": 18445
    },
    {
      "epoch": 6.003,
      "grad_norm": 8.865683555603027,
      "learning_rate": 8.176070256824209e-07,
      "loss": 0.7465,
      "step": 18450
    },
    {
      "epoch": 6.00325,
      "grad_norm": 7.468060493469238,
      "learning_rate": 8.12372730478772e-07,
      "loss": 0.7608,
      "step": 18455
    },
    {
      "epoch": 6.0035,
      "grad_norm": 6.9655680656433105,
      "learning_rate": 8.071549672902856e-07,
      "loss": 0.6916,
      "step": 18460
    },
    {
      "epoch": 6.00375,
      "grad_norm": 6.115088939666748,
      "learning_rate": 8.019537396832533e-07,
      "loss": 0.6477,
      "step": 18465
    },
    {
      "epoch": 6.004,
      "grad_norm": 7.739883899688721,
      "learning_rate": 7.967690512126647e-07,
      "loss": 0.7364,
      "step": 18470
    },
    {
      "epoch": 6.00425,
      "grad_norm": 10.198657989501953,
      "learning_rate": 7.916009054222046e-07,
      "loss": 0.7449,
      "step": 18475
    },
    {
      "epoch": 6.0045,
      "grad_norm": 7.965575695037842,
      "learning_rate": 7.864493058442585e-07,
      "loss": 0.6709,
      "step": 18480
    },
    {
      "epoch": 6.00475,
      "grad_norm": 9.557981491088867,
      "learning_rate": 7.813142559998904e-07,
      "loss": 0.7584,
      "step": 18485
    },
    {
      "epoch": 6.005,
      "grad_norm": 8.515338897705078,
      "learning_rate": 7.761957593988566e-07,
      "loss": 0.7536,
      "step": 18490
    },
    {
      "epoch": 6.00525,
      "grad_norm": 8.151549339294434,
      "learning_rate": 7.710938195396089e-07,
      "loss": 0.7718,
      "step": 18495
    },
    {
      "epoch": 6.0055,
      "grad_norm": 7.627954483032227,
      "learning_rate": 7.66008439909266e-07,
      "loss": 0.7593,
      "step": 18500
    },
    {
      "epoch": 6.0055,
      "eval_loss": 2.1475565433502197,
      "eval_runtime": 5.1222,
      "eval_samples_per_second": 199.916,
      "eval_steps_per_second": 24.989,
      "step": 18500
    },
    {
      "epoch": 6.00575,
      "grad_norm": 9.058732032775879,
      "learning_rate": 7.609396239836474e-07,
      "loss": 0.7395,
      "step": 18505
    },
    {
      "epoch": 6.006,
      "grad_norm": 8.95475959777832,
      "learning_rate": 7.55887375227235e-07,
      "loss": 0.7613,
      "step": 18510
    },
    {
      "epoch": 6.00625,
      "grad_norm": 8.405522346496582,
      "learning_rate": 7.508516970931883e-07,
      "loss": 0.7457,
      "step": 18515
    },
    {
      "epoch": 6.0065,
      "grad_norm": 8.373041152954102,
      "learning_rate": 7.458325930233517e-07,
      "loss": 0.7907,
      "step": 18520
    },
    {
      "epoch": 6.00675,
      "grad_norm": 7.124508380889893,
      "learning_rate": 7.408300664482365e-07,
      "loss": 0.6682,
      "step": 18525
    },
    {
      "epoch": 6.007,
      "grad_norm": 8.410907745361328,
      "learning_rate": 7.358441207870215e-07,
      "loss": 0.6663,
      "step": 18530
    },
    {
      "epoch": 6.00725,
      "grad_norm": 8.066904067993164,
      "learning_rate": 7.308747594475502e-07,
      "loss": 0.7158,
      "step": 18535
    },
    {
      "epoch": 6.0075,
      "grad_norm": 7.5332183837890625,
      "learning_rate": 7.25921985826336e-07,
      "loss": 0.798,
      "step": 18540
    },
    {
      "epoch": 6.00775,
      "grad_norm": 6.776494979858398,
      "learning_rate": 7.209858033085549e-07,
      "loss": 0.7239,
      "step": 18545
    },
    {
      "epoch": 6.008,
      "grad_norm": 7.104357719421387,
      "learning_rate": 7.16066215268038e-07,
      "loss": 0.7189,
      "step": 18550
    },
    {
      "epoch": 6.00825,
      "grad_norm": 7.465789318084717,
      "learning_rate": 7.111632250672851e-07,
      "loss": 0.7313,
      "step": 18555
    },
    {
      "epoch": 6.0085,
      "grad_norm": 8.590730667114258,
      "learning_rate": 7.062768360574345e-07,
      "loss": 0.6938,
      "step": 18560
    },
    {
      "epoch": 6.00875,
      "grad_norm": 7.553831100463867,
      "learning_rate": 7.0140705157829e-07,
      "loss": 0.6398,
      "step": 18565
    },
    {
      "epoch": 6.009,
      "grad_norm": 8.841495513916016,
      "learning_rate": 6.965538749583028e-07,
      "loss": 0.6932,
      "step": 18570
    },
    {
      "epoch": 6.00925,
      "grad_norm": 6.0930256843566895,
      "learning_rate": 6.917173095145724e-07,
      "loss": 0.6213,
      "step": 18575
    },
    {
      "epoch": 6.0095,
      "grad_norm": 8.432561874389648,
      "learning_rate": 6.868973585528543e-07,
      "loss": 0.6773,
      "step": 18580
    },
    {
      "epoch": 6.00975,
      "grad_norm": 8.27431869506836,
      "learning_rate": 6.820940253675273e-07,
      "loss": 0.7264,
      "step": 18585
    },
    {
      "epoch": 6.01,
      "grad_norm": 6.042911052703857,
      "learning_rate": 6.773073132416291e-07,
      "loss": 0.6113,
      "step": 18590
    },
    {
      "epoch": 6.01025,
      "grad_norm": 7.114754676818848,
      "learning_rate": 6.725372254468343e-07,
      "loss": 0.6385,
      "step": 18595
    },
    {
      "epoch": 6.0105,
      "grad_norm": 5.861974239349365,
      "learning_rate": 6.677837652434543e-07,
      "loss": 0.674,
      "step": 18600
    },
    {
      "epoch": 6.01075,
      "grad_norm": 6.46366024017334,
      "learning_rate": 6.630469358804292e-07,
      "loss": 0.608,
      "step": 18605
    },
    {
      "epoch": 6.011,
      "grad_norm": 6.439981460571289,
      "learning_rate": 6.583267405953414e-07,
      "loss": 0.6491,
      "step": 18610
    },
    {
      "epoch": 6.01125,
      "grad_norm": 5.9129228591918945,
      "learning_rate": 6.536231826143963e-07,
      "loss": 0.5969,
      "step": 18615
    },
    {
      "epoch": 6.0115,
      "grad_norm": 6.186031818389893,
      "learning_rate": 6.489362651524333e-07,
      "loss": 0.6265,
      "step": 18620
    },
    {
      "epoch": 6.01175,
      "grad_norm": 7.256247043609619,
      "learning_rate": 6.442659914129179e-07,
      "loss": 0.6744,
      "step": 18625
    },
    {
      "epoch": 6.012,
      "grad_norm": 6.353715419769287,
      "learning_rate": 6.396123645879381e-07,
      "loss": 0.7169,
      "step": 18630
    },
    {
      "epoch": 6.01225,
      "grad_norm": 5.666987895965576,
      "learning_rate": 6.349753878581971e-07,
      "loss": 0.8388,
      "step": 18635
    },
    {
      "epoch": 6.0125,
      "grad_norm": 6.24338436126709,
      "learning_rate": 6.303550643930317e-07,
      "loss": 0.7293,
      "step": 18640
    },
    {
      "epoch": 6.01275,
      "grad_norm": 6.193025588989258,
      "learning_rate": 6.257513973503909e-07,
      "loss": 0.7287,
      "step": 18645
    },
    {
      "epoch": 6.013,
      "grad_norm": 7.19304895401001,
      "learning_rate": 6.211643898768327e-07,
      "loss": 0.7037,
      "step": 18650
    },
    {
      "epoch": 6.01325,
      "grad_norm": 8.235129356384277,
      "learning_rate": 6.165940451075352e-07,
      "loss": 0.7247,
      "step": 18655
    },
    {
      "epoch": 6.0135,
      "grad_norm": 8.789649963378906,
      "learning_rate": 6.120403661662888e-07,
      "loss": 0.6734,
      "step": 18660
    },
    {
      "epoch": 6.01375,
      "grad_norm": 4.294572830200195,
      "learning_rate": 6.075033561654813e-07,
      "loss": 0.6617,
      "step": 18665
    },
    {
      "epoch": 6.014,
      "grad_norm": 5.473461627960205,
      "learning_rate": 6.02983018206127e-07,
      "loss": 0.4952,
      "step": 18670
    },
    {
      "epoch": 6.01425,
      "grad_norm": 4.445402145385742,
      "learning_rate": 5.984793553778262e-07,
      "loss": 0.5381,
      "step": 18675
    },
    {
      "epoch": 6.0145,
      "grad_norm": 4.693842887878418,
      "learning_rate": 5.939923707587974e-07,
      "loss": 0.525,
      "step": 18680
    },
    {
      "epoch": 6.01475,
      "grad_norm": 4.580007553100586,
      "learning_rate": 5.895220674158458e-07,
      "loss": 0.5433,
      "step": 18685
    },
    {
      "epoch": 6.015,
      "grad_norm": 3.697129726409912,
      "learning_rate": 5.850684484043856e-07,
      "loss": 0.4707,
      "step": 18690
    },
    {
      "epoch": 6.01525,
      "grad_norm": 4.7364277839660645,
      "learning_rate": 5.806315167684234e-07,
      "loss": 0.6351,
      "step": 18695
    },
    {
      "epoch": 6.0155,
      "grad_norm": 6.142502784729004,
      "learning_rate": 5.762112755405558e-07,
      "loss": 0.4887,
      "step": 18700
    },
    {
      "epoch": 6.01575,
      "grad_norm": 4.2345123291015625,
      "learning_rate": 5.71807727741977e-07,
      "loss": 0.4394,
      "step": 18705
    },
    {
      "epoch": 6.016,
      "grad_norm": 4.2441816329956055,
      "learning_rate": 5.674208763824768e-07,
      "loss": 0.4984,
      "step": 18710
    },
    {
      "epoch": 6.01625,
      "grad_norm": 4.621620178222656,
      "learning_rate": 5.630507244604205e-07,
      "loss": 0.5067,
      "step": 18715
    },
    {
      "epoch": 6.0165,
      "grad_norm": 9.043259620666504,
      "learning_rate": 5.586972749627634e-07,
      "loss": 0.5164,
      "step": 18720
    },
    {
      "epoch": 6.01675,
      "grad_norm": 7.63554573059082,
      "learning_rate": 5.543605308650557e-07,
      "loss": 0.7353,
      "step": 18725
    },
    {
      "epoch": 6.017,
      "grad_norm": 9.126182556152344,
      "learning_rate": 5.500404951314153e-07,
      "loss": 0.7635,
      "step": 18730
    },
    {
      "epoch": 6.01725,
      "grad_norm": 8.367263793945312,
      "learning_rate": 5.45737170714547e-07,
      "loss": 0.7829,
      "step": 18735
    },
    {
      "epoch": 6.0175,
      "grad_norm": 7.878735542297363,
      "learning_rate": 5.41450560555734e-07,
      "loss": 0.7645,
      "step": 18740
    },
    {
      "epoch": 6.01775,
      "grad_norm": 9.146770477294922,
      "learning_rate": 5.371806675848329e-07,
      "loss": 0.7165,
      "step": 18745
    },
    {
      "epoch": 6.018,
      "grad_norm": 9.278007507324219,
      "learning_rate": 5.329274947202728e-07,
      "loss": 0.7501,
      "step": 18750
    },
    {
      "epoch": 6.01825,
      "grad_norm": 9.590038299560547,
      "learning_rate": 5.286910448690591e-07,
      "loss": 0.7629,
      "step": 18755
    },
    {
      "epoch": 6.0185,
      "grad_norm": 6.610872745513916,
      "learning_rate": 5.244713209267671e-07,
      "loss": 0.752,
      "step": 18760
    },
    {
      "epoch": 6.01875,
      "grad_norm": 7.31695556640625,
      "learning_rate": 5.202683257775342e-07,
      "loss": 0.6922,
      "step": 18765
    },
    {
      "epoch": 6.019,
      "grad_norm": 8.72964859008789,
      "learning_rate": 5.160820622940737e-07,
      "loss": 0.7004,
      "step": 18770
    },
    {
      "epoch": 6.01925,
      "grad_norm": 7.166251182556152,
      "learning_rate": 5.119125333376523e-07,
      "loss": 0.6847,
      "step": 18775
    },
    {
      "epoch": 6.0195,
      "grad_norm": 8.176078796386719,
      "learning_rate": 5.07759741758107e-07,
      "loss": 0.7178,
      "step": 18780
    },
    {
      "epoch": 6.01975,
      "grad_norm": 6.35422945022583,
      "learning_rate": 5.036236903938285e-07,
      "loss": 0.6558,
      "step": 18785
    },
    {
      "epoch": 6.02,
      "grad_norm": 7.449253082275391,
      "learning_rate": 4.995043820717721e-07,
      "loss": 0.7574,
      "step": 18790
    },
    {
      "epoch": 6.02025,
      "grad_norm": 9.440828323364258,
      "learning_rate": 4.954018196074439e-07,
      "loss": 0.7892,
      "step": 18795
    },
    {
      "epoch": 6.0205,
      "grad_norm": 6.831736087799072,
      "learning_rate": 4.913160058049066e-07,
      "loss": 0.7631,
      "step": 18800
    },
    {
      "epoch": 6.02075,
      "grad_norm": 7.645646095275879,
      "learning_rate": 4.872469434567762e-07,
      "loss": 0.7261,
      "step": 18805
    },
    {
      "epoch": 6.021,
      "grad_norm": 9.347010612487793,
      "learning_rate": 4.831946353442223e-07,
      "loss": 0.7341,
      "step": 18810
    },
    {
      "epoch": 6.02125,
      "grad_norm": 6.2487711906433105,
      "learning_rate": 4.79159084236952e-07,
      "loss": 0.6914,
      "step": 18815
    },
    {
      "epoch": 6.0215,
      "grad_norm": 7.841977119445801,
      "learning_rate": 4.751402928932308e-07,
      "loss": 0.7485,
      "step": 18820
    },
    {
      "epoch": 6.02175,
      "grad_norm": 9.300353050231934,
      "learning_rate": 4.711382640598616e-07,
      "loss": 0.6433,
      "step": 18825
    },
    {
      "epoch": 6.022,
      "grad_norm": 6.61582088470459,
      "learning_rate": 4.67153000472198e-07,
      "loss": 0.6242,
      "step": 18830
    },
    {
      "epoch": 6.02225,
      "grad_norm": 8.13792896270752,
      "learning_rate": 4.631845048541306e-07,
      "loss": 0.685,
      "step": 18835
    },
    {
      "epoch": 6.0225,
      "grad_norm": 6.674307823181152,
      "learning_rate": 4.592327799180812e-07,
      "loss": 0.743,
      "step": 18840
    },
    {
      "epoch": 6.02275,
      "grad_norm": 6.36258602142334,
      "learning_rate": 4.552978283650228e-07,
      "loss": 0.6556,
      "step": 18845
    },
    {
      "epoch": 6.023,
      "grad_norm": 8.30418872833252,
      "learning_rate": 4.513796528844538e-07,
      "loss": 0.6789,
      "step": 18850
    },
    {
      "epoch": 6.02325,
      "grad_norm": 7.39647102355957,
      "learning_rate": 4.4747825615441274e-07,
      "loss": 0.6336,
      "step": 18855
    },
    {
      "epoch": 6.0235,
      "grad_norm": 6.525386333465576,
      "learning_rate": 4.4359364084147206e-07,
      "loss": 0.7149,
      "step": 18860
    },
    {
      "epoch": 6.02375,
      "grad_norm": 6.730978012084961,
      "learning_rate": 4.3972580960071897e-07,
      "loss": 0.6346,
      "step": 18865
    },
    {
      "epoch": 6.024,
      "grad_norm": 7.140395641326904,
      "learning_rate": 4.3587476507578883e-07,
      "loss": 0.7435,
      "step": 18870
    },
    {
      "epoch": 6.02425,
      "grad_norm": 5.872252941131592,
      "learning_rate": 4.3204050989882895e-07,
      "loss": 0.6112,
      "step": 18875
    },
    {
      "epoch": 6.0245,
      "grad_norm": 8.318904876708984,
      "learning_rate": 4.2822304669052073e-07,
      "loss": 0.8537,
      "step": 18880
    },
    {
      "epoch": 6.02475,
      "grad_norm": 6.884391784667969,
      "learning_rate": 4.2442237806006314e-07,
      "loss": 0.6671,
      "step": 18885
    },
    {
      "epoch": 6.025,
      "grad_norm": 8.076007843017578,
      "learning_rate": 4.2063850660517266e-07,
      "loss": 0.6901,
      "step": 18890
    },
    {
      "epoch": 6.02525,
      "grad_norm": 5.666559219360352,
      "learning_rate": 4.1687143491209446e-07,
      "loss": 0.6597,
      "step": 18895
    },
    {
      "epoch": 6.0255,
      "grad_norm": 8.941696166992188,
      "learning_rate": 4.1312116555558556e-07,
      "loss": 0.7145,
      "step": 18900
    },
    {
      "epoch": 6.02575,
      "grad_norm": 9.70930290222168,
      "learning_rate": 4.093877010989178e-07,
      "loss": 0.7231,
      "step": 18905
    },
    {
      "epoch": 6.026,
      "grad_norm": 6.931685924530029,
      "learning_rate": 4.0567104409388326e-07,
      "loss": 0.6381,
      "step": 18910
    },
    {
      "epoch": 6.02625,
      "grad_norm": 6.1946024894714355,
      "learning_rate": 4.0197119708078047e-07,
      "loss": 0.7689,
      "step": 18915
    },
    {
      "epoch": 6.0265,
      "grad_norm": 7.530642509460449,
      "learning_rate": 3.982881625884144e-07,
      "loss": 0.6283,
      "step": 18920
    },
    {
      "epoch": 6.02675,
      "grad_norm": 7.718896865844727,
      "learning_rate": 3.9462194313411303e-07,
      "loss": 0.8654,
      "step": 18925
    },
    {
      "epoch": 6.027,
      "grad_norm": 6.6106438636779785,
      "learning_rate": 3.909725412236997e-07,
      "loss": 0.729,
      "step": 18930
    },
    {
      "epoch": 6.02725,
      "grad_norm": 6.772953987121582,
      "learning_rate": 3.8733995935150426e-07,
      "loss": 0.6784,
      "step": 18935
    },
    {
      "epoch": 6.0275,
      "grad_norm": 6.937615871429443,
      "learning_rate": 3.837242000003599e-07,
      "loss": 0.662,
      "step": 18940
    },
    {
      "epoch": 6.02775,
      "grad_norm": 7.1430463790893555,
      "learning_rate": 3.8012526564160933e-07,
      "loss": 0.6627,
      "step": 18945
    },
    {
      "epoch": 6.028,
      "grad_norm": 7.6989827156066895,
      "learning_rate": 3.765431587350876e-07,
      "loss": 0.7225,
      "step": 18950
    },
    {
      "epoch": 6.02825,
      "grad_norm": 7.8672566413879395,
      "learning_rate": 3.729778817291307e-07,
      "loss": 0.7585,
      "step": 18955
    },
    {
      "epoch": 6.0285,
      "grad_norm": 6.819809436798096,
      "learning_rate": 3.694294370605755e-07,
      "loss": 0.6285,
      "step": 18960
    },
    {
      "epoch": 6.02875,
      "grad_norm": 7.576523780822754,
      "learning_rate": 3.6589782715474585e-07,
      "loss": 0.6801,
      "step": 18965
    },
    {
      "epoch": 6.029,
      "grad_norm": 9.185318946838379,
      "learning_rate": 3.623830544254664e-07,
      "loss": 0.6655,
      "step": 18970
    },
    {
      "epoch": 6.02925,
      "grad_norm": 6.472637176513672,
      "learning_rate": 3.588851212750488e-07,
      "loss": 0.6424,
      "step": 18975
    },
    {
      "epoch": 6.0295,
      "grad_norm": 5.119192600250244,
      "learning_rate": 3.554040300943001e-07,
      "loss": 0.6086,
      "step": 18980
    },
    {
      "epoch": 6.02975,
      "grad_norm": 7.329091548919678,
      "learning_rate": 3.519397832625143e-07,
      "loss": 0.6614,
      "step": 18985
    },
    {
      "epoch": 6.03,
      "grad_norm": 6.045851707458496,
      "learning_rate": 3.484923831474668e-07,
      "loss": 0.6498,
      "step": 18990
    },
    {
      "epoch": 6.03025,
      "grad_norm": 7.2557373046875,
      "learning_rate": 3.4506183210542555e-07,
      "loss": 0.6904,
      "step": 18995
    },
    {
      "epoch": 6.0305,
      "grad_norm": 5.6696457862854,
      "learning_rate": 3.416481324811399e-07,
      "loss": 0.6731,
      "step": 19000
    },
    {
      "epoch": 6.0305,
      "eval_loss": 2.1436662673950195,
      "eval_runtime": 5.3216,
      "eval_samples_per_second": 192.425,
      "eval_steps_per_second": 24.053,
      "step": 19000
    },
    {
      "epoch": 6.03075,
      "grad_norm": 6.036118507385254,
      "learning_rate": 3.382512866078408e-07,
      "loss": 0.6983,
      "step": 19005
    },
    {
      "epoch": 6.031,
      "grad_norm": 5.31749963760376,
      "learning_rate": 3.348712968072432e-07,
      "loss": 0.6245,
      "step": 19010
    },
    {
      "epoch": 6.03125,
      "grad_norm": 7.211766719818115,
      "learning_rate": 3.315081653895352e-07,
      "loss": 0.707,
      "step": 19015
    },
    {
      "epoch": 6.0315,
      "grad_norm": 8.483599662780762,
      "learning_rate": 3.2816189465338653e-07,
      "loss": 0.667,
      "step": 19020
    },
    {
      "epoch": 6.03175,
      "grad_norm": 5.648169994354248,
      "learning_rate": 3.248324868859398e-07,
      "loss": 0.6582,
      "step": 19025
    },
    {
      "epoch": 6.032,
      "grad_norm": 7.569453239440918,
      "learning_rate": 3.215199443628164e-07,
      "loss": 0.5978,
      "step": 19030
    },
    {
      "epoch": 6.03225,
      "grad_norm": 7.110361099243164,
      "learning_rate": 3.18224269348108e-07,
      "loss": 0.6189,
      "step": 19035
    },
    {
      "epoch": 6.0325,
      "grad_norm": 7.320435523986816,
      "learning_rate": 3.149454640943739e-07,
      "loss": 0.6606,
      "step": 19040
    },
    {
      "epoch": 6.03275,
      "grad_norm": 7.354891300201416,
      "learning_rate": 3.116835308426491e-07,
      "loss": 0.5804,
      "step": 19045
    },
    {
      "epoch": 6.033,
      "grad_norm": 7.045391082763672,
      "learning_rate": 3.0843847182243367e-07,
      "loss": 0.6746,
      "step": 19050
    },
    {
      "epoch": 6.03325,
      "grad_norm": 5.206526279449463,
      "learning_rate": 3.052102892516978e-07,
      "loss": 0.6616,
      "step": 19055
    },
    {
      "epoch": 6.0335,
      "grad_norm": 6.428595066070557,
      "learning_rate": 3.01998985336871e-07,
      "loss": 0.6567,
      "step": 19060
    },
    {
      "epoch": 6.03375,
      "grad_norm": 6.732783794403076,
      "learning_rate": 2.9880456227284745e-07,
      "loss": 0.6497,
      "step": 19065
    },
    {
      "epoch": 6.034,
      "grad_norm": 6.913427829742432,
      "learning_rate": 2.956270222429891e-07,
      "loss": 0.7298,
      "step": 19070
    },
    {
      "epoch": 6.03425,
      "grad_norm": 5.083329677581787,
      "learning_rate": 2.924663674191114e-07,
      "loss": 0.6387,
      "step": 19075
    },
    {
      "epoch": 6.0345,
      "grad_norm": 8.263044357299805,
      "learning_rate": 2.893225999614918e-07,
      "loss": 0.6609,
      "step": 19080
    },
    {
      "epoch": 6.03475,
      "grad_norm": 5.092406749725342,
      "learning_rate": 2.861957220188727e-07,
      "loss": 0.6798,
      "step": 19085
    },
    {
      "epoch": 6.035,
      "grad_norm": 6.777709484100342,
      "learning_rate": 2.830857357284361e-07,
      "loss": 0.7603,
      "step": 19090
    },
    {
      "epoch": 6.03525,
      "grad_norm": 6.41132116317749,
      "learning_rate": 2.7999264321583174e-07,
      "loss": 0.6999,
      "step": 19095
    },
    {
      "epoch": 6.0355,
      "grad_norm": 7.638424396514893,
      "learning_rate": 2.7691644659516293e-07,
      "loss": 0.6739,
      "step": 19100
    },
    {
      "epoch": 6.03575,
      "grad_norm": 6.844451904296875,
      "learning_rate": 2.7385714796897553e-07,
      "loss": 0.6938,
      "step": 19105
    },
    {
      "epoch": 6.036,
      "grad_norm": 8.891005516052246,
      "learning_rate": 2.7081474942827754e-07,
      "loss": 0.7806,
      "step": 19110
    },
    {
      "epoch": 6.03625,
      "grad_norm": 6.053333759307861,
      "learning_rate": 2.677892530525139e-07,
      "loss": 0.6461,
      "step": 19115
    },
    {
      "epoch": 6.0365,
      "grad_norm": 6.689225196838379,
      "learning_rate": 2.64780660909586e-07,
      "loss": 0.7137,
      "step": 19120
    },
    {
      "epoch": 6.03675,
      "grad_norm": 6.094173908233643,
      "learning_rate": 2.61788975055835e-07,
      "loss": 0.6697,
      "step": 19125
    },
    {
      "epoch": 6.037,
      "grad_norm": 6.6626458168029785,
      "learning_rate": 2.5881419753605306e-07,
      "loss": 0.7014,
      "step": 19130
    },
    {
      "epoch": 6.03725,
      "grad_norm": 6.510977745056152,
      "learning_rate": 2.5585633038347213e-07,
      "loss": 0.6855,
      "step": 19135
    },
    {
      "epoch": 6.0375,
      "grad_norm": 8.801438331604004,
      "learning_rate": 2.529153756197666e-07,
      "loss": 0.7809,
      "step": 19140
    },
    {
      "epoch": 6.03775,
      "grad_norm": 7.467658042907715,
      "learning_rate": 2.4999133525504813e-07,
      "loss": 0.6793,
      "step": 19145
    },
    {
      "epoch": 6.038,
      "grad_norm": 6.802034378051758,
      "learning_rate": 2.470842112878763e-07,
      "loss": 0.6953,
      "step": 19150
    },
    {
      "epoch": 6.03825,
      "grad_norm": 7.7068705558776855,
      "learning_rate": 2.441940057052394e-07,
      "loss": 0.6888,
      "step": 19155
    },
    {
      "epoch": 6.0385,
      "grad_norm": 7.634429931640625,
      "learning_rate": 2.413207204825657e-07,
      "loss": 0.6759,
      "step": 19160
    },
    {
      "epoch": 6.03875,
      "grad_norm": 6.458838939666748,
      "learning_rate": 2.384643575837203e-07,
      "loss": 0.7604,
      "step": 19165
    },
    {
      "epoch": 6.039,
      "grad_norm": 5.411010265350342,
      "learning_rate": 2.3562491896099714e-07,
      "loss": 0.7054,
      "step": 19170
    },
    {
      "epoch": 6.03925,
      "grad_norm": 8.298126220703125,
      "learning_rate": 2.3280240655512987e-07,
      "loss": 0.6707,
      "step": 19175
    },
    {
      "epoch": 6.0395,
      "grad_norm": 7.809189796447754,
      "learning_rate": 2.299968222952753e-07,
      "loss": 0.6562,
      "step": 19180
    },
    {
      "epoch": 6.03975,
      "grad_norm": 6.247745037078857,
      "learning_rate": 2.2720816809902734e-07,
      "loss": 0.6117,
      "step": 19185
    },
    {
      "epoch": 6.04,
      "grad_norm": 6.241885185241699,
      "learning_rate": 2.2443644587240297e-07,
      "loss": 0.6219,
      "step": 19190
    },
    {
      "epoch": 6.04025,
      "grad_norm": 5.203590393066406,
      "learning_rate": 2.2168165750984516e-07,
      "loss": 0.6936,
      "step": 19195
    },
    {
      "epoch": 6.0405,
      "grad_norm": 6.154806137084961,
      "learning_rate": 2.1894380489423106e-07,
      "loss": 0.6375,
      "step": 19200
    },
    {
      "epoch": 6.04075,
      "grad_norm": 6.205841541290283,
      "learning_rate": 2.1622288989685546e-07,
      "loss": 0.6157,
      "step": 19205
    },
    {
      "epoch": 6.041,
      "grad_norm": 8.106502532958984,
      "learning_rate": 2.1351891437743632e-07,
      "loss": 0.7105,
      "step": 19210
    },
    {
      "epoch": 6.04125,
      "grad_norm": 6.096228122711182,
      "learning_rate": 2.1083188018412025e-07,
      "loss": 0.6034,
      "step": 19215
    },
    {
      "epoch": 6.0415,
      "grad_norm": 7.11438512802124,
      "learning_rate": 2.081617891534604e-07,
      "loss": 0.6395,
      "step": 19220
    },
    {
      "epoch": 6.04175,
      "grad_norm": 6.961639881134033,
      "learning_rate": 2.0550864311044692e-07,
      "loss": 0.6804,
      "step": 19225
    },
    {
      "epoch": 6.042,
      "grad_norm": 7.2400898933410645,
      "learning_rate": 2.0287244386847925e-07,
      "loss": 0.6658,
      "step": 19230
    },
    {
      "epoch": 6.04225,
      "grad_norm": 7.2084856033325195,
      "learning_rate": 2.002531932293744e-07,
      "loss": 0.6434,
      "step": 19235
    },
    {
      "epoch": 6.0425,
      "grad_norm": 7.978647232055664,
      "learning_rate": 1.9765089298336148e-07,
      "loss": 0.6733,
      "step": 19240
    },
    {
      "epoch": 6.04275,
      "grad_norm": 6.5934672355651855,
      "learning_rate": 1.950655449090899e-07,
      "loss": 0.7423,
      "step": 19245
    },
    {
      "epoch": 6.043,
      "grad_norm": 7.797590732574463,
      "learning_rate": 1.9249715077362396e-07,
      "loss": 0.7094,
      "step": 19250
    },
    {
      "epoch": 6.04325,
      "grad_norm": 6.354489326477051,
      "learning_rate": 1.899457123324344e-07,
      "loss": 0.7099,
      "step": 19255
    },
    {
      "epoch": 6.0435,
      "grad_norm": 8.370857238769531,
      "learning_rate": 1.8741123132940685e-07,
      "loss": 0.7803,
      "step": 19260
    },
    {
      "epoch": 6.04375,
      "grad_norm": 5.56908655166626,
      "learning_rate": 1.8489370949683339e-07,
      "loss": 0.6145,
      "step": 19265
    },
    {
      "epoch": 6.044,
      "grad_norm": 6.392630100250244,
      "learning_rate": 1.8239314855541535e-07,
      "loss": 0.6545,
      "step": 19270
    },
    {
      "epoch": 6.04425,
      "grad_norm": 4.892670154571533,
      "learning_rate": 1.7990955021426615e-07,
      "loss": 0.6464,
      "step": 19275
    },
    {
      "epoch": 6.0445,
      "grad_norm": 5.787752151489258,
      "learning_rate": 1.7744291617089736e-07,
      "loss": 0.6035,
      "step": 19280
    },
    {
      "epoch": 6.04475,
      "grad_norm": 6.023453712463379,
      "learning_rate": 1.7499324811123542e-07,
      "loss": 0.6447,
      "step": 19285
    },
    {
      "epoch": 6.045,
      "grad_norm": 6.940763473510742,
      "learning_rate": 1.7256054770960207e-07,
      "loss": 0.6372,
      "step": 19290
    },
    {
      "epoch": 6.04525,
      "grad_norm": 5.638411998748779,
      "learning_rate": 1.7014481662872562e-07,
      "loss": 0.6791,
      "step": 19295
    },
    {
      "epoch": 6.0455,
      "grad_norm": 7.047427654266357,
      "learning_rate": 1.677460565197353e-07,
      "loss": 0.6423,
      "step": 19300
    },
    {
      "epoch": 6.04575,
      "grad_norm": 4.884451866149902,
      "learning_rate": 1.653642690221613e-07,
      "loss": 0.6539,
      "step": 19305
    },
    {
      "epoch": 6.046,
      "grad_norm": 6.325741767883301,
      "learning_rate": 1.6299945576393195e-07,
      "loss": 0.6724,
      "step": 19310
    },
    {
      "epoch": 6.04625,
      "grad_norm": 7.896563529968262,
      "learning_rate": 1.6065161836137654e-07,
      "loss": 0.7586,
      "step": 19315
    },
    {
      "epoch": 6.0465,
      "grad_norm": 6.956372261047363,
      "learning_rate": 1.5832075841921701e-07,
      "loss": 0.6392,
      "step": 19320
    },
    {
      "epoch": 6.04675,
      "grad_norm": 5.8317179679870605,
      "learning_rate": 1.5600687753057896e-07,
      "loss": 0.6362,
      "step": 19325
    },
    {
      "epoch": 6.047,
      "grad_norm": 6.277667045593262,
      "learning_rate": 1.5370997727697235e-07,
      "loss": 0.6607,
      "step": 19330
    },
    {
      "epoch": 6.04725,
      "grad_norm": 6.578365802764893,
      "learning_rate": 1.5143005922831355e-07,
      "loss": 0.6569,
      "step": 19335
    },
    {
      "epoch": 6.0475,
      "grad_norm": 6.1448893547058105,
      "learning_rate": 1.4916712494289776e-07,
      "loss": 0.6172,
      "step": 19340
    },
    {
      "epoch": 6.04775,
      "grad_norm": 8.048575401306152,
      "learning_rate": 1.4692117596742383e-07,
      "loss": 0.708,
      "step": 19345
    },
    {
      "epoch": 6.048,
      "grad_norm": 7.192996501922607,
      "learning_rate": 1.4469221383697495e-07,
      "loss": 0.7379,
      "step": 19350
    },
    {
      "epoch": 6.04825,
      "grad_norm": 6.492591381072998,
      "learning_rate": 1.424802400750269e-07,
      "loss": 0.7239,
      "step": 19355
    },
    {
      "epoch": 6.0485,
      "grad_norm": 5.96885871887207,
      "learning_rate": 1.4028525619344257e-07,
      "loss": 0.7021,
      "step": 19360
    },
    {
      "epoch": 6.04875,
      "grad_norm": 6.998283386230469,
      "learning_rate": 1.3810726369247462e-07,
      "loss": 0.7556,
      "step": 19365
    },
    {
      "epoch": 6.049,
      "grad_norm": 7.96668815612793,
      "learning_rate": 1.359462640607545e-07,
      "loss": 0.693,
      "step": 19370
    },
    {
      "epoch": 6.04925,
      "grad_norm": 6.173548698425293,
      "learning_rate": 1.3380225877531184e-07,
      "loss": 0.7515,
      "step": 19375
    },
    {
      "epoch": 6.0495,
      "grad_norm": 11.77735710144043,
      "learning_rate": 1.316752493015494e-07,
      "loss": 0.6879,
      "step": 19380
    },
    {
      "epoch": 6.04975,
      "grad_norm": 5.467530250549316,
      "learning_rate": 1.2956523709325986e-07,
      "loss": 0.6771,
      "step": 19385
    },
    {
      "epoch": 6.05,
      "grad_norm": 7.031332969665527,
      "learning_rate": 1.2747222359261734e-07,
      "loss": 0.6981,
      "step": 19390
    },
    {
      "epoch": 6.05025,
      "grad_norm": 7.393059730529785,
      "learning_rate": 1.2539621023017756e-07,
      "loss": 0.7408,
      "step": 19395
    },
    {
      "epoch": 6.0505,
      "grad_norm": 7.659244537353516,
      "learning_rate": 1.2333719842487211e-07,
      "loss": 0.7034,
      "step": 19400
    },
    {
      "epoch": 6.05075,
      "grad_norm": 9.315934181213379,
      "learning_rate": 1.2129518958401974e-07,
      "loss": 0.7461,
      "step": 19405
    },
    {
      "epoch": 6.051,
      "grad_norm": 4.951281547546387,
      "learning_rate": 1.1927018510331234e-07,
      "loss": 0.6773,
      "step": 19410
    },
    {
      "epoch": 6.05125,
      "grad_norm": 7.003556728363037,
      "learning_rate": 1.172621863668233e-07,
      "loss": 0.6934,
      "step": 19415
    },
    {
      "epoch": 6.0515,
      "grad_norm": 5.282439708709717,
      "learning_rate": 1.1527119474699644e-07,
      "loss": 0.5797,
      "step": 19420
    },
    {
      "epoch": 6.05175,
      "grad_norm": 7.103753566741943,
      "learning_rate": 1.1329721160466267e-07,
      "loss": 0.6975,
      "step": 19425
    },
    {
      "epoch": 6.052,
      "grad_norm": 5.420994281768799,
      "learning_rate": 1.1134023828901497e-07,
      "loss": 0.6318,
      "step": 19430
    },
    {
      "epoch": 6.05225,
      "grad_norm": 6.492717742919922,
      "learning_rate": 1.094002761376306e-07,
      "loss": 0.6154,
      "step": 19435
    },
    {
      "epoch": 6.0525,
      "grad_norm": 7.8449201583862305,
      "learning_rate": 1.074773264764517e-07,
      "loss": 0.6677,
      "step": 19440
    },
    {
      "epoch": 6.05275,
      "grad_norm": 6.41070556640625,
      "learning_rate": 1.0557139061979915e-07,
      "loss": 0.608,
      "step": 19445
    },
    {
      "epoch": 6.053,
      "grad_norm": 6.235457420349121,
      "learning_rate": 1.0368246987035868e-07,
      "loss": 0.6351,
      "step": 19450
    },
    {
      "epoch": 6.05325,
      "grad_norm": 6.963171005249023,
      "learning_rate": 1.0181056551919199e-07,
      "loss": 0.6456,
      "step": 19455
    },
    {
      "epoch": 6.0535,
      "grad_norm": 6.590118885040283,
      "learning_rate": 9.995567884572843e-08,
      "loss": 0.5783,
      "step": 19460
    },
    {
      "epoch": 6.05375,
      "grad_norm": 5.970996379852295,
      "learning_rate": 9.811781111776497e-08,
      "loss": 0.5832,
      "step": 19465
    },
    {
      "epoch": 6.054,
      "grad_norm": 8.377972602844238,
      "learning_rate": 9.629696359146622e-08,
      "loss": 0.6287,
      "step": 19470
    },
    {
      "epoch": 6.05425,
      "grad_norm": 8.277788162231445,
      "learning_rate": 9.449313751136724e-08,
      "loss": 0.6222,
      "step": 19475
    },
    {
      "epoch": 6.0545,
      "grad_norm": 8.049116134643555,
      "learning_rate": 9.270633411036234e-08,
      "loss": 0.7382,
      "step": 19480
    },
    {
      "epoch": 6.05475,
      "grad_norm": 6.132537364959717,
      "learning_rate": 9.093655460971628e-08,
      "loss": 0.6738,
      "step": 19485
    },
    {
      "epoch": 6.055,
      "grad_norm": 7.741829872131348,
      "learning_rate": 8.918380021905592e-08,
      "loss": 0.6458,
      "step": 19490
    },
    {
      "epoch": 6.05525,
      "grad_norm": 5.95512056350708,
      "learning_rate": 8.744807213637573e-08,
      "loss": 0.6534,
      "step": 19495
    },
    {
      "epoch": 6.0555,
      "grad_norm": 5.838165760040283,
      "learning_rate": 8.572937154802396e-08,
      "loss": 0.6191,
      "step": 19500
    },
    {
      "epoch": 6.0555,
      "eval_loss": 2.1195225715637207,
      "eval_runtime": 6.1646,
      "eval_samples_per_second": 166.109,
      "eval_steps_per_second": 20.764,
      "step": 19500
    },
    {
      "epoch": 6.05575,
      "grad_norm": 7.681279182434082,
      "learning_rate": 8.402769962872203e-08,
      "loss": 0.687,
      "step": 19505
    },
    {
      "epoch": 6.056,
      "grad_norm": 7.153112411499023,
      "learning_rate": 8.234305754154237e-08,
      "loss": 0.7293,
      "step": 19510
    },
    {
      "epoch": 6.05625,
      "grad_norm": 7.589371204376221,
      "learning_rate": 8.067544643792224e-08,
      "loss": 0.6731,
      "step": 19515
    },
    {
      "epoch": 6.0565,
      "grad_norm": 4.468410015106201,
      "learning_rate": 7.902486745766102e-08,
      "loss": 0.5849,
      "step": 19520
    },
    {
      "epoch": 6.05675,
      "grad_norm": 7.4651031494140625,
      "learning_rate": 7.739132172891184e-08,
      "loss": 0.6812,
      "step": 19525
    },
    {
      "epoch": 6.057,
      "grad_norm": 4.638514518737793,
      "learning_rate": 7.577481036818712e-08,
      "loss": 0.579,
      "step": 19530
    },
    {
      "epoch": 6.05725,
      "grad_norm": 3.775815725326538,
      "learning_rate": 7.417533448035863e-08,
      "loss": 0.644,
      "step": 19535
    },
    {
      "epoch": 6.0575,
      "grad_norm": 9.105448722839355,
      "learning_rate": 7.259289515865186e-08,
      "loss": 0.6981,
      "step": 19540
    },
    {
      "epoch": 6.05775,
      "grad_norm": 6.294280529022217,
      "learning_rate": 7.102749348465165e-08,
      "loss": 0.5675,
      "step": 19545
    },
    {
      "epoch": 6.058,
      "grad_norm": 5.2584991455078125,
      "learning_rate": 6.94791305282938e-08,
      "loss": 0.5635,
      "step": 19550
    },
    {
      "epoch": 6.05825,
      "grad_norm": 6.1161370277404785,
      "learning_rate": 6.794780734786788e-08,
      "loss": 0.5893,
      "step": 19555
    },
    {
      "epoch": 6.0585,
      "grad_norm": 5.253473281860352,
      "learning_rate": 6.643352499001998e-08,
      "loss": 0.6374,
      "step": 19560
    },
    {
      "epoch": 6.05875,
      "grad_norm": 5.460765361785889,
      "learning_rate": 6.493628448974998e-08,
      "loss": 0.6154,
      "step": 19565
    },
    {
      "epoch": 6.059,
      "grad_norm": 9.84659481048584,
      "learning_rate": 6.345608687040316e-08,
      "loss": 0.7517,
      "step": 19570
    },
    {
      "epoch": 6.05925,
      "grad_norm": 4.369110107421875,
      "learning_rate": 6.199293314368692e-08,
      "loss": 0.6502,
      "step": 19575
    },
    {
      "epoch": 6.0595,
      "grad_norm": 6.213047504425049,
      "learning_rate": 6.054682430964853e-08,
      "loss": 0.6884,
      "step": 19580
    },
    {
      "epoch": 6.05975,
      "grad_norm": 5.524839401245117,
      "learning_rate": 5.9117761356689e-08,
      "loss": 0.7372,
      "step": 19585
    },
    {
      "epoch": 6.06,
      "grad_norm": 6.762035846710205,
      "learning_rate": 5.770574526156314e-08,
      "loss": 0.7957,
      "step": 19590
    },
    {
      "epoch": 6.06025,
      "grad_norm": 7.326217174530029,
      "learning_rate": 5.631077698936837e-08,
      "loss": 0.6519,
      "step": 19595
    },
    {
      "epoch": 6.0605,
      "grad_norm": 8.928184509277344,
      "learning_rate": 5.493285749355037e-08,
      "loss": 0.7182,
      "step": 19600
    },
    {
      "epoch": 6.06075,
      "grad_norm": 6.660667896270752,
      "learning_rate": 5.3571987715908545e-08,
      "loss": 0.6503,
      "step": 19605
    },
    {
      "epoch": 6.061,
      "grad_norm": 6.999350547790527,
      "learning_rate": 5.222816858658219e-08,
      "loss": 0.6501,
      "step": 19610
    },
    {
      "epoch": 6.06125,
      "grad_norm": 8.239933967590332,
      "learning_rate": 5.0901401024058825e-08,
      "loss": 0.6038,
      "step": 19615
    },
    {
      "epoch": 6.0615,
      "grad_norm": 6.418981552124023,
      "learning_rate": 4.9591685935171385e-08,
      "loss": 0.6863,
      "step": 19620
    },
    {
      "epoch": 6.06175,
      "grad_norm": 5.281942367553711,
      "learning_rate": 4.829902421509824e-08,
      "loss": 0.6536,
      "step": 19625
    },
    {
      "epoch": 6.062,
      "grad_norm": 5.161301612854004,
      "learning_rate": 4.702341674736321e-08,
      "loss": 0.6577,
      "step": 19630
    },
    {
      "epoch": 6.06225,
      "grad_norm": 7.557443618774414,
      "learning_rate": 4.576486440382999e-08,
      "loss": 0.6548,
      "step": 19635
    },
    {
      "epoch": 6.0625,
      "grad_norm": 4.940941333770752,
      "learning_rate": 4.4523368044704916e-08,
      "loss": 0.5941,
      "step": 19640
    },
    {
      "epoch": 6.06275,
      "grad_norm": 6.629286766052246,
      "learning_rate": 4.329892851854533e-08,
      "loss": 0.6086,
      "step": 19645
    },
    {
      "epoch": 6.063,
      "grad_norm": 7.248476505279541,
      "learning_rate": 4.2091546662237356e-08,
      "loss": 0.6579,
      "step": 19650
    },
    {
      "epoch": 6.06325,
      "grad_norm": 7.884383678436279,
      "learning_rate": 4.0901223301020863e-08,
      "loss": 0.8217,
      "step": 19655
    },
    {
      "epoch": 6.0635,
      "grad_norm": 5.279299736022949,
      "learning_rate": 3.972795924846451e-08,
      "loss": 0.5918,
      "step": 19660
    },
    {
      "epoch": 6.06375,
      "grad_norm": 5.377590656280518,
      "learning_rate": 3.857175530649071e-08,
      "loss": 0.6992,
      "step": 19665
    },
    {
      "epoch": 6.064,
      "grad_norm": 7.544283390045166,
      "learning_rate": 3.743261226534789e-08,
      "loss": 0.6603,
      "step": 19670
    },
    {
      "epoch": 6.06425,
      "grad_norm": 6.547097206115723,
      "learning_rate": 3.631053090363268e-08,
      "loss": 0.6627,
      "step": 19675
    },
    {
      "epoch": 6.0645,
      "grad_norm": 6.375401496887207,
      "learning_rate": 3.520551198827604e-08,
      "loss": 0.6129,
      "step": 19680
    },
    {
      "epoch": 6.06475,
      "grad_norm": 6.24376106262207,
      "learning_rate": 3.411755627454605e-08,
      "loss": 0.6711,
      "step": 19685
    },
    {
      "epoch": 6.065,
      "grad_norm": 6.814030647277832,
      "learning_rate": 3.3046664506053425e-08,
      "loss": 0.7272,
      "step": 19690
    },
    {
      "epoch": 6.06525,
      "grad_norm": 7.440474987030029,
      "learning_rate": 3.1992837414743236e-08,
      "loss": 0.722,
      "step": 19695
    },
    {
      "epoch": 6.0655,
      "grad_norm": 5.664860248565674,
      "learning_rate": 3.0956075720892096e-08,
      "loss": 0.6268,
      "step": 19700
    },
    {
      "epoch": 6.06575,
      "grad_norm": 6.794972896575928,
      "learning_rate": 2.993638013311928e-08,
      "loss": 0.6074,
      "step": 19705
    },
    {
      "epoch": 6.066,
      "grad_norm": 6.59063196182251,
      "learning_rate": 2.893375134837839e-08,
      "loss": 0.705,
      "step": 19710
    },
    {
      "epoch": 6.06625,
      "grad_norm": 12.477669715881348,
      "learning_rate": 2.7948190051954593e-08,
      "loss": 0.8149,
      "step": 19715
    },
    {
      "epoch": 6.0665,
      "grad_norm": 6.861174583435059,
      "learning_rate": 2.6979696917470156e-08,
      "loss": 0.597,
      "step": 19720
    },
    {
      "epoch": 6.06675,
      "grad_norm": 7.083471775054932,
      "learning_rate": 2.6028272606881677e-08,
      "loss": 0.5961,
      "step": 19725
    },
    {
      "epoch": 6.067,
      "grad_norm": 4.932433605194092,
      "learning_rate": 2.5093917770477315e-08,
      "loss": 0.6167,
      "step": 19730
    },
    {
      "epoch": 6.06725,
      "grad_norm": 4.536001682281494,
      "learning_rate": 2.4176633046882335e-08,
      "loss": 0.5526,
      "step": 19735
    },
    {
      "epoch": 6.0675,
      "grad_norm": 5.260538101196289,
      "learning_rate": 2.327641906305078e-08,
      "loss": 0.5589,
      "step": 19740
    },
    {
      "epoch": 6.06775,
      "grad_norm": 5.620445728302002,
      "learning_rate": 2.2393276434268253e-08,
      "loss": 0.5489,
      "step": 19745
    },
    {
      "epoch": 6.068,
      "grad_norm": 4.915350437164307,
      "learning_rate": 2.1527205764157454e-08,
      "loss": 0.7406,
      "step": 19750
    },
    {
      "epoch": 6.06825,
      "grad_norm": 6.896722316741943,
      "learning_rate": 2.067820764466988e-08,
      "loss": 0.6367,
      "step": 19755
    },
    {
      "epoch": 6.0685,
      "grad_norm": 7.698459148406982,
      "learning_rate": 1.9846282656085792e-08,
      "loss": 0.6436,
      "step": 19760
    },
    {
      "epoch": 6.06875,
      "grad_norm": 6.090261459350586,
      "learning_rate": 1.903143136701979e-08,
      "loss": 0.6101,
      "step": 19765
    },
    {
      "epoch": 6.069,
      "grad_norm": 6.487439155578613,
      "learning_rate": 1.8233654334412487e-08,
      "loss": 0.612,
      "step": 19770
    },
    {
      "epoch": 6.06925,
      "grad_norm": 8.306944847106934,
      "learning_rate": 1.745295210353881e-08,
      "loss": 0.6113,
      "step": 19775
    },
    {
      "epoch": 6.0695,
      "grad_norm": 7.391904830932617,
      "learning_rate": 1.6689325208002482e-08,
      "loss": 0.6507,
      "step": 19780
    },
    {
      "epoch": 6.06975,
      "grad_norm": 7.824214458465576,
      "learning_rate": 1.594277416973322e-08,
      "loss": 0.6476,
      "step": 19785
    },
    {
      "epoch": 6.07,
      "grad_norm": 9.249482154846191,
      "learning_rate": 1.521329949898953e-08,
      "loss": 0.6937,
      "step": 19790
    },
    {
      "epoch": 6.07025,
      "grad_norm": 7.1453986167907715,
      "learning_rate": 1.450090169436702e-08,
      "loss": 0.6197,
      "step": 19795
    },
    {
      "epoch": 6.0705,
      "grad_norm": 5.9648871421813965,
      "learning_rate": 1.38055812427762e-08,
      "loss": 0.6779,
      "step": 19800
    },
    {
      "epoch": 6.07075,
      "grad_norm": 8.106938362121582,
      "learning_rate": 1.3127338619467467e-08,
      "loss": 0.5977,
      "step": 19805
    },
    {
      "epoch": 6.071,
      "grad_norm": 8.937257766723633,
      "learning_rate": 1.2466174288008892e-08,
      "loss": 0.6075,
      "step": 19810
    },
    {
      "epoch": 6.07125,
      "grad_norm": 6.333935260772705,
      "learning_rate": 1.1822088700300105e-08,
      "loss": 0.5581,
      "step": 19815
    },
    {
      "epoch": 6.0715,
      "grad_norm": 7.301527500152588,
      "learning_rate": 1.119508229657229e-08,
      "loss": 0.5675,
      "step": 19820
    },
    {
      "epoch": 6.07175,
      "grad_norm": 8.37347412109375,
      "learning_rate": 1.0585155505374312e-08,
      "loss": 0.7542,
      "step": 19825
    },
    {
      "epoch": 6.072,
      "grad_norm": 6.956866264343262,
      "learning_rate": 9.992308743586587e-09,
      "loss": 0.6305,
      "step": 19830
    },
    {
      "epoch": 6.07225,
      "grad_norm": 8.552780151367188,
      "learning_rate": 9.416542416412766e-09,
      "loss": 0.597,
      "step": 19835
    },
    {
      "epoch": 6.0725,
      "grad_norm": 8.055275917053223,
      "learning_rate": 8.857856917388053e-09,
      "loss": 0.5228,
      "step": 19840
    },
    {
      "epoch": 6.07275,
      "grad_norm": 6.851837158203125,
      "learning_rate": 8.316252628362553e-09,
      "loss": 0.6462,
      "step": 19845
    },
    {
      "epoch": 6.073,
      "grad_norm": 7.740211486816406,
      "learning_rate": 7.791729919523482e-09,
      "loss": 0.5835,
      "step": 19850
    },
    {
      "epoch": 6.07325,
      "grad_norm": 6.686211109161377,
      "learning_rate": 7.284289149372958e-09,
      "loss": 0.5342,
      "step": 19855
    },
    {
      "epoch": 6.0735,
      "grad_norm": 5.371942520141602,
      "learning_rate": 6.793930664744652e-09,
      "loss": 0.5601,
      "step": 19860
    },
    {
      "epoch": 6.07375,
      "grad_norm": 6.340028762817383,
      "learning_rate": 6.320654800792691e-09,
      "loss": 0.6417,
      "step": 19865
    },
    {
      "epoch": 6.074,
      "grad_norm": 7.852954864501953,
      "learning_rate": 5.8644618809972075e-09,
      "loss": 0.6006,
      "step": 19870
    },
    {
      "epoch": 6.07425,
      "grad_norm": 9.311531066894531,
      "learning_rate": 5.425352217158786e-09,
      "loss": 0.585,
      "step": 19875
    },
    {
      "epoch": 6.0745,
      "grad_norm": 8.087472915649414,
      "learning_rate": 5.003326109409567e-09,
      "loss": 0.6727,
      "step": 19880
    },
    {
      "epoch": 6.07475,
      "grad_norm": 6.593381881713867,
      "learning_rate": 4.598383846199372e-09,
      "loss": 0.5285,
      "step": 19885
    },
    {
      "epoch": 6.075,
      "grad_norm": 7.013921737670898,
      "learning_rate": 4.210525704298474e-09,
      "loss": 0.5342,
      "step": 19890
    },
    {
      "epoch": 6.07525,
      "grad_norm": 9.569089889526367,
      "learning_rate": 3.8397519488087006e-09,
      "loss": 0.5932,
      "step": 19895
    },
    {
      "epoch": 6.0755,
      "grad_norm": 8.356281280517578,
      "learning_rate": 3.4860628331495616e-09,
      "loss": 0.6221,
      "step": 19900
    },
    {
      "epoch": 6.07575,
      "grad_norm": 7.658703327178955,
      "learning_rate": 3.1494585990637924e-09,
      "loss": 0.5608,
      "step": 19905
    },
    {
      "epoch": 6.076,
      "grad_norm": 8.0249662399292,
      "learning_rate": 2.8299394766145847e-09,
      "loss": 0.6777,
      "step": 19910
    },
    {
      "epoch": 6.07625,
      "grad_norm": 8.66127872467041,
      "learning_rate": 2.5275056841939094e-09,
      "loss": 0.6492,
      "step": 19915
    },
    {
      "epoch": 6.0765,
      "grad_norm": 8.72442626953125,
      "learning_rate": 2.24215742850864e-09,
      "loss": 0.614,
      "step": 19920
    },
    {
      "epoch": 6.07675,
      "grad_norm": 7.05318021774292,
      "learning_rate": 1.973894904597207e-09,
      "loss": 0.6266,
      "step": 19925
    },
    {
      "epoch": 6.077,
      "grad_norm": 7.04749059677124,
      "learning_rate": 1.7227182958101661e-09,
      "loss": 0.5704,
      "step": 19930
    },
    {
      "epoch": 6.07725,
      "grad_norm": 6.5638747215271,
      "learning_rate": 1.4886277738240806e-09,
      "loss": 0.5959,
      "step": 19935
    },
    {
      "epoch": 6.0775,
      "grad_norm": 9.437196731567383,
      "learning_rate": 1.2716234986415166e-09,
      "loss": 0.6834,
      "step": 19940
    },
    {
      "epoch": 6.07775,
      "grad_norm": 7.5473809242248535,
      "learning_rate": 1.0717056185799434e-09,
      "loss": 0.6378,
      "step": 19945
    },
    {
      "epoch": 6.078,
      "grad_norm": 9.150385856628418,
      "learning_rate": 8.888742702800601e-10,
      "loss": 0.6808,
      "step": 19950
    },
    {
      "epoch": 6.07825,
      "grad_norm": 8.708932876586914,
      "learning_rate": 7.231295787085701e-10,
      "loss": 0.635,
      "step": 19955
    },
    {
      "epoch": 6.0785,
      "grad_norm": 9.4970703125,
      "learning_rate": 5.744716571470798e-10,
      "loss": 0.5707,
      "step": 19960
    },
    {
      "epoch": 6.07875,
      "grad_norm": 8.908957481384277,
      "learning_rate": 4.42900607205976e-10,
      "loss": 0.619,
      "step": 19965
    },
    {
      "epoch": 6.079,
      "grad_norm": 8.865569114685059,
      "learning_rate": 3.2841651881054814e-10,
      "loss": 0.6022,
      "step": 19970
    },
    {
      "epoch": 6.07925,
      "grad_norm": 8.600667953491211,
      "learning_rate": 2.310194702093149e-10,
      "loss": 0.6903,
      "step": 19975
    },
    {
      "epoch": 6.0795,
      "grad_norm": 9.179512977600098,
      "learning_rate": 1.5070952797124894e-10,
      "loss": 0.581,
      "step": 19980
    },
    {
      "epoch": 6.07975,
      "grad_norm": 8.883241653442383,
      "learning_rate": 8.748674698855208e-11,
      "loss": 0.6237,
      "step": 19985
    },
    {
      "epoch": 6.08,
      "grad_norm": 6.368789196014404,
      "learning_rate": 4.135117047388004e-11,
      "loss": 0.634,
      "step": 19990
    },
    {
      "epoch": 6.08025,
      "grad_norm": 7.999098777770996,
      "learning_rate": 1.2302829963117824e-11,
      "loss": 0.6371,
      "step": 19995
    },
    {
      "epoch": 6.0805,
      "grad_norm": 10.007863998413086,
      "learning_rate": 3.4174530427755203e-13,
      "loss": 0.7083,
      "step": 20000
    },
    {
      "epoch": 6.0805,
      "eval_loss": 2.117126941680908,
      "eval_runtime": 5.214,
      "eval_samples_per_second": 196.393,
      "eval_steps_per_second": 24.549,
      "step": 20000
    }
  ],
  "logging_steps": 5,
  "max_steps": 20000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 9223372036854775807,
  "save_steps": 2500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.5425222666305536e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
