{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'narrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     12\u001b[0m     AutoTokenizer,\n\u001b[1;32m     13\u001b[0m     AutoConfig,\n\u001b[1;32m     14\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_pruned_to_variable_size\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'narrow'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# os.environ['HF_HOME'] = os.environ.get('SCRATCH') + '/iaifi_lab/Lab/ericjm/.cache/huggingface'\n",
    "os.environ['HF_HOME'] = '/om/user/ericjm/.cache/huggingface'\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "from narrow.modeling import convert_pruned_to_variable_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCRATCH = os.environ.get('SCRATCH')\n",
    "model_dir = os.path.join(SCRATCH, 'iaifi_lab/Lab/ericjm/narrow/pruneandtrain01/n0.80_r0.80/checkpoint-20000')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_model = convert_pruned_to_variable_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input object\n",
    "input_obj = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "\n",
    "# run each model on the input object\n",
    "baseline_output = model(input_obj[\"input_ids\"])\n",
    "pruned_output   = pruned_model(input_obj[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_obj.input_ids[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.3774,  8.1457, 14.4361,  ..., -4.6480, -4.6479, -4.6481],\n",
       "         [ 9.4960,  3.8903,  4.7837,  ..., -7.2911, -7.2912, -7.2913],\n",
       "         [ 5.6897,  1.6978,  2.1098,  ..., -6.6646, -6.6646, -6.6647],\n",
       "         ...,\n",
       "         [ 5.7636,  2.8113,  1.0498,  ..., -6.6710, -6.6713, -6.6708],\n",
       "         [ 8.9889,  3.9790,  1.7644,  ..., -7.4253, -7.4253, -7.4250],\n",
       "         [ 8.3706,  3.4622,  5.3251,  ..., -7.1916, -7.1918, -7.1914]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.6760,  3.3142,  6.1423,  ..., -1.9703, -1.9702, -1.9703],\n",
       "         [ 5.4280,  3.8526,  3.9846,  ..., -4.8080, -4.8081, -4.8082],\n",
       "         [ 5.1023,  3.6254,  3.5077,  ..., -4.6185, -4.6185, -4.6186],\n",
       "         ...,\n",
       "         [ 5.2408,  2.5246,  2.5794,  ..., -5.2450, -5.2451, -5.2452],\n",
       "         [ 5.2377,  2.4547,  2.4712,  ..., -5.3923, -5.3925, -5.3925],\n",
       "         [ 5.2791,  1.9092,  2.3428,  ..., -4.9808, -4.9809, -4.9809]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0173,  0.0277,  ..., -0.0000, -0.0429, -0.0000],\n",
       "        [ 0.0000, -0.0265,  0.0265,  ..., -0.0000,  0.0026, -0.0000],\n",
       "        [ 0.0000,  0.0209,  0.0192,  ...,  0.0000, -0.0059,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0220, -0.0299,  ..., -0.0000, -0.0031,  0.0000],\n",
       "        [ 0.0000,  0.0220, -0.0299,  ..., -0.0000, -0.0031,  0.0000],\n",
       "        [ 0.0000,  0.0221, -0.0299,  ..., -0.0000, -0.0031,  0.0000]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import NNsight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel = NNsight(model)\n",
    "nnpruned_model = NNsight(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nnmodel.trace(input_obj[\"input_ids\"]):\n",
    "    emb_out = nnmodel.model.embed_tokens.output.save()\n",
    "with nnpruned_model.trace(input_obj[\"input_ids\"]):\n",
    "    emb_out_pruned = nnpruned_model.model.embed_tokens.output.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0030, -0.0042,  ..., -0.0000,  0.0012,  0.0000],\n",
       "         [ 0.0000,  0.0043,  0.0247,  ...,  0.0000,  0.0334, -0.0000],\n",
       "         [ 0.0000, -0.0326,  0.0655,  ..., -0.0000, -0.0248, -0.0000],\n",
       "         ...,\n",
       "         [-0.0000,  0.0339,  0.0362,  ..., -0.0000, -0.0167,  0.0000],\n",
       "         [-0.0000,  0.0451,  0.0511,  ...,  0.0000, -0.0358, -0.0000],\n",
       "         [-0.0000, -0.0218,  0.0227,  ..., -0.0000, -0.0221,  0.0000]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0030, -0.0042,  0.0063,  ...,  0.0006,  0.0019,  0.0012],\n",
       "         [ 0.0043,  0.0247,  0.0219,  ...,  0.0063,  0.0057,  0.0334],\n",
       "         [-0.0326,  0.0655, -0.0131,  ...,  0.0363,  0.0078, -0.0248],\n",
       "         ...,\n",
       "         [ 0.0339,  0.0362,  0.0060,  ..., -0.0021, -0.0072, -0.0167],\n",
       "         [ 0.0451,  0.0511, -0.0138,  ..., -0.0082,  0.0031, -0.0358],\n",
       "         [-0.0218,  0.0227,  0.0207,  ...,  0.0183,  0.0318, -0.0221]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_out_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 2048])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.layers[0].mlp.gate_proj.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from narrow.modeling import convert_pruned_to_variable_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Layer 1 has different pruned residual stream dimensions than layer 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_pruned_to_variable_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/narrow/narrow/modeling.py:182\u001b[0m, in \u001b[0;36mconvert_pruned_to_variable_size\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# check that all layers have the same pruned residual stream dimensions\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layeri \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[layeri]\u001b[38;5;241m.\u001b[39mmlp\u001b[38;5;241m.\u001b[39mgate_proj\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnonzero()[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;241m==\u001b[39m pruned_residual_stream_dimensions, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(layeri) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m has different pruned residual stream dimensions than layer 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# get intermediate sizes\u001b[39;00m\n\u001b[1;32m    185\u001b[0m intermediate_sizes \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAssertionError\u001b[0m: Layer 1 has different pruned residual stream dimensions than layer 0"
     ]
    }
   ],
   "source": [
    "new_model = convert_pruned_to_variable_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,  ..., 8177, 8179, 8186])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.layers[0].mlp.gate_proj.weight.sum(dim=1).abs() == 0).nonzero()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,  ..., 8177, 8179, 8186])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.layers[0].mlp.up_proj.weight.sum(dim=1).abs() == 0).nonzero()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,    2,    3,  ..., 8177, 8179, 8186])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.layers[0].mlp.down_proj.weight.sum(dim=0).abs() == 0).nonzero()[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 8,\n",
       " 13,\n",
       " 15,\n",
       " 17,\n",
       " 18,\n",
       " 24,\n",
       " 27,\n",
       " 29,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 40,\n",
       " 44,\n",
       " 49,\n",
       " 50,\n",
       " 52,\n",
       " 54,\n",
       " 55,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 63,\n",
       " 72,\n",
       " 80,\n",
       " 81,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 88,\n",
       " 91,\n",
       " 94,\n",
       " 96,\n",
       " 100,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 109,\n",
       " 111,\n",
       " 112,\n",
       " 115,\n",
       " 117,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 126,\n",
       " 129,\n",
       " 130,\n",
       " 134,\n",
       " 136,\n",
       " 140,\n",
       " 142,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 150,\n",
       " 153,\n",
       " 156,\n",
       " 158,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 171,\n",
       " 180,\n",
       " 183,\n",
       " 189,\n",
       " 192,\n",
       " 194,\n",
       " 195,\n",
       " 198,\n",
       " 202,\n",
       " 207,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 213,\n",
       " 216,\n",
       " 218,\n",
       " 219,\n",
       " 224,\n",
       " 235,\n",
       " 239,\n",
       " 240,\n",
       " 251,\n",
       " 256,\n",
       " 264,\n",
       " 265,\n",
       " 267,\n",
       " 269,\n",
       " 275,\n",
       " 280,\n",
       " 281,\n",
       " 288,\n",
       " 294,\n",
       " 298,\n",
       " 299,\n",
       " 308,\n",
       " 309,\n",
       " 313,\n",
       " 315,\n",
       " 317,\n",
       " 318,\n",
       " 320,\n",
       " 321,\n",
       " 324,\n",
       " 326,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 334,\n",
       " 336,\n",
       " 341,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 354,\n",
       " 356,\n",
       " 357,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 368,\n",
       " 371,\n",
       " 373,\n",
       " 382,\n",
       " 384,\n",
       " 390,\n",
       " 394,\n",
       " 400,\n",
       " 405,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 411,\n",
       " 415,\n",
       " 417,\n",
       " 419,\n",
       " 422,\n",
       " 423,\n",
       " 427,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 440,\n",
       " 444,\n",
       " 445,\n",
       " 450,\n",
       " 455,\n",
       " 456,\n",
       " 458,\n",
       " 460,\n",
       " 463,\n",
       " 465,\n",
       " 467,\n",
       " 472,\n",
       " 473,\n",
       " 475,\n",
       " 480,\n",
       " 485,\n",
       " 487,\n",
       " 491,\n",
       " 492,\n",
       " 493,\n",
       " 494,\n",
       " 497,\n",
       " 498,\n",
       " 499,\n",
       " 500,\n",
       " 501,\n",
       " 503,\n",
       " 507,\n",
       " 509,\n",
       " 510,\n",
       " 512,\n",
       " 516,\n",
       " 520,\n",
       " 521,\n",
       " 527,\n",
       " 530,\n",
       " 532,\n",
       " 536,\n",
       " 537,\n",
       " 540,\n",
       " 541,\n",
       " 542,\n",
       " 544,\n",
       " 545,\n",
       " 546,\n",
       " 548,\n",
       " 553,\n",
       " 554,\n",
       " 556,\n",
       " 559,\n",
       " 561,\n",
       " 563,\n",
       " 568,\n",
       " 569,\n",
       " 570,\n",
       " 579,\n",
       " 582,\n",
       " 583,\n",
       " 588,\n",
       " 591,\n",
       " 595,\n",
       " 596,\n",
       " 600,\n",
       " 603,\n",
       " 605,\n",
       " 607,\n",
       " 615,\n",
       " 616,\n",
       " 622,\n",
       " 623,\n",
       " 626,\n",
       " 627,\n",
       " 629,\n",
       " 630,\n",
       " 631,\n",
       " 632,\n",
       " 636,\n",
       " 643,\n",
       " 644,\n",
       " 646,\n",
       " 647,\n",
       " 651,\n",
       " 655,\n",
       " 662,\n",
       " 664,\n",
       " 669,\n",
       " 670,\n",
       " 672,\n",
       " 673,\n",
       " 678,\n",
       " 680,\n",
       " 684,\n",
       " 687,\n",
       " 689,\n",
       " 693,\n",
       " 700,\n",
       " 704,\n",
       " 709,\n",
       " 711,\n",
       " 716,\n",
       " 722,\n",
       " 724,\n",
       " 729,\n",
       " 730,\n",
       " 731,\n",
       " 738,\n",
       " 741,\n",
       " 742,\n",
       " 752,\n",
       " 755,\n",
       " 759,\n",
       " 761,\n",
       " 764,\n",
       " 770,\n",
       " 773,\n",
       " 774,\n",
       " 775,\n",
       " 776,\n",
       " 777,\n",
       " 778,\n",
       " 779,\n",
       " 782,\n",
       " 787,\n",
       " 790,\n",
       " 793,\n",
       " 796,\n",
       " 797,\n",
       " 803,\n",
       " 805,\n",
       " 806,\n",
       " 807,\n",
       " 810,\n",
       " 813,\n",
       " 814,\n",
       " 815,\n",
       " 817,\n",
       " 820,\n",
       " 821,\n",
       " 822,\n",
       " 823,\n",
       " 825,\n",
       " 832,\n",
       " 838,\n",
       " 842,\n",
       " 843,\n",
       " 844,\n",
       " 849,\n",
       " 850,\n",
       " 852,\n",
       " 854,\n",
       " 857,\n",
       " 858,\n",
       " 862,\n",
       " 863,\n",
       " 870,\n",
       " 873,\n",
       " 882,\n",
       " 886,\n",
       " 892,\n",
       " 894,\n",
       " 896,\n",
       " 902,\n",
       " 903,\n",
       " 904,\n",
       " 906,\n",
       " 909,\n",
       " 910,\n",
       " 911,\n",
       " 914,\n",
       " 915,\n",
       " 919,\n",
       " 921,\n",
       " 924,\n",
       " 925,\n",
       " 926,\n",
       " 928,\n",
       " 933,\n",
       " 936,\n",
       " 940,\n",
       " 941,\n",
       " 945,\n",
       " 949,\n",
       " 951,\n",
       " 953,\n",
       " 955,\n",
       " 960,\n",
       " 963,\n",
       " 965,\n",
       " 969,\n",
       " 973,\n",
       " 974,\n",
       " 977,\n",
       " 985,\n",
       " 991,\n",
       " 992,\n",
       " 994,\n",
       " 995,\n",
       " 998,\n",
       " 1001,\n",
       " 1010,\n",
       " 1011,\n",
       " 1013,\n",
       " 1015,\n",
       " 1017,\n",
       " 1018,\n",
       " 1019,\n",
       " 1022,\n",
       " 1026,\n",
       " 1028,\n",
       " 1031,\n",
       " 1037,\n",
       " 1042,\n",
       " 1043,\n",
       " 1044,\n",
       " 1045,\n",
       " 1047,\n",
       " 1049,\n",
       " 1050,\n",
       " 1055,\n",
       " 1062,\n",
       " 1068,\n",
       " 1072,\n",
       " 1073,\n",
       " 1074,\n",
       " 1075,\n",
       " 1076,\n",
       " 1078,\n",
       " 1080,\n",
       " 1081,\n",
       " 1085,\n",
       " 1088,\n",
       " 1089,\n",
       " 1090,\n",
       " 1092,\n",
       " 1093,\n",
       " 1094,\n",
       " 1097,\n",
       " 1101,\n",
       " 1104,\n",
       " 1108,\n",
       " 1117,\n",
       " 1121,\n",
       " 1124,\n",
       " 1127,\n",
       " 1128,\n",
       " 1129,\n",
       " 1131,\n",
       " 1132,\n",
       " 1133,\n",
       " 1137,\n",
       " 1141,\n",
       " 1143,\n",
       " 1144,\n",
       " 1145,\n",
       " 1146,\n",
       " 1147,\n",
       " 1148,\n",
       " 1150,\n",
       " 1152,\n",
       " 1159,\n",
       " 1164,\n",
       " 1168,\n",
       " 1169,\n",
       " 1170,\n",
       " 1172,\n",
       " 1173,\n",
       " 1175,\n",
       " 1176,\n",
       " 1177,\n",
       " 1184,\n",
       " 1186,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1197,\n",
       " 1209,\n",
       " 1214,\n",
       " 1216,\n",
       " 1217,\n",
       " 1221,\n",
       " 1222,\n",
       " 1223,\n",
       " 1230,\n",
       " 1231,\n",
       " 1232,\n",
       " 1236,\n",
       " 1239,\n",
       " 1241,\n",
       " 1248,\n",
       " 1249,\n",
       " 1250,\n",
       " 1251,\n",
       " 1253,\n",
       " 1255,\n",
       " 1259,\n",
       " 1261,\n",
       " 1262,\n",
       " 1270,\n",
       " 1272,\n",
       " 1276,\n",
       " 1279,\n",
       " 1280,\n",
       " 1281,\n",
       " 1284,\n",
       " 1285,\n",
       " 1286,\n",
       " 1287,\n",
       " 1290,\n",
       " 1294,\n",
       " 1302,\n",
       " 1303,\n",
       " 1306,\n",
       " 1312,\n",
       " 1320,\n",
       " 1321,\n",
       " 1323,\n",
       " 1324,\n",
       " 1327,\n",
       " 1333,\n",
       " 1336,\n",
       " 1344,\n",
       " 1345,\n",
       " 1346,\n",
       " 1349,\n",
       " 1350,\n",
       " 1356,\n",
       " 1357,\n",
       " 1358,\n",
       " 1362,\n",
       " 1363,\n",
       " 1373,\n",
       " 1374,\n",
       " 1375,\n",
       " 1377,\n",
       " 1378,\n",
       " 1379,\n",
       " 1381,\n",
       " 1384,\n",
       " 1387,\n",
       " 1388,\n",
       " 1389,\n",
       " 1399,\n",
       " 1400,\n",
       " 1401,\n",
       " 1402,\n",
       " 1404,\n",
       " 1407,\n",
       " 1413,\n",
       " 1415,\n",
       " 1422,\n",
       " 1427,\n",
       " 1428,\n",
       " 1429,\n",
       " 1430,\n",
       " 1437,\n",
       " 1447,\n",
       " 1448,\n",
       " 1449,\n",
       " 1453,\n",
       " 1455,\n",
       " 1461,\n",
       " 1462,\n",
       " 1463,\n",
       " 1472,\n",
       " 1474,\n",
       " 1476,\n",
       " 1482,\n",
       " 1483,\n",
       " 1486,\n",
       " 1487,\n",
       " 1490,\n",
       " 1492,\n",
       " 1493,\n",
       " 1496,\n",
       " 1497,\n",
       " 1498,\n",
       " 1499,\n",
       " 1500,\n",
       " 1504,\n",
       " 1506,\n",
       " 1507,\n",
       " 1508,\n",
       " 1515,\n",
       " 1516,\n",
       " 1521,\n",
       " 1525,\n",
       " 1528,\n",
       " 1531,\n",
       " 1535,\n",
       " 1538,\n",
       " 1539,\n",
       " 1541,\n",
       " 1542,\n",
       " 1543,\n",
       " 1544,\n",
       " 1545,\n",
       " 1546,\n",
       " 1548,\n",
       " 1554,\n",
       " 1558,\n",
       " 1561,\n",
       " 1563,\n",
       " 1564,\n",
       " 1566,\n",
       " 1570,\n",
       " 1571,\n",
       " 1573,\n",
       " 1574,\n",
       " 1576,\n",
       " 1580,\n",
       " 1589,\n",
       " 1593,\n",
       " 1596,\n",
       " 1597,\n",
       " 1600,\n",
       " 1605,\n",
       " 1608,\n",
       " 1610,\n",
       " 1615,\n",
       " 1623,\n",
       " 1625,\n",
       " 1627,\n",
       " 1633,\n",
       " 1635,\n",
       " 1640,\n",
       " 1641,\n",
       " 1645,\n",
       " 1649,\n",
       " 1650,\n",
       " 1662,\n",
       " 1663,\n",
       " 1664,\n",
       " 1676,\n",
       " 1677,\n",
       " 1680,\n",
       " 1693,\n",
       " 1695,\n",
       " 1699,\n",
       " 1700,\n",
       " 1701,\n",
       " 1702,\n",
       " 1704,\n",
       " 1705,\n",
       " 1714,\n",
       " 1715,\n",
       " 1717,\n",
       " 1720,\n",
       " 1723,\n",
       " 1724,\n",
       " 1726,\n",
       " 1727,\n",
       " 1729,\n",
       " 1730,\n",
       " 1732,\n",
       " 1733,\n",
       " 1735,\n",
       " 1736,\n",
       " 1740,\n",
       " 1741,\n",
       " 1742,\n",
       " 1743,\n",
       " 1750,\n",
       " 1754,\n",
       " 1759,\n",
       " 1760,\n",
       " 1764,\n",
       " 1765,\n",
       " 1766,\n",
       " 1767,\n",
       " 1770,\n",
       " 1771,\n",
       " 1773,\n",
       " 1775,\n",
       " 1776,\n",
       " 1778,\n",
       " 1779,\n",
       " 1781,\n",
       " 1783,\n",
       " 1784,\n",
       " 1785,\n",
       " 1786,\n",
       " 1790,\n",
       " 1792,\n",
       " 1795,\n",
       " 1799,\n",
       " 1800,\n",
       " 1808,\n",
       " 1809,\n",
       " 1810,\n",
       " 1814,\n",
       " 1818,\n",
       " 1824,\n",
       " 1825,\n",
       " 1829,\n",
       " 1830,\n",
       " 1831,\n",
       " 1833,\n",
       " 1834,\n",
       " 1836,\n",
       " 1837,\n",
       " 1839,\n",
       " 1842,\n",
       " 1844,\n",
       " 1846,\n",
       " 1847,\n",
       " 1849,\n",
       " 1857,\n",
       " 1858,\n",
       " 1860,\n",
       " 1862,\n",
       " 1863,\n",
       " 1864,\n",
       " 1871,\n",
       " 1876,\n",
       " 1877,\n",
       " 1883,\n",
       " 1887,\n",
       " 1891,\n",
       " 1896,\n",
       " 1900,\n",
       " 1905,\n",
       " 1908,\n",
       " 1909,\n",
       " 1918,\n",
       " 1919,\n",
       " 1922,\n",
       " 1923,\n",
       " 1925,\n",
       " 1927,\n",
       " 1930,\n",
       " 1939,\n",
       " 1941,\n",
       " 1944,\n",
       " 1945,\n",
       " 1948,\n",
       " 1951,\n",
       " 1955,\n",
       " 1957,\n",
       " 1958,\n",
       " 1959,\n",
       " 1960,\n",
       " 1961,\n",
       " 1967,\n",
       " 1969,\n",
       " 1973,\n",
       " 1975,\n",
       " 1978,\n",
       " 1982,\n",
       " 1984,\n",
       " 1986,\n",
       " 1988,\n",
       " 1990,\n",
       " 1993,\n",
       " 1994,\n",
       " 2001,\n",
       " 2004,\n",
       " 2008,\n",
       " 2010,\n",
       " 2011,\n",
       " 2017,\n",
       " 2018,\n",
       " 2019,\n",
       " 2020,\n",
       " 2027,\n",
       " 2031,\n",
       " 2033,\n",
       " 2035,\n",
       " 2040,\n",
       " 2043,\n",
       " 2044,\n",
       " 2046,\n",
       " 2047,\n",
       " 2049,\n",
       " 2051,\n",
       " 2055,\n",
       " 2057,\n",
       " 2060,\n",
       " 2062,\n",
       " 2066,\n",
       " 2072,\n",
       " 2073,\n",
       " 2075,\n",
       " 2082,\n",
       " 2083,\n",
       " 2087,\n",
       " 2088,\n",
       " 2089,\n",
       " 2090,\n",
       " 2092,\n",
       " 2097,\n",
       " 2098,\n",
       " 2099,\n",
       " 2100,\n",
       " 2107,\n",
       " 2109,\n",
       " 2112,\n",
       " 2115,\n",
       " 2116,\n",
       " 2118,\n",
       " 2121,\n",
       " 2122,\n",
       " 2125,\n",
       " 2126,\n",
       " 2131,\n",
       " 2132,\n",
       " 2135,\n",
       " 2137,\n",
       " 2139,\n",
       " 2144,\n",
       " 2149,\n",
       " 2150,\n",
       " 2156,\n",
       " 2159,\n",
       " 2162,\n",
       " 2163,\n",
       " 2164,\n",
       " 2167,\n",
       " 2168,\n",
       " 2170,\n",
       " 2171,\n",
       " 2172,\n",
       " 2173,\n",
       " 2175,\n",
       " 2177,\n",
       " 2179,\n",
       " 2186,\n",
       " 2187,\n",
       " 2188,\n",
       " 2193,\n",
       " 2194,\n",
       " 2195,\n",
       " 2196,\n",
       " 2199,\n",
       " 2206,\n",
       " 2207,\n",
       " 2219,\n",
       " 2221,\n",
       " 2224,\n",
       " 2226,\n",
       " 2228,\n",
       " 2231,\n",
       " 2241,\n",
       " 2247,\n",
       " 2253,\n",
       " 2254,\n",
       " 2255,\n",
       " 2259,\n",
       " 2260,\n",
       " 2264,\n",
       " 2268,\n",
       " 2269,\n",
       " 2270,\n",
       " 2272,\n",
       " 2275,\n",
       " 2277,\n",
       " 2278,\n",
       " 2279,\n",
       " 2281,\n",
       " 2282,\n",
       " 2284,\n",
       " 2289,\n",
       " 2292,\n",
       " 2293,\n",
       " 2294,\n",
       " 2299,\n",
       " 2300,\n",
       " 2302,\n",
       " 2306,\n",
       " 2307,\n",
       " 2308,\n",
       " 2311,\n",
       " 2315,\n",
       " 2316,\n",
       " 2321,\n",
       " 2334,\n",
       " 2335,\n",
       " 2336,\n",
       " 2337,\n",
       " 2339,\n",
       " 2342,\n",
       " 2343,\n",
       " 2344,\n",
       " 2347,\n",
       " 2352,\n",
       " 2357,\n",
       " 2363,\n",
       " 2364,\n",
       " 2365,\n",
       " 2371,\n",
       " 2372,\n",
       " 2374,\n",
       " 2375,\n",
       " 2376,\n",
       " 2377,\n",
       " 2378,\n",
       " 2379,\n",
       " 2381,\n",
       " 2385,\n",
       " 2389,\n",
       " 2391,\n",
       " 2392,\n",
       " 2402,\n",
       " 2404,\n",
       " 2405,\n",
       " 2406,\n",
       " 2408,\n",
       " 2418,\n",
       " 2419,\n",
       " 2421,\n",
       " 2424,\n",
       " 2433,\n",
       " 2435,\n",
       " 2437,\n",
       " 2438,\n",
       " 2440,\n",
       " 2442,\n",
       " 2447,\n",
       " 2451,\n",
       " 2455,\n",
       " 2457,\n",
       " 2460,\n",
       " 2463,\n",
       " 2467,\n",
       " 2470,\n",
       " 2478,\n",
       " 2480,\n",
       " 2487,\n",
       " 2490,\n",
       " 2494,\n",
       " 2498,\n",
       " 2503,\n",
       " 2506,\n",
       " 2511,\n",
       " 2512,\n",
       " 2514,\n",
       " 2515,\n",
       " 2519,\n",
       " 2520,\n",
       " 2524,\n",
       " 2527,\n",
       " 2528,\n",
       " 2529,\n",
       " 2531,\n",
       " 2532,\n",
       " 2535,\n",
       " 2538,\n",
       " 2539,\n",
       " 2543,\n",
       " 2552,\n",
       " 2554,\n",
       " 2563,\n",
       " 2565,\n",
       " 2568,\n",
       " 2569,\n",
       " 2573,\n",
       " 2574,\n",
       " 2575,\n",
       " 2577,\n",
       " 2578,\n",
       " 2580,\n",
       " 2582,\n",
       " 2590,\n",
       " 2592,\n",
       " 2593,\n",
       " 2595,\n",
       " 2601,\n",
       " 2603,\n",
       " 2606,\n",
       " 2607,\n",
       " 2611,\n",
       " 2612,\n",
       " 2614,\n",
       " 2615,\n",
       " 2616,\n",
       " 2622,\n",
       " 2623,\n",
       " 2625,\n",
       " 2629,\n",
       " 2631,\n",
       " 2634,\n",
       " 2638,\n",
       " 2642,\n",
       " 2646,\n",
       " 2651,\n",
       " 2656,\n",
       " 2657,\n",
       " 2661,\n",
       " 2662,\n",
       " 2665,\n",
       " 2666,\n",
       " 2668,\n",
       " 2673,\n",
       " 2677,\n",
       " 2678,\n",
       " 2681,\n",
       " 2683,\n",
       " 2684,\n",
       " 2694,\n",
       " 2696,\n",
       " 2699,\n",
       " 2708,\n",
       " 2709,\n",
       " 2710,\n",
       " 2715,\n",
       " 2716,\n",
       " 2720,\n",
       " 2721,\n",
       " 2722,\n",
       " 2723,\n",
       " 2726,\n",
       " 2728,\n",
       " 2737,\n",
       " 2738,\n",
       " 2740,\n",
       " 2741,\n",
       " 2743,\n",
       " 2746,\n",
       " 2748,\n",
       " 2754,\n",
       " 2755,\n",
       " 2757,\n",
       " 2759,\n",
       " 2761,\n",
       " 2762,\n",
       " 2765,\n",
       " 2766,\n",
       " 2767,\n",
       " 2768,\n",
       " 2771,\n",
       " 2782,\n",
       " 2783,\n",
       " 2784,\n",
       " 2793,\n",
       " 2794,\n",
       " 2795,\n",
       " 2798,\n",
       " 2803,\n",
       " 2807,\n",
       " 2812,\n",
       " 2822,\n",
       " 2823,\n",
       " 2824,\n",
       " 2827,\n",
       " 2831,\n",
       " 2838,\n",
       " 2839,\n",
       " 2841,\n",
       " 2843,\n",
       " 2845,\n",
       " ...]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.model.layers[0].mlp.gate_proj.weight.data == 0).all(dim=1).nonzero()[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256, 2048])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"/n/netscratch/iaifi_lab/Lab/ericjm/narrow/trainwhilepruning01/both/checkpoint-15000\",\n",
       "  \"architectures\": [\n",
       "    \"LlamaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"eos_token_id\": 128001,\n",
       "  \"head_dim\": 64,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 2048,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 8192,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"mlp_bias\": false,\n",
       "  \"model_type\": \"llama\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 16,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_scaling\": {\n",
       "    \"factor\": 32.0,\n",
       "    \"high_freq_factor\": 4.0,\n",
       "    \"low_freq_factor\": 1.0,\n",
       "    \"original_max_position_embeddings\": 8192,\n",
       "    \"rope_type\": \"llama3\"\n",
       "  },\n",
       "  \"rope_theta\": 500000.0,\n",
       "  \"tie_word_embeddings\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.49.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 128256\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.models.llama.modeling_llama import (\n",
    "    LlamaMLP, \n",
    "    LlamaAttention, \n",
    "    LlamaRMSNorm, \n",
    "    LlamaRotaryEmbedding, \n",
    "    LlamaDecoderLayer, \n",
    "    LlamaModel, \n",
    "    LlamaForCausalLM,\n",
    "    LlamaPreTrainedModel,\n",
    "    ACT2FN\n",
    ")\n",
    "from transformers.models.llama.configuration_llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSizeLlamaConfig(LlamaConfig):\n",
    "    r\"\"\"\n",
    "    This is a subclass of LlamaConfig that allows for variable intermediate sizes across layers.\n",
    "\n",
    "    Args:\n",
    "        intermediate_size (`int` or `List[int]`, *optional*, defaults to 11008):\n",
    "            Dimension of the MLP representations. Can be a single integer (all layers have the same size) or a list of \n",
    "            integers (one per layer). If a list is provided, its length must match `num_hidden_layers`.\n",
    "        \n",
    "        # ... all other args from LlamaConfig ...\n",
    "    \"\"\"\n",
    "\n",
    "    model_type = \"variable_size_llama\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        hidden_size=4096,\n",
    "        intermediate_size=11008,\n",
    "        num_hidden_layers=32,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=None,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        pretraining_tp=1,\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        mlp_bias=False,\n",
    "        head_dim=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # Process the intermediate_size parameter\n",
    "        if isinstance(intermediate_size, list):\n",
    "            if len(intermediate_size) != num_hidden_layers:\n",
    "                raise ValueError(\n",
    "                    f\"If intermediate_size is a list, it must have {num_hidden_layers} elements \"\n",
    "                    f\"(one per layer), but got {len(intermediate_size)} elements.\"\n",
    "                )\n",
    "            self.intermediate_sizes = intermediate_size\n",
    "            # Use the first value for compatibility with parent class\n",
    "            intermediate_size_for_parent = intermediate_size[0]\n",
    "        else:\n",
    "            # If it's a single integer, create a list with the same value for all layers\n",
    "            self.intermediate_sizes = [intermediate_size] * num_hidden_layers\n",
    "            intermediate_size_for_parent = intermediate_size\n",
    "\n",
    "        # Call the parent class constructor with the processed intermediate_size\n",
    "        super().__init__(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=intermediate_size_for_parent,  # Use the first value or original value\n",
    "            num_hidden_layers=num_hidden_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            num_key_value_heads=num_key_value_heads,\n",
    "            hidden_act=hidden_act,\n",
    "            max_position_embeddings=max_position_embeddings,\n",
    "            initializer_range=initializer_range,\n",
    "            rms_norm_eps=rms_norm_eps,\n",
    "            use_cache=use_cache,\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            pretraining_tp=pretraining_tp,\n",
    "            tie_word_embeddings=tie_word_embeddings,\n",
    "            rope_theta=rope_theta,\n",
    "            rope_scaling=rope_scaling,\n",
    "            attention_bias=attention_bias,\n",
    "            attention_dropout=attention_dropout,\n",
    "            mlp_bias=mlp_bias,\n",
    "            head_dim=head_dim,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "class VariableSizeLlamaMLP(LlamaMLP):\n",
    "    def __init__(self, config, layer_idx):\n",
    "        # Don't call super().__init__(config) since we want to override the intermediate_size\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        # Use the layer-specific intermediate size\n",
    "        self.intermediate_size = config.intermediate_sizes[layer_idx]\n",
    "        \n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n",
    "        self.act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "\n",
    "class VariableSizeLlamaDecoderLayer(LlamaDecoderLayer):\n",
    "    def __init__(self, config: LlamaConfig, layer_idx: int):\n",
    "        super(LlamaDecoderLayer, self).__init__()  # Call nn.Module.__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        \n",
    "        self.self_attn = LlamaAttention(config=config, layer_idx=layer_idx)\n",
    "        \n",
    "        # Use the variable size MLP\n",
    "        self.mlp = VariableSizeLlamaMLP(config, layer_idx)\n",
    "        \n",
    "        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "\n",
    "class VariableSizeLlamaModel(LlamaModel):\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super(LlamaModel, self).__init__(config)  # Call PreTrainedModel.__init__\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        \n",
    "        # Use variable-size decoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [VariableSizeLlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n",
    "        )\n",
    "        \n",
    "        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(config=config)\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "\n",
    "class VariableSizeLlamaForCausalLM(LlamaForCausalLM):\n",
    "    config_class = VariableSizeLlamaConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        # Don't call super().__init__(config) to avoid initializing the standard LlamaModel\n",
    "        LlamaPreTrainedModel.__init__(self, config)\n",
    "        \n",
    "        self.model = VariableSizeLlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = model.config\n",
    "new_config.intermediate_sizes = [model.config.intermediate_size] * model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_config = VariableSizeLlamaConfig(**new_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = VariableSizeLlamaForCausalLM(new_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariableSizeLlamaForCausalLM(\n",
       "  (model): VariableSizeLlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x VariableSizeLlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): VariableSizeLlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narrow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
